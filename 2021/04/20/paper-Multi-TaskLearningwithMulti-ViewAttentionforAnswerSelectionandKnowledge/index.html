<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge 笔记"><meta name="keywords" content="MTL MRC"><meta name="author" content="CinKate"><meta name="copyright" content="CinKate"><title>Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge 笔记 | CinKate's Blogs</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b77222dd6b9929f160b8a04fc8705337";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '3.9.0'
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#论文链接"><span class="toc-number">1.</span> <span class="toc-text">论文链接</span></a></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number"></span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number"></span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Task-Learning-for-Question-Answering"><span class="toc-number"></span> <span class="toc-text">Multi-Task Learning for Question Answering</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Task-Learning"><span class="toc-number">1.</span> <span class="toc-text">Multi-Task Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Task-Model-with-Multi-View-Attention"><span class="toc-number"></span> <span class="toc-text">Multi-Task Model with Multi-View Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-View-Attention-Scheme"><span class="toc-number">1.</span> <span class="toc-text">Multi-View Attention Scheme</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiment"><span class="toc-number"></span> <span class="toc-text">Experiment</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Datasets-amp-Preprocessing"><span class="toc-number">1.</span> <span class="toc-text">Datasets &amp; Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Task-Learning-Results"><span class="toc-number">2.</span> <span class="toc-text">Multi-Task Learning Results</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-Work"><span class="toc-number"></span> <span class="toc-text">Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Answer-Selection"><span class="toc-number">1.</span> <span class="toc-text">Answer Selection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Task-Learning-1"><span class="toc-number">2.</span> <span class="toc-text">Multi-Task Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number"></span> <span class="toc-text">Conclusion</span></a></li></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://imgsrc.baidu.com/forum/w%3D580/sign=f8de0e9b3e87e9504217f3642039531b/1c3bd133c895d143e395e57b77f082025baf0726.jpg"></div><div class="author-info__name text-center">CinKate</div><div class="author-info__description text-center">长笛一声人倚楼~</div><div class="follow-button"><a href="https://github.com/renxingkai">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">53</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">33</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">17</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CinKate's Blogs</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge 笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-04-19</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/机器阅读理解/">机器阅读理解</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="论文链接"><a href="#论文链接" class="headerlink" title="论文链接"></a><a href="https://arxiv.org/pdf/1812.02354.pdf" target="_blank" rel="noopener">论文链接</a></h3><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>答案选择和知识库问答（KBQA）是问答（QA）系统的两个重要任务。现有方法分别分开解决这两个任务，这需要大量的重复工作，而忽略了任务之间的丰富的相关信息。在本文中，我们基于以下动机，通过多任务学习（MTL）同时解决答案选择和KBQA任务。首先，答案选择和KBQA都可以视为排序问题，一个在文本级别，另一个在知识级别。其次，这两个任务可以互惠互利：答案选择可以结合知识库（KB）中的外部知识，而KBQA可以通过从答案选择中学习上下文信息来进行改进。为了实现共同学习这两个任务的目标，我们提出了一种新颖的多任务学习方案，该方案利用从各个角度学习到的多视图注意力来使这些任务能够相互交互以及学习更全面的句子表示形式。在多个真实的数据集上进行的实验证明了该方法的有效性，并提高了答案选择和KBQA的性能。同样，从不同的表示角度来看，多视图注意力方案在组合注意力信息方面被证明是有效的。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>多任务学习在NLP领域是广泛的，然而QA中的MTL研究较少。本文中，我们探索了利用MTL去同时处理答案选择和KBQA，这两种任务都可以视为排序任务，一个在答案文本级别，另一个是知识级别。答案选择是在多个答案片段中选出一个最合适的答案，KBQA专注于在KB中抽取出相关的知识，去回答问题。<br>大多多任务学习模块，划分共享层和特殊任务层，共享层在各个任务中共享信息，特殊任务层对不同任务是不同的。大多模型忽视了共享层和特殊任务层之间的交互，并且忽视了不同任务之间的交互。因此，本文提出了MTL从不同方面去学习多视角的attention，能够使不同的任务互相交互。具体而言，我们从任务特定的层中收集注意力信息，以在共享层中学习更全面的句子表示形式。 此外，多视图注意力机制通过结合单词级和知识级信息来增强句子的表示学习。 也就是说，通过使用多视图注意力机制，可以在不同任务之间共享和传输单词级和知识级的注意力信息。</p>
<p>根据实验，与单独答案选择和KBQA任务相比，联合学习可以显着提高每个任务的性能。 实验结果还表明了多视图注意力方案的有效性，并且每个视图的注意力都做出了贡献。本文主要贡献如下：</p>
<ul>
<li>我们探索用于选择答案和知识库问题解答的多任务学习方法。 知识级别的KBQA任务可以改善答案选择任务，而词汇级别的答案选择任务可以增强KBQA任务。</li>
<li>我们提出了一种新颖的多任务学习方案，该方案利用多视图注意力机制来桥接不同的任务，该任务将特定于任务的层的重要信息集成到共享层中，并使模型能够交互式地学习单词级别和知识 级表示。</li>
<li>实验结果表明，答案选择和KBQA的多任务学习优于最新的单任务学习方法。 此外，基于多视图注意力的MTL方案进一步提高了性能。</li>
</ul>
<h2 id="Multi-Task-Learning-for-Question-Answering"><a href="#Multi-Task-Learning-for-Question-Answering" class="headerlink" title="Multi-Task Learning for Question Answering"></a>Multi-Task Learning for Question Answering</h2><p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419162623.png" alt></p>
<p>图一为多任务QA网络的架构(包含AS和KBQA)。</p>
<p><strong>Task-specific Encoder Layer</strong> 首先将预处理后的句子编码成向量表示。不同的QA任务在数据分布和底层表示上应该是不同的。因此，每个任务都配备了一个用于问答的特定任务编码器，每个特定任务编码器都包含一个单词编码器和一个知识编码器来学习完整的句子表示，如图2所示</p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419163335.png" alt></p>
<p><strong>Word Encoder.</strong> 输入为词向量，使用BiLSTM去捕获全文信息，得到Hw。</p>
<p><strong>Knowledge Encoder.</strong> 由于知识序列是由一系列标记化的实体或关系名称组成的，因此后一种学习过程需要基于高级知识的表示。针对这一问题，我们将CNN应用到知识序列上，其中大小为n的filters在知识嵌入矩阵上滑动以捕获局部n-gram特征，每次移动都会计算一个隐藏层向量。由于实体长度不确定，因此，会使用不同大小的卷积核来计算，然后使用Dense层来获取知识表示Hk</p>
<p>将Hk和Hw进行拼接，<code>$H_q=[H_{W_q}:H_{K_q}]$</code>，<code>$H_a=[H_{W_a}:H_{K_a}]$</code>，分别为问题和答案的隐藏表示。</p>
<p><strong>Shared Representation Learning Layer</strong><br>与任务特定编码层的输入相比，整句表示具有更丰富的语义，与其他任务的分布更相似。因此，我们整合来自所有任务的编码向量，并通过一个高级共享的Siamese-Bi-LSTM来生成最终的QA表示<code>$S_q$</code>和<code>$S_a$</code>，然后进行平均池化，并且使用了一些词级别特征<code>$x_{ol}$</code>，最终表示为<code>$x=[s_q,s_a,x_{ol}]$</code></p>
<p><strong>Task-specific Softmax Layer</strong> 使用softmax最终分类。</p>
<h3 id="Multi-Task-Learning"><a href="#Multi-Task-Learning" class="headerlink" title="Multi-Task Learning"></a>Multi-Task Learning</h3><p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419165307.png" alt></p>
<p>最小化以上目标函数，<code>$\lambda$</code>为不同任务的权重</p>
<h2 id="Multi-Task-Model-with-Multi-View-Attention"><a href="#Multi-Task-Model-with-Multi-View-Attention" class="headerlink" title="Multi-Task Model with Multi-View Attention"></a>Multi-Task Model with Multi-View Attention</h2><p>为了增强潜在表征空间中不同QA任务之间的交互作用，我们提出了一种多视角注意机制，从任务特定层和共享层提取重要信息。</p>
<h3 id="Multi-View-Attention-Scheme"><a href="#Multi-View-Attention-Scheme" class="headerlink" title="Multi-View Attention Scheme"></a>Multi-View Attention Scheme</h3><p>如图3所示，与其他注意共享方案不同的是，我们不仅从任务特定层提取注意力，而且还将来自共享层的信息进行组合。此外，我们从词汇和知识两个角度获得注意信息，因为词汇水平和知识水平的信息可能共同促进表示学习。具体来说，我们计算了五种注意观，包括词、知识、语义、知识语义和共同注意力。</p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419165737.png" alt></p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170008.png" alt></p>
<p><strong>Semantic View &amp; Knowledge Semantic View</strong>使用mean/max池化去获取句子的语义表示</p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170129.png" alt></p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170156.png" alt></p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170246.png" alt></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Datasets-amp-Preprocessing"><a href="#Datasets-amp-Preprocessing" class="headerlink" title="Datasets &amp; Preprocessing"></a>Datasets &amp; Preprocessing</h3><p>AS: YahooQA,TREC QA<br>KBQA: SimpleQuestions,WebQSP</p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170445.png" alt></p>
<h3 id="Multi-Task-Learning-Results"><a href="#Multi-Task-Learning-Results" class="headerlink" title="Multi-Task Learning Results"></a>Multi-Task Learning Results</h3><p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170646.png" alt></p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210419170700.png" alt></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="Answer-Selection"><a href="#Answer-Selection" class="headerlink" title="Answer Selection"></a>Answer Selection</h3><h3 id="Multi-Task-Learning-1"><a href="#Multi-Task-Learning-1" class="headerlink" title="Multi-Task Learning"></a>Multi-Task Learning</h3><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文研究了同时解决答案选择和知识库问答问题的多任务学习方法。我们提出了一种新的多任务学习方案，该方案利用从不同角度学习的多视角注意，使这些任务能够相互作用，并学习更全面的句子表示，包括词视角、知识视角、语义视角、知识语义视角和共注意力视角。在几个广泛使用的QA基准数据集上进行的实验表明，答案选择和知识库问答的联合学习方法明显优于单任务学习方法。同时，多视角注意策略能有效地从不同的表征视角收集注意信息，提高整体表征学习效果。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">CinKate</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://renxingkai.github.io/2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/">http://renxingkai.github.io/2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MTL-MRC/">MTL MRC</a></div><div class="social-share pull-right" data-disabled="google,facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/04/20/emnlp2020-NoAnswerisBetter/"><i class="fa fa-chevron-left">  </i><span>No Answer is Better Than Wrong Answer A Reflection Model for Document Level Machine Reading Comprehension 笔记</span></a></div><div class="next-post pull-right"><a href="/2021/02/07/ijcai2020-StyIns/"><span>Text Style Transfer via Learning Style Instance Supported Latent Space 阅读笔记</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By CinKate</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://renxingkai.github.io">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>