{"meta":{"title":"CinKate's Blogs","subtitle":"长笛一声人倚楼~","description":"长笛一声人倚楼~","author":"CinKate","url":"http://renxingkai.github.io","root":"/"},"pages":[{"title":"","date":"2021-01-26T01:11:10.597Z","updated":"2021-01-26T01:11:10.597Z","comments":true,"path":"about/index.html","permalink":"http://renxingkai.github.io/about/index.html","excerpt":"","text":"个人信息姓名： 任星凯 性别： 男 出生年月： 1996/02 邮箱： renxingkai0101@163.com QQ： 179049243 教育经历 2018.09–至今 中南大学，计算机学院，硕士 2019.07–至今 国防科技大学，电子科学学院，联合培养 2014.09–2018.06 中北大学，物联网工程，学士 实习经历 2020.06-2020.07 贝壳找房 NLP算法工程师 2020.07-2020.09 联想研究院 AI Lab NLP算法工程师 比赛获奖 时间 比赛 结果 2021.01 2020年CCF BDCI 房产行业聊天问答匹配竞赛 3rd/2985 2020.09 2020年法研杯法律阅读理解竞赛 1st 2020.09 2020 年 CCL 小牛杯幽默情绪识别竞赛 2nd 2020.09 2020 年 CCKS 新冠知识图谱构建与问答评测 2nd 2020.09 2020 年百度人工智能开源大赛 10th/826 2020.08 2020 ICDM Knowledge Graph Contest : Specification 10th 2020.04 中国人工智能大赛·语言与知识技术竞赛（个人赛) 7th/738 2020.02 Kaggle Google QUEST Q&amp;A Labeling 5th/1571 金牌 2020.01 Kaggle TensorFlow 2.0 Question Answering 53rd/1233 银牌 2019.11 汽车论坛消费者用车体验内容的判别与标注 竞赛 5th/837 2019.10 莱斯杯军事阅读理解竞赛 16th/625 2019.10 CCF技术需求匹配竞赛 23rd/862 2019.05 2019年法研杯法律阅读理解竞赛 4th/148 2017.05 蓝桥杯程序设计竞赛(Java) 国家二等奖 2017.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2016.11 华北五省计算机应用大赛 国家二等奖 2016.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2015.09 全国大学生英语竞赛 三等奖 奖学金 时间 奖项 2020.09 中南大学研究生学业一等奖学金 2019.09 中南大学研究生学业二等奖学金 2018.09 中南大学研究生学业一等奖学金 2017.09 本科生国家奖学金 2017.01 中北大学综合素质一等奖学金 论文 Xingkai Ren, Ronghua Shi, Fangfang Li. Distill BERT to Traditional Models in Chinese Machine Reading Comprehension. AAAI Workshop, 2020"},{"title":"archives","date":"2019-03-19T17:20:15.000Z","updated":"2020-05-17T16:11:58.945Z","comments":true,"path":"archives/index.html","permalink":"http://renxingkai.github.io/archives/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-03-19T18:33:09.000Z","updated":"2020-05-17T16:11:59.906Z","comments":true,"path":"categories/index.html","permalink":"http://renxingkai.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-19T17:12:23.000Z","updated":"2020-05-17T16:11:59.002Z","comments":true,"path":"tags/index.html","permalink":"http://renxingkai.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"智能搜索和推荐系统第七章--推荐框架及原理","slug":"SearchAndRec-Chapter07","date":"2021-05-19T22:35:50.000Z","updated":"2021-05-19T14:36:15.094Z","comments":true,"path":"2021/05/20/SearchAndRec-Chapter07/","link":"","permalink":"http://renxingkai.github.io/2021/05/20/SearchAndRec-Chapter07/","excerpt":"","text":"1.推荐系统的框架及运行推荐系统关注的三大核心问题，分别是预测、排序和可解释性。预测主要是推断用户对物品的喜好程度。排序是对已经推断出的结果进行排序。可解释性是指对推荐的结果给出合理的解释，甚至可以通过关系图谱的方式展示。 1.1 基本框架一个推荐系统大致可以分为4层，分别为离线层、存储层、近线层和在线层。离线层：不使用实时数据，不提供实时服务。存储层：顾名思义，就是负责存储数据和索引。近线层：使用实时数据，但是不保证实时服务。在线层：使用实时数据，保证实时服务。当然，我们可以把4层的推荐系统简化，去掉近线层从而演变成三层的推荐系统。下图所示是一个典型的Web推荐架构。 最近，比较火热的推荐系统是一种基于信息流的推荐系统。这里的信息流也叫Feed流或者兴趣Feed。顾名思义，Feed流就是将内容按照个人的兴趣组织在一起。基于信息流的推荐系统又可以分为两大类：一类是基于聚合信息流的架构，另一类是基于社交动态信息流的架构。两类基于信息流的推荐系统如下两个图所示。 基于聚合信息流架构借鉴了搜索引擎的架构，在技术上需要一定改造。图7-2所示架构可以划分成几个模块：日志收集、内容发布、机器学习、信息流服务、监控报警。日志收集是所有排序训练的数据来源，要收集的最核心数据是用户在信息流上的行为数据，用于机器学习更新排序模型；内容发布就是用推或者拉的模式把信息流内容从源头发布到受众端；机器学习是利用收集的用户行为数据训练模型，然后为每一个用户即将收到的信息流内容打分；信息流服务是为信息流的展示前端提供服务接口；监控报警是系统的运维标配，保证系统的安全和稳定等。 比较上面两图的架构发现，基于社交动态信息流和基于聚合信息流的推荐系统的不同之处在于产生内容的方式不同，数据分发时所依据的数据来源不同。 虽然基于聚合信息流的推荐系统会逐渐演化成基于社交动态信息流的推荐系统，从图中可以看出两种信息流的架构并不完全一样。我们也可以抽象出一些共有的架构部分。 1.2 组件及功能典型的Web推荐架构主要由4部分组件组成，分别是推荐服务、存储系统、离线学习和在线学习。 推荐服务：用户从Web服务器上获取推荐请求，然后获取系统推荐的物品信息。 存储系统：主要作用是存储必要的数据和索引。比如存储用户特征，包括用户画像数据和用户行为数据；存储物品特征，主要包含物品的属性等；存储推荐算法模型的参数以及物品的索引。 离线学习：利用用户数据进行大量学习，由于通常需要学习的数据量大而且耗时长，因此这部分组件一般在离线环境中运行。 在线学习：利用用户的即时数据，不断实时更新一些模型参数，并逐步对模型进行调整。 1.2.1 推荐引擎是如何工作的前文已经讲解了推荐系统的三大核心问题，同时我们知道推荐算法在整个推荐系统的地位和作用是相当重要的。下面学习推荐引擎是如何工作的。如图7-4所示，推荐引擎从一个大的结果集中通过协同过滤模型或者一些相关性模型或算法进行结果召回，然后把召回的结果集进行排序。排序阶段又可以细分为粗排、精排以及再排序更为细致的阶段。推荐引擎根据不同的推荐机制并利用数据源中的一部分用户数据，分析出一定的规律或者直接预测用户对其他物品或内容的喜好。这样，推荐引擎就可以给用户推荐他可能感兴趣的物品或者内容了。 对比搜索系统和推荐系统可知，搜索系统最重要的目标是降低延迟和提高相关性分析。推荐系统的目标不是帮助用户找到相关内容，而是希望用户消费内容。当然，搜索系统和推荐系统也有很多相似的地方。比如，推荐系统和搜索系统底层技术实现基本上是相同的。基于内容的推荐系统本质上就是一个小的搜索系统。 广告系统是一个特殊的存在。搜索系统和推荐系统都是为人找信息，而广告系统是为信息找人。广告系统在形式上更像推荐系统，在技术实现上又兼有推荐系统和搜索系统两者的特点。其实在技术实现上，我们可以将搜索系统和推荐系统进行完美的统一。 1.2.2 推荐系统的经典问题推荐系统一直存在两个比较经典的问题：探索和利用（Exploration &amp; Exploitation，EE）、冷启动问题。本节主要介绍探索和利用问题。探索指探索未知的领域；利用指根据当前信息，由训练的模型做出最佳的决策。实际上，探索是指做你以前从来没有做过的事情，以期望获得更高的回报；利用是指做你当前知道的、能产生最大回报的事情。 在推荐系统中为了可以准确估计每件物品的响应率，我们可以将每件候选物品展示给一部分用户，并及时收集物品的响应数据，以此对候选物品进行探索。然后，利用响应率估值较高的物品来优化目标。但是探索过程中存在机会成本，如果仅根据当前收集的数据估算物品响应率，那么，实际上候选物品可能并没有机会展示给用户，这是一个权衡和博弈的过程。 如果利用太多，那么模型比较容易陷入局部最优，但是探索太多，模型收敛速度太慢，这就是EE的困境。EE问题的核心是平衡推荐系统的准确性和多样性。所以，解决EE问题的关键是找到一种长期收益最高，但可能对短期奖励（Short-term Reward）有损失的策略。现实中，我们可以用求解多臂赌博机（Multi-Armed Bandit，MAB）的方法来解决EE问题。 Bandit算法来源于历史悠久的赌博学。事情是这样的：一个赌徒要去摇老虎机，走进赌场一看，一排老虎机外表一模一样，但是每个老虎机吐钱的概率不一样。他不知道每个老虎机吐钱的概率分布，那么每次该选择哪个老虎机来最大化收益呢？这就是MAB问题。Bandit算法不是指一个算法而是指一类算法。 下表介绍了几个最常用的Bandit算法。 下表为推荐系统与Bandit算法对应关系 在推荐系统中，我们常采用三类策略解决EE问题，包括贝叶斯方法、极小/极大方法以及启发式赌博方案。这里只介绍前两种方案。 1.贝叶斯方法 贝叶斯方法解决MAB问题如下表所示。MAB问题可以转化成马尔可夫决策过程（MDP）。MDP问题的最优解需要通过动态规划（DP）的方式求解，虽然存在最优解，但是求解的成本极高。MDP是一个研究序列决策问题的框架。其利用状态空间、奖励函数以及转移概率定义了一个序列问题。贝叶斯方法的目标是找到与MAB问题对应的贝叶斯最优解。 下表为使用贝叶斯方法解决MAB问题 2.极小/极大方法 EE问题也可以利用极小/极大方法来解决。极小/极大方法的核心思想是找到一种方案，使该方案的最差性能限定在合理范围内。性能可以由遗憾来衡量。假设臂中奖概率是固定的，那么中奖概率最高的臂就是最优臂。所以在T次拉臂后，遗憾就是拉最优臂T次获得的期望总奖励与根据拉臂方案获得总奖励之间的差值。 在极小/极大方法中，UCB的解决方案最为流行，其通常会不断探索以降低最差性能。 2.推荐系统的冷启动随着越来越多的互联网平台对推荐系统的使用以及推广，用户对于通过推荐系统获取信息的方式也越来越习惯。当用户当前搜索的历史行为为空时，推荐系统面临一个比较独特的状态，即冷启动状态。冷启动问题处理不好会导致推荐的满意度降低。针对新用户，推荐系统如何生成推荐结果，尤其在当下引入新用户的成本相当高的情况下，如何快速让新用户留存下来并转化是非常重要的。所以，对于推荐系统来说，处理冷启动问题是一门学问，也是一个难点。 推荐系统冷启动主要分为三类：用户冷启动、物品冷启动、系统冷启动。冷启动问题的解决方案可以有以下几种，比如利用热门数据、利用用户注册信息、利用第三方数据、利用物品内容属性和利用专家标注数据，如下图所示。下面举例介绍这几类冷启动问题解决方案。 1.利用热门数据热门数据是物品按照一定规则排序得到的排名靠前的数据，反映了大众的偏好。在某些场景下，我们可以先用热门数据作为冷启动问题的解决方案。 2.利用用户注册信息用户注册时，系统会对新用户的信息进行收集。推荐系统可以利用用户基本信息，如年龄、学历等对用户分类，然后根据用户所属分类推荐同类别下其他用户喜欢的物品；利用用户在注册过程中授权的信息，如定位信息、通讯录信息等，推荐给通讯录好友喜欢的物品等；利用用户注册过程中填的兴趣标签，推荐与标签相关物品。 3.利用第三方数据目前，很多网站或者App支持第三方账号登录。用户登录功能支持QQ、微信、邮箱或者第三方账号登录。系统可以获取第三方平台提供的相关信息，这个相关信息可能包括用户本身信息和朋友关系信息。系统通过协同过滤或者聚类等算法计算出用户的兴趣度，弥补用户冷启动所带来的推荐不足。 4.利用物品内容属性在新闻类、咨询类网站中利用物品的内容属性推荐是十分重要的。物品的内容属性分为三大类：物品本身的属性、物品的归纳属性、物品的被动属性。物品本身的属性包括标题、产出时间等。物品的归纳属性是物品的类别属性，包括类别、品牌等。物品的被动属性表示物品的被动行为的属性，如浏览、点击、评论等。由于新物品缺少被动属性，因此在进行推荐时，我们可以根据其本身属性和归纳属性推荐给喜欢同类物品的用户。例如，周杰伦的《说好不哭》这首歌在刚推出时，我们可以根据其本身属性（歌手、发行时间、歌曲简介等）和归纳属性（类型、流派等）找到相关歌曲。 5.利用专家标注数据有些系统在刚建立的时候，既没有用户行为数据，也没有充分的物品内容数据，因此很难进行物品相似度度量。这种情况属于系统冷启动问题，可利用专家标注解决。以Pandora电台为例，从音频信息上解决相似度问题，技术实现难度较大，而仅仅使用专辑、歌手等信息，推荐效果又不是很好。Pandora电台为了更加精准地进行推荐，聘请一批音乐专家对几万名歌手的歌曲从400多个维度去标注，构建每首歌曲的音乐基因向量，然后通过常见的向量相似度算法计算出歌曲的相似度。 3.推荐系统的召回策略前文中讲到大型的推荐系统一般都会有两个阶段——召回和排序阶段。为什么需要召回阶段？首先是因为物品众多，系统无法为每一个用户逐一计算每一个物品的评分，这就需要召回阶段。召回阶段的作用就是圈出一部分物品，以此降低系统计算量。根据不同的业务场景，我们可以选择不同的召回策略。召回策略有很多种，比较重要的有基于行为相似的召回和基于内容相似的召回。 3.1基于行为相似的召回协同过滤算法（Collaborative Filtering Recommendation）是推荐系统最基础和最常用的算法。该算法通过分析用户的兴趣，在用户群中找出与当前用户相似的用户。但是，该算法有一个前提条件，即相似的人对于同一个事物所表现出的兴趣度是相同的。 协同过滤算法包括以下几个步骤：收集用户偏好、找到相似的用户、计算并推荐。 协同过滤算法也可分为两种：一种是基于用户（User-based CF），另一种是基于物品（Item-based CF）。下面具体讲解这两种算法。 1）User-based CF算法的核心思想是利用用户的行为去定义与其相似的用户，即先使用统计方法寻找与当前用户有相同喜好的近邻用户，然后根据近邻用户的行为数据产生推荐结果，如下两图所示。 2）Item-based CF算法的核心思想是根据用户对物品的评价，发现物品间的相似度，根据目标用户的历史偏好将类似的物品推荐给用户，如下两图所示。 在计算两个用户的兴趣相似度过程中，我们可以使用以下几种方法。 1.Jacard相似度Jacard相似度计算如式（7-2）所示： 3.2基于内容相似的召回基于内容相似的召回往往又建立在对内容理解的基础上。它的核心思想是根据推荐物品的元数据或描述内容，发现物品间的相关性，然后基于用户的喜好，推荐给用户相似的物品。 前文中提到过语言模型，这里介绍另一种语言模型Word2Vector。这个模型概念是Mikolov在2013年提出来的。Mikolov在NNLM（Neural Network Language Model）模型的基础上，提出了Word2Vector的算法。Word2Vector有两种训练模式，分别是CBOW和Skip-gram。在结构上，CBOW和原NNLM类似，去掉了隐藏层，使投影层直接映射到了Softmax输出；在原理上，CBOW和原NNLM一样，也是利用被估计词的上下文来预测该词的向量。但是，Skip-gram和CBOW在原理上正好相反，是用某个词来预测该词的上下文。为了减少计算量，Mikolov提出了两套解决方法，一种是Hierarchical Softmax，另一种是Negative Sampling。 Hierarchical Softmax是把输出层改造成基于词频设计的Huffman Tree，用叶子节点表示每个词，通过根节点到词的路径为词编码，从而计算得到每个词的词向量。词频越高，离Tree的根节点越近，则该词更加容易被搜索到。 尽管分层Softmax在计算上已经达到了实用的程度，但是Mikolov依然对计算速度不够满意，于是在简化噪声对比估计的基础上，得到Negative Sampling方法，以代替分层Softmax的Huffman Tree结构。 1.Huffman编码与Huffman tree Huffman Tree是带权重的最优二叉树，即构造一棵二叉树，使带权重的路径长度值最小。权重越大的节点离根节点的路径距离就越短；反之，离根节点的路径距离也就越长。 Huffman Tree的构造方法如下。1）假设存在n个权重值的序列θ={θ1，θ2，…，θn}，每一个权重可以视为一棵单独的树。2）从大到小为权重序列重新排序，找出权重相对最小的两棵树作为左右子树，构造出一棵新的二叉树。我们可以指定左右子树哪个权重更小一些（比如左边比右边小）。新的二叉树的根节点的权重是两个子节点权重的和。3）在原权重序列中删除已经合并的树，并加入新的树。4）重复第2步和第3步，直到序列中只有一棵树为止。如果给出一句话“我”“爱”“北京”“天安门”，它们在整篇文章中的出现频率分别是1、2、3、4，把词出现的频率当作权重来构建一棵Huffman Tree，具体步骤如下图所示。 1）根据构建Huffman Tree的步骤，首先挑取权重相对最小的两个值（1和2）作为左右子树，然后合并构建新的二叉树（权重小的为左子树，大的为右子树），新的二叉树根节点值为3，并删去原来的1和2，用新二叉树代替。2）在新的集合中挑选两个最小的权重为左右子树，即两个3，合并两个子树构成新的二叉树，它的根节点的权重值是6。删去旧的部分，加入新的二叉树，得到第二步。3）在新的集合，4和根节点为6的二叉树中，4较小作为左子树，6较大作为右子树，它们新的根节点为10。删去旧的部分，只留下新的二叉树。我们发现只有新的二叉树存在。新的二叉树即所求的Huffman Tree。我们也可以使用Huffman编码的方式来表示Huffman Tree。下面再来看看Huffman编码的构造方法。这里举一个例子。 Huffman Tree在构造的过程中统一给出左右子树大小的约定，在本节中左子树比右子树的权重值小。如果采用二进制编码的方式，左节点标识为0，右节点标识为1。根据这个规则我们尝试为“我”“爱”“北京”“天安门”4个词找出它们的Huffman编码，如下图所示。 “我”对应的编码为110，“爱”对应的编码为111，“北京”对应的编码为10，“天安门”对应的编码为0；很显然字符出现频率小编码就长，出现频率高编码就短，这样保证了此树的带权路径长度，效果上就是传送报文的最短长度。 2.CBOW-Hierarchical Softmax CBOW（Continuous Bag-Of-Words Model）模型只包括了输入层、投影层和输出层。如果已知当前词是wt，上下文词是wt–2、wt–1、wt+1、wt+2。模型CBOW可以看作是利用上下文wt–2、wt–1、wt+1、wt+2来预测当前词wt的模型，如下图所示。 根据之前的介绍可以知道，计算每个词的词向量只和这个词与其对应的上下文有关系，即与（Context(w)，w）有关。一般，我们可以给定一个范围来限制这个词的上下文Context(w)。目标函数可以用对数似然函数来描述，公式如（7-9）所示： CBOW-Hierarchical Softmax模型框架如下图所示。 如果词典中词的个数为N，那么叶子的节点数也是N，非叶子节点的数目为N–1个。 3.Skip-Gram-Hierarchical Softmax 如下图所示，Continuous Skip-Gram模型和CBOW一样，也包括输入层、投影层和输出层。如果已知当前词是wt，则上下文词是wt–2、wt–1、wt+1、wt+2。Skip-Gram可以看作是利用当前词wt来预测上下文wt–2、wt–1、wt+1、wt+2的模型，和CBOW的输入和预测正好相反。 对于Hierarchical Softmax的Skip-Gram模型来讲，其需要优化的目标似然函数是： 4.推荐系统排序4.1特征选择的方法排序之前，我们需要考虑影响排序的特征。特征选取的优劣最终会影响到用户体验。工业界的认识是：数据和特征决定了机器学习的上限，而模型和算法只是用于无限地逼近这个上限。先来给特征工程下一个定义：特征工程的本质是一项工程活动，目的是从原始数据中提取供算法和模型使用的有效数据。下面给出一张特征工程示意图。 特征工程中特征处理是最核心的部分。特征处理可以分为数据预处理、特征清洗。而我们所说的特征通常可以分为基础特征和组合特征。 基础特征包括但不限于用户的基础信息，比如用户的性别、年龄、身高、生日和注册时间等；用户的输入内容，比如一些平台建议用户填写的兴趣标签、用户自身的描述信息和用户的评论信息等；用户的行为信息，比如用户的登录信息、登录时间段、使用时长、对物品的评价、物品页面的停留信息和物品页面的点击信息等。这些特征又可以根据不同的标签、类别、时间属性和位置信息等再次分割成更细微的特征。我们将这些特征归类为基础特征主要是因为它们通常是在产品日志中直接产生的，其中不少直接对推荐结果产生不可忽略的影响，但是有些不能直接使用，这就需要组合特征的存在。 组合特征主要是通过对基础特征乃至组合特征本身不断再组合的方式产生的特征。组合方法主要包括分箱、分解类别特征再组合、加减乘除、平方、开平方等。在不同的推荐模型下，对特征的选取以及再加工过程也不同。比如业界常用的线性模型LR，在使用的时候其实要求所有选用的特征都与预测的目标线性相关，所以在进行特征工程的时候，对组合特征的使用更为频繁及复杂。而在深度交叉模型中如Deep FM，对高阶组合特征的生成更依赖模型本身。但是，这并不代表深度交叉模型中，特征的选取与特征工程就不再重要，还是需要根据生产场景，选择不同的侧重点进行挖掘。 在生成了特征之后，特征验证也是一个比较重要的工作。由于生产场景的不同，生成的特征中往往存在不用或者暂时不可用的情况，这需要我们在一开始就将这类特征排除，以减少后面的工作，进而优化特征生产的流程等。 1.特征预处理 经过特征提取，我们可以得到未经处理的特征，这些特征数据可能有一些问题，不能直接使用。存在的问题总结如下。1）不属于同一量纲。特征的规格不一致，不能放到一起。2）信息冗余。对于某些定量特征，其包含的信息没有按区间划分。如征婚对象的高度，如果只关心合适、不合适可以转换为1和0表示。3）定性特征不能直接使用。某些机器学习算法只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征值为1，其他扩展特征值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作。对于线性模型来说，其使用哑编码后的特征可达到非线性的效果。4）特征存在缺失值。缺失值需要补充。5）信息利用率低。不同的机器学习算法对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。因为有上面这些问题的存在，我们有一些特别的方法进行特征处理。 （1）无量纲化特征处理 对于无量纲化数据的处理可以采用标准化和区间缩放法进行处理。标准化处理的前提是特征服从正态分布，标准化后的特征服从标准正态分布。区间缩放法是利用边界值信息，将特征值缩放到某个范围。 from sklearn.preprocessing import StandardScaler StandardScaler().fit_transform(input_data) from sklearn.preprocessing import StandardScalerMinMaxScaler().fit_transform(input_data) 2.特征选择 常用的特征选取方法主要包括过滤法、封装法、嵌入法。 过滤法：即按照相关性对各个特征进行评分，设定阈值或者待选阈值的个数，选择特征。例如方差选择法：先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征；相关系数法：将P值作为评分标准，选择K个特征值；卡方检验和互信息法等。 方差选择法实现代码如下： from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(input_data) 用sklearn中feature_selection库的SelectKBest类结合相关系数选择特征的代码如下： from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为评估特征是否好的函数，该函数输入特征矩阵和目标向量#输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值，在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_ transform(input_data, input.target) 卡方检验实现代码如下： from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择k个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(input_data, input_target) 互信息方法也是评价两个变量之间相关性的方法，计算公式如下： 封装法：对于备选特征，每次在模型中选择或者删除部分特征，基于现有的评价标准，利用模型或者评分标准去评价变动特征对结果的影响，反向选择特征。 嵌入法：先使用某些机器学习算法进行训练，得到各个特征的权重，再根据权重从小到大选择特征。 4.2推荐系统的排序过程在拥有了备选数据集和大量确定的特征之后，我们进入推荐系统的排序阶段。推荐排序问题和搜索排序问题完全一致。之前已经介绍过排序学习(L2R)的一些基本理论知识。排序学习可以分为单文档方法、文档对方法和文档列表方法。在实际的应用过程中，我们会把排序模型分为线性模型、树模型、深度学习模型，以及它们之间的组合模型等。业界普遍认为的模型迭代是从早期的线性模型LR，到引入自动二阶交叉特征的FM和FFM，再到非线性树模型GBDT和GBDT+LR，然后到深度学习模型，如下图所示。 这里主要是比较一下传统机器学习模型的优缺点。 1）LR模型的优点是可解释性强。通常，排序模型良好的可解释性是业界比较在意的指标。但是，LR需要依赖大量人工挖掘特征，而且有限的特征组合无法提供较强的表达能力。 2）FM在LR的基础之上做了改进，引入了交叉项作为特征，可以减少人工特征挖掘的过程，捕捉更多的信息。但是，FM模型只能捕捉两两特征间的关系，无法获得更高阶的交叉特征。 3）GBDT是一个提升模型，它通过组合多个弱模型拟合残差得到一个更强的模型。GBDT属于树模型，能够很好地挖掘组合高阶特征，具有一定可解释性。但是，它对高维度稀疏特征、时间序列特征处理得不是很好。 随着业务场景的扩展，在传统模型上优化和收益将会受限。与此同时，海量数据、知识图谱等多维度特征的引入，使传统的排序学习继续向深度学习模型发展。前文中也提到了几个深度学习模型实例，这里具体讲讲深度学习模型优势。 1）强大的模型拟合能力。深度学习模型包含多个隐藏层和隐藏结点，配合非线性激活函数可以模仿神经细胞工作方式去拟合任何函数。 2）强大的特征表征和泛化能力。深度学习模型可以处理很多传统模型无法处理的特征。例如深度学习模型可以直接从海量训练样本中学习到高维稀疏特征的隐含信息，并通过嵌入的方式表征；对于文本、序列特征以及图像特征，深度学习模型均可处理。 3）自动组合和发现特征的能力。华为提出的Deep FM以及Google提出的Deep Cross网络模型可以自动组合特征，代替大量人工组合特征。 当然，深度学习模型也存在一些现实问题。比如深度学习的黑盒属性会带来巨大的解释成本，也会带来一些业务问题。比如，对于负例的快速响应、模型是否能充分学习无从得知。但是，我们相信深度学习一定是推荐系统发展的方向。 5.基于知识图谱的推荐系统知识图谱是认知智能的重要一环，知识赋能的智能推荐将成为未来推荐系统的主流。智能推荐可以表现在多个方面，包括场景化推荐、任务型推荐、冷启动场景下推荐、跨领域推荐、知识型推荐等。 1）场景化推荐。比如在淘宝上搜“沙滩裤”“沙滩鞋”，通过这些搜索词，系统可以推测用户近期去海边度假，可以按照这个场景推荐防晒霜、草帽、遮阳帽等。 2）任务型推荐。用户购买了羊肉卷、火锅底料等，系统可以根据完成涮火锅任务所需物品进行推荐，比如推荐火锅、电磁炉等。 3）冷启动场景下的推荐。这是推荐领域比较棘手的问题。我们可以通过知识图谱解决推荐系统数据稀疏及冷启动问题。 4）跨领域的推荐。现在流量入口成为吸金入口，各大网站纷纷寻找新的模型进行流量变现。做好垂类的知识图谱以及打通多个知识图谱具有一定的经济效益。比如，如果一个短视频用户经常晒风景照片或视频，那么平台可以考虑为该用户推荐一些淘宝的登山装备。再比如百科知识图谱告诉我们九寨沟是个风景名胜区，旅游需要登山装备，登山装备包括登山杖、登山鞋等，从而实现跨领域推荐。 我们知道推荐系统的最大瓶颈是推荐的可解释性差。现实中，图是解释万物的基础。多种关系的交织可以组成一张图。知识图谱正好以关系图将现实中的实体连接起来。所以，知识图谱必定是推荐系统一个强大的技术支持。 我们可以通过三种方式将知识图谱引入推荐系统。依次学习：首先使用知识图谱得到实体向量和关系向量，然后将这些低维向量引入推荐系统，学习得到用户向量和物品向量，如图下图所示。 联合学习：将知识图谱的特征学习和推荐算法的目标函数结合，使用端到端的方法进行联合学习，如下图所示。 交替学习：将知识图谱和推荐算法视为两个分离但又相关的任务，使用多任务学习的框架进行交替学习，如下图所示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"2020法研杯阅读理解赛道第一名方案","slug":"cail-2020-mrc","date":"2021-05-14T11:31:31.000Z","updated":"2021-05-14T07:15:41.665Z","comments":true,"path":"2021/05/14/cail-2020-mrc/","link":"","permalink":"http://renxingkai.github.io/2021/05/14/cail-2020-mrc/","excerpt":"","text":"2020法研杯阅读理解赛道第一名方案2020年法研杯阅读理解竞赛结束了，我们团队在最终排行榜获得了第一名的成绩，去年也参加了，过了一年，还是一只鶸，害，首先感谢各个队友的帮助，接下来是我对参加这次比赛的总结与分享，希望和大家相互学习，多交流。 数据介绍今年的数据集是去年的升级版，去年格式类似SQuAD 2.0，今年格式类似于HotpotQA，不仅文书种类由民事、刑事扩展为民事、刑事、行政，问题类型也由单步预测扩展为多步推理，难度有所升级。具体而言，对于给定问题，只通过单句文本很难得出正确回答，选手需要结合多句话通过推理得出答案。 本任务技术评测训练集包括两部分，一部分为去年的CJRC训练集，一部分为重新标注的约5100个问答对，其中民事、刑事、行政各约1700个问答对，均为需要多步推理的问题类型。验证集和测试集各分别约为1900和2600个问答对，同样均为需要多步推理的问题类型。第一阶段多步推理数据仅提供民事的一部分数据，规模较小，选手可充分利用CAIL2019的数据进行训练。（来自官网介绍） 简单来看个数据就明白了，需要让模型阅读完问题question和文章context之后，不仅回答出问题的答案(答案仍然有四种类型：span、yes、no、unknown)，还需要找出回答出该答案所依据的线索句子。如下例：通过线索句子1和3可以回答出正确的答案“魏7”。 &#123; &quot;_id&quot;: 5048, &quot;context&quot;: [ [ &quot;经审理查明，&quot;, [ &quot;经审理查明，&quot;, &quot;被告人胡x1的犯罪事实与起诉书指控的一致。&quot;, &quot;另查明，&quot;, &quot;案发后被告人胡x1与魏7达成和解协议，&quot;, &quot;并已履行完毕，&quot;, &quot;魏7对被告人胡x1的行为表示谅解。&quot;, &quot;上述事实，&quot;, &quot;被告人胡x1在庭审过程中亦无异议，&quot;, &quot;且有书证被告人胡x1的户籍证明、金乡县公安局刑警大队出具的到案经过、山东省金乡县人民法院移送公安机关侦查函、民事诉状及三份借据复印件、财产保全申请书、山东省金乡县人民法院（2009）金民初字第826-1号民事裁定书、EMS全球邮政特快专递回执联复印件、金乡县人民法院（2009）金8协字826-1号协助执行通知书及送达回证、（2009）金民初字第826号民事调解书及送达回证、调解笔录、协议书、谅解书、立案审批表及申请执行书、法律文书生效证明书、（2010）金执字第256号执行通知书、特快专递回执联复印件、申请书、（2010）金执字第256-2号执行裁定书、改退批条、（2010）金8协字256号协助执行通知书及送达回执、本地机动车详细查询基本信息、证明、金乡县人民法院传票及执行笔录、金乡县人民法院关于查封被执行人胡x1车辆情况说明、鲁H车辆照片复印件、机动车注册、转移、注销登记/转入申请表及委托书、二手车销售统一发票、机动车交通事故责任强制保险单、齐鲁晚报一份、办案说明、机动车单项查询信息，&quot;, &quot;证人魏7、付某甲、徐某、付某乙的证言，&quot;, &quot;被害人胡x9的陈述，&quot;, &quot;被告人胡x1的供述等证据，&quot;, &quot;足以认定。&quot; ] ] ], &quot;question&quot;: &quot;被起诉书指控的人和谁达成了和解协议?&quot;, &quot;answer&quot;: &quot;魏7&quot;, &quot;supporting_facts&quot;: [ [ &quot;经审理查明，&quot;, 1 ], [ &quot;经审理查明，&quot;, 3 ] ] &#125; 评价方式本任务采用F1进行评估。对于每个问题，需要结合案情描述内容，给出回答，回答为Span（内容的一个片段）、YES/NO、Unknown中的一种，并且给出答案依据，即所有参与推理的句子编号。评价包括两部分：1）Answer-F1，即预测答案会与标准答案作比较，计算F1；2）SupFact-F1，即预测句子编号序列会与标准句子编号序列作比较，计算F1。最终为这两部分F1的联合F1宏平均。 比赛流程本次比赛流程仍然和去年一样，分为三个阶段，第一阶段：报名，发放samll数据集，编写、测试模型，结果不计入最终成绩；第二阶段：发放big数据集，结果将计入最终成绩；第三阶段：选手在第二阶段提交的模型中选择三个模型作为第三阶段封闭测试的模型。比赛的最终成绩计算方式：最终成绩 = 第二阶段的成绩 0.3 + 第三阶段的成绩 0.7评测方式仍然和之前一样，需要提供训练好的模型，主办方线上评测，选手看不到测试集。 接下来我也继续按着时间线来分享下我们团队此次比赛中使用的方案和一些经验。 第一阶段拿到数据之后日常进行数据分析，第一阶段主办方给出了1565个样本，文章(context)长度99%分位数为592.0，问题(question)长度99%分位数为34.7，答案(answer)长度99%分位数为23.0；第二阶段公布了5054个样本，文章(context)长度99%分位数为645.0，问题(question)长度99%分位数为40.0，答案(answer)长度99%分位数为30.0。 主办方同时也发布了基于BERT的多任务联合训练的strong baseline，应该是基于Spider-Net改的，代码结构很清晰，是一个很棒的baseline。我们也是接着baseline继续优化，其实是3个子任务的联合训练：阅读任务、答案类型四分类任务、线索句子二分类任务。 我们也是接着baseline继续优化，其实是3个子任务的联合训练：阅读任务、答案类型四分类任务、线索句子二分类任务。 数据增强部分主办方说明可以使用去年的数据，但是去年数据是类似SQuAD 2.0格式，仅有答案和答案类型，并没有提供线索句子，因此线索句子需要自己构造。我们构造方式主要分为两种，第一种方式是分句之后直接在每个句子中find答案文本，如果该句子中包含答案文本，则将该句子作为支持句子。However，这样做其实是有问题的，留个彩蛋，后面说。。。 第二种方式也是基于第一种方式的，我们考虑到不仅仅是包含答案文本的句子可以是线索句子，有些句子虽然不包含答案文本，但是要回答出答案，需要这些句子的推理，其实也是线索句子的。因此，我们将问题和答案拼接[question:answer]，作为句子A，依次去计算拼接后的句子与文章每个句子的TFIDF相似度，分别取相似度最高的前1、2、3条句子作为线索句子(同时也添加了答案文本所在的句子)。 之后的实验发现第一阶段采用第二种方式取TFIDF top2句子效果最好，然而，比赛第二阶段发现，第一种构造数据方式虽然简单粗暴、正负样本也不均衡，但是比第二种方式效果更好，可能是第二阶段发布了较多的训练集原因。 对于阅读任务，有了去年的经验，我们并没有在BERT后面继续添加一些基于Attention的阅读模型，而是直接取出BERT最后两层的sequence_out进行拼接去做答案的抽取，实验表明，比仅取最后一层效果会更好，BERT不同层学习到了不同的特征表示。对于答案类型分类任务，沿用去年的方法，在BERT的最后一层sequence_out后面接CapsNet和答案四分类的Attention层进行答案的分类。对于线索句子分类任务，基本还没做什么大的改进，沿用baseline的方式对每个句子去做2分类，尝试了下简单的添加多层网络，效果并不明显。 将三个子任务的loss加权相加在一起，作为整个任务的loss进行联合训练。第一阶段初期，我们一直使用的是RoBERTa_base预训练模型，也尝试改各种模型，数据做了一周多些，发现线下基本没啥提升，但是排行榜上前排比我们高了3-4个点，我们觉得还是预训练模型的问题，然后换了RoBERTa_large，确实分数也就提上来了。 第二阶段第二阶段我们做的就更细致一些了。 数据部分 接上面说的，“我们第一种方式是分句之后直接在每个句子中find答案文本，如果该句子中包含答案文本，则将该句子作为支持句子。”由于数据中只给出了答案文本，并没给准确的答案开始位置，主办方给的baseline也是find答案文本，然后取匹配到的第一个答案文本位置作为答案开始位置，但是这样其实是有问题的，会出现错误标记的问题。举个例子：下面样例的问题是“谁垫付了后续的政府奖补资金？”，答案文本是“原告”，但是“原告”两个字在context中出现了多次，第一次出现“原告”位置句子为“被告在进贤县文港镇采砂办渡头榨下的采砂场所占的股权转让给原告所有”，明显不是真正答案的位置，真正答案线索句子为“原告先行垫付被告后续政府奖补资金”。如果按baseline的方式，会有不少样本出现这样的答案标注偏差问题，对于阅读理解这样的任务，这种影响其实挺大的。我们根据训练集给的supporting_facts中的线索句子，去重新标定答案，这样可以确保答案是和线索句子相关，提高了真正答案位置的准确率，不至于完全和问题不相关。 &#123; &quot;_id&quot;: 65, &quot;context&quot;: [ [ &quot;经庭审质证、认证，&quot;, [ &quot;经庭审质证、认证，&quot;, &quot;本院认定事实如下：2014年元月18日，&quot;, &quot;原、被告双方签订了一份采砂场股权转让协议书。&quot;, &quot;协议约定：原、被告自签订该协议起，&quot;, &quot;被告在进贤县文港镇采砂办渡头榨下的采砂场所占的股权转让给原告所有，&quot;, &quot;被告不再拥有采砂场的开采权益及股份权，&quot;, &quot;砂场平台以上存砂、石、机械设备、棚房及其他地面建筑物等全部清理出砂场；&quot;, &quot;被告无条件配合市、县采砂办、文港镇政府切割采砂设备和清理砂场；&quot;, &quot;同时，&quot;, &quot;原告先行垫付被告后续政府奖补资金，&quot;, &quot;原告支付后，&quot;, &quot;后续的砂场设备奖补资金归原告所有。&quot;, &quot;协议还就其他事项作了约定。&quot;, &quot;协议签订后，&quot;, &quot;原告按规定给付了被告全部股权、作业房、装载机、吸沙船及挖沙船的转让金175320元，&quot;, &quot;并垫付了后续的政府奖补资金。&quot;, &quot;为此原告具状法院，&quot;, &quot;提出如前的诉讼请求。&quot;, &quot;另查明，&quot;, &quot;原、被告签订协议后，&quot;, &quot;原告从进贤县文港镇采砂办领取了采砂场的部分分红。&quot; ] ] ], &quot;question&quot;: &quot;谁垫付了后续的政府奖补资金？&quot;, &quot;answer&quot;: &quot;原告&quot;, &quot;supporting_facts&quot;: [ [ &quot;经庭审质证、认证，&quot;, 14 ], [ &quot;经庭审质证、认证，&quot;, 15 ] ] &#125; 使用19年数据时，19年数据文章context长度普遍较长，平均约800-900，这样直接切句子时，容易超出max_len=512，因此，我们使用了下截断，将答案开始位置在480长度之后的，进行根据答案开始结束位置前后取150长度作为新的文章context，这样保证了文本长度基本在512之内，扩充了正样本量。仅通过这两个对数据集的修正，我们ans_f1就提高了1个多点，确实证明了正确数据的重要性。 预训练模型方面的尝试：去年做法研杯的时候，当时预训练模型只有中文版的BERT_base和ERNIE 1.0，短短一年时间，NLP预训练模型五花八门，从理解到生成都发布了很多优秀的预训练模型。因此，除了RoBERTa模型之外，我们还尝试了NEZHA和ELECTRA，但是效果都没有RoBERTa好，差距约2-3个点。通过阅读ACL 2020最佳论文提名奖《Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks》，我们也受到启发，继续对下游任务进行预训练，使用的18年的法研杯数据，约3.5G，继续进行MLM任务的预训练，跑了20W步，然后进行测试，发现也没效果，23333….，也尝试了FGM等对抗训练方式，也没有提升。 对于阅读任务，除了第一阶段的拼接BERT最后两层句向量，还在后面接了BiAttention，类似BiDAF的query、context双向attention，然后去做答案抽取。当然也做了其他各种操作，答案验证、接BiGRU、接Highway Net等。 对于答案类型分类任务，除了在BERT的最后一层sequence_out后面接CapsNet和答案四分类的Attention层进行答案的分类，再结合CLS和sequence_out做了层attention，联合四个logit进行答案四分类，也尝试了DiceLoss这些，但并没有提升。 对于线索句子分类任务，先将BERT的sequence_out再次经过SelfAttention，然后使用了线索句子开始和结束的映射，去和输出的所有句子表示做Attention，突出线索句子的表示，最终concat线索句子开始和结束表示，与max、mean方式进行联合分类线索句子。在训练时，非线索句子数量一般远大于线索句子，由于是二分类，使用的是BCE损失，我们也设置了pos_weight，让正样本权重提高一些。 第二阶段用以上方式，我们最好的单模ansf1有83.46，support_f1有78.23，离比赛第二阶段结束还有一周多时，单模取得了线上第四的成绩。最终的模型集成就是不同的下游模型集成了，以及不同的随机种子，然后选取加权权重这样的常规集成方式。 总结&amp;TO DO 首先感谢主办方举办的第二届法研杯机器阅读理解比赛，赛题也更加有趣和挑战性，期待明年阅读任务和数据格式会是什么样子。 其实HotpotQA排行榜上面前排很多模型都是基于图网络的，用图网络做线索句子识别效果会更好，但也由于我们对图网络了解不够，所以没有采用这样的方法，图网络技能以后还是必须得点上。 这次比赛数据质量很重要，阅读理解答案位置的正确标定，真实的线索句子都会带来不小的提升，所以不论竞赛还是工程项目，在数据上做的要足够细致。 通过分析bad case，发现模型对答案文本靠后的答案有的识别不出来，以及如果原文中多次出现了答案文本，比如多次出现“原告”，也抽取不出来，所以可以对这些bad case再做进一步的优化。 参考文献[1] 法研杯2020官方github [2] RoBERTa: A Robustly Optimized BERTPretraining Approach [3] ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS [4] NEZHA: Neural Contextualized Representation for Chinese Language Understanding [5] Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks","categories":[{"name":"比赛","slug":"比赛","permalink":"http://renxingkai.github.io/categories/比赛/"}],"tags":[{"name":"法研杯2021MRC","slug":"法研杯2021MRC","permalink":"http://renxingkai.github.io/tags/法研杯2021MRC/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第六章--搜索系统评价","slug":"SearchAndRec-Chapter06","date":"2021-05-13T15:26:18.000Z","updated":"2021-05-13T07:27:19.508Z","comments":true,"path":"2021/05/13/SearchAndRec-Chapter06/","link":"","permalink":"http://renxingkai.github.io/2021/05/13/SearchAndRec-Chapter06/","excerpt":"","text":"搜索系统的评价维度一般包括性能评价和效益评价。性能评价一般包括时间性能和空间性能。系统响应的时间越短，占用的存储空间越小，系统的性能就越好。但时间优越性和空间优越性一般不能兼得，需要取舍。对于搜索系统而言，我们除了考虑响应时间和存储空间之外，还需要考虑其他重要指标，如排序相关性等，即需要考虑检索结果列表的全面性、准确性等效果指标。效益评价主要用来测量搜索系统的服务或者系统本身投入使用时所获得的收益，包括经济效益和社会效益。但效益评价一般很难量化，因为其具有滞后性和不确定性。 1.搜索系统的评价体系1.1 效率评价效率评价是对性能评价中时间性能和空间性能的概括。响应时间、开销和索引量属于时间性能和空间性能的范畴，是需要重点评测的方面。 1.响应时间和开销 对于任何一个应用系统，响应时间是评价系统的重要指标。搜索系统的响应时间分为两种情形：委托检索和非委托检索。委托检索表示用户送交查询给专业检索人员，由专业人员操作搜索系统进行检索，然后将检索结果返回给用户。非委托检索表示用户自行操作检索系统得到结果。随着网络的快速发展，多数用户采用的是非委托检索。另外，计算响应时间一般是针对某一个查询而言，但查询的长短、复杂度是不同的，所以对应的响应时间也是不同的。 除去网络拥挤、通信等外部因素的影响，影响响应时间的因素如下。1）文档库规模。文档库规模越大，检索时间越长，响应时间也就越长。2）硬件因素。机器配置越高，运行速度越快，响应时间也就越短。3）检索软件。检索软件性能越好，检索时间越短，响应时间也就越短。4）存储设备类型和数据的存储结构。存储设备的访问速度越快，数据的存储结构越合理，检索越容易，响应时间也就越短。在实际应用中，响应时间的估算不可避免地受到网络和工具性能的影响，造成同一检索在不同时间、不同地点、不同检索系统中，响应时间不同，因此有必要在不同时间段、地点等不同环境下多次计算。 存储空间的开销主要是指系统所占用的内存空间和外存空间。当检索系统内存空间有限时需要合理分配，一般情况采用中型计算机，不存在内存不足的问题。不同的文档结构所需的外存空间区别较大，比如正排索引和倒排索引所需的外存空间不同；同样是倒排索引，系统选择布尔检索还是全文检索，所需的外存空间也不同。 2.索引量 所谓“索引量”，就是搜索引擎抓取到页面后，经过分析筛选可以被用户搜索到的页面的数量。所以索引量越大，搜索引擎的数据覆盖范围越广。索引量经常和收录量混淆，收录量是搜索引擎抓取过、分析过的页面。所以从定义看，收录在索引之前，即页面没有被收录就不可能被索引。以百度搜索引擎为例，提高网站索引量的方法如下。 1）提高网站内容的质量，爬虫喜欢原创的文字性内容。2）设置合理的内链，确保爬虫可以顺利爬行。爬虫通过内链爬向网站的各个部分，所以内链越强大，爬虫爬行越顺利，可以给网址排名和收录加分。3）建设高质量的外部链接，因为网页的权重和外部链接的内容质量相关。4）网站首页保持更新，以便搜索引擎认为该网站是活跃的。5）网站不要轻易更换域名。域名更换会导致搜索引擎对网站的信用度和友好度下降，也会影响搜索引擎对网站的收录量。 在知道了索引量的定义之后，我们再来看看如何设定合适的索引量。由于索引量是一个与工程息息相关的概念，这里我们先要厘清索引的使用和存储。 以常见的Lucene系统举例，Lucene系统中索引其实是对固定分析字段拆解的倒排索引结果。索引的内容主要与拆解后的词、词在文本中的位置、文本的标号相关。而实战中，通常注重当前使用的搜索引擎的耗时和稳定性。所以，我们会考虑给一个庞大的搜索集群配备主集群、备份集群以及备用集群等。在不考虑索引量对磁盘空间占用的情况下，索引量其实与搜索集群的性能息息相关。来自搜索集群外部的搜索请求的大致流程如下。 1）请求到达主节点，主节点对请求进行分析重构，分发给子节点。2）子节点遍历底层的索引，将所有与结果相关的数据返回给主节点。3）主节点综合处理所有子节点的返回数据，并返给搜索集群外部。 那么在衡量一个搜索引擎的耗时时，我们可从以下4方面考虑：搜索集群外部与搜索引擎通信消耗的时间、主节点处理消耗的时间、主节点与子节点通信消耗的时间、子节点搜索索引消耗的时间。同等情况下，索引量越大，搜索耗时就越长。同时，更大的索引量对于服务器I/O性能的要求也更高，所以如何选择索引量的大小，需具体情况具体分析。 1.2 效果评价本节的效果评价主要包含：准确率和召回率、平均化和插值以及排序靠前文档的质量。 1.准确率和召回率 准确率和召回率是搜索引擎在效果评价中最常用，也是公众最认可的指标。准确率又叫查准率（Precision Ratio，P），用来衡量检索结果中有多少文献与查询相关；召回率又叫查全率（Recall Ratio，R），用来衡量一次检索中与查询相关的文献有多少被检索出。准确率和召回率的计算如式（6-1）和（6-2）所示。 利用数学语言分析准确率和召回率：假设某搜索系统的数据库中所有文献的总量是L，对于某个查询，a表示被检索出的与查询相关的文献数量；b表示被检索出的与查询无关的文献数量；c表示与查询相关，但是没有被检索出的文献数量，则： 准确率和召回率之间存在什么关系呢？一个理想的搜索系统，应该是P=1、R=1，但实际上不存在这样的搜索系统。一般来说，准确率和召回率之间存在着反变关系，即如果提高准确率，召回率往往就会下降；如果提高召回率，准确率就会下降。两者相互制约。准确率和召回率的关系如下图所示，A点召回率很高，但是准确率很低；D点和A点相反，准确率很高，召回率很低；B、C两点是上述两种情况的折中。 影响搜索系统准确率和召回率的主要因素如下。1）文档库的质量。文档库的文档是否齐全、索引体系是否完善、检索途径的多少都会影响准确率和召回率。2）对检索需求的理解。要想达到较高的召回率和准确率，就应该较好地理解需求，制定相应的检索策略。3）检索语言的一致性。检索的实质是比较查询和数据库文档的一致性，所以需要不同检索人员表达检索主题的语言和数据库文档一致，更需要标识查询和标识文档的语言一致，这样才能够确保检索的准确率和召回率。4）标引的网罗性。对数据库文档主题分析得越透彻，抽出的关键词就越多，检索时能够检出的相关文献就越多，召回率就越高，但准确率就会降低；反之，如果标引是只标注中心主题，检出的文档的准确率就会比较高，但是漏检会增多，召回率会降低。5）检索词的专指性。检索词的词意越狭窄、越具体、越专深，检出的文档就会越对口，准确率就会越高，但命中的文档数量就越少，召回率会降低；反之，如果检索词越笼统、越宽泛，检出的文档数量就会增多，召回率就会升高，但检出的文档不相关的可能性也会增加，准确率会下降。 一个优质的搜索系统应该具备高的准确率和召回率，但并不是每个用户在任何时候都需要高准确率和召回率。用户需求可以分为以下4种情况。1）要求召回率R=1。例如申请专利、发明等，需要对全世界范围的有关文档全面了解，才能做出客观的评价，这时就需要R=1的搜索系统。2）要求较高的召回率。例如编写教材、某技术的发展综述，往往需要较全面地获得有关文档，这时对召回率要求较高，但不一定要求R=1。3）要求较高的准确率。例如要了解某种产品的有关信息，解决某一具体问题，往往只需要了解某一个方面或者相关信息，这时对准确率要求较高。4）对准确率、召回率没有具体的要求。对于某些检索，用户本身不能够做出明确的表达，因此，对准确率和召回率也无法提出确切的要求。如何综合评价准确率和召回率呢？一般使用F值度量（F-measure）综合衡量准确率和召回率。它的好处在于能够使用单一的数字综合反映系统性能，被定义为准确率和召回率的调和平均数，如公式（6-7）所示。使用调和平均数而不是数字平均数的原因是，调和平均数强调较小的值的重要性，数字平均数受极值影响较大。假设一个查询的召回率为1，准确率为0，数字平均数是0.5，但调和平均数为0，更好地评价了搜索结果。 一个查询会返回很多结果，但一般用户关注的数量是有限的，所以在计算准确率和召回率的时候不能按照所有的返回结果去统计，而是可以简单地在一些预定义的位置上计算准确率、召回率来评估此次搜索结果，这种评估方式被称作位置p的准确率（Precision at Rank p）。由于用户比较关心排序靠前文档的数量，最常使用的是p@10或者p@20。第二种评价方法是，当召回率每增加0.1时，计算准确率的变化。这种方式能够评价排序结果中所有相关的文档，不只是排序靠前的文档。第三种评价方法是，计算一个额外的相关文档被检出时，平均准确率的变化情况。这种方法能够衡量所有相关文档的排序结果，同时严格依赖于排序位置靠前的相关文档。 2.平均化和插值 以上描述的评价方法都是针对一次查询结果进行评估的，要想更加客观准确地评价检索算法的优劣，必须使用多个查询结果进行测试。本节将讨论对查询集合进行评估的方法——平均化和插值法。平均准确率（Mean Average Precision，MAP）是综合多个查询结果进行评价的最简单的方法，就是对多次查询结果求平均，这也意味着它假设每个用户都期望找到更多相关的文档。MAP计算方法示例如下图所示，黑色表示查询的相关文档，白色表示查询的不相关文档，查询1的平均准确率=(1.0+1.0+0.67+0.75+0.6)/5=0.804，查询2的平均准确率=(0+0.5+0.33+0.5+0.4)/5≈0.35。MAP=(0.804+0.35)/2=0.577。 MAP评价方法简单，便于计算且有效，能够给出详细的搜索算法的性能，但会丢失很多文档信息。下图为两次查询的准确率–召回率图，其中系列1对应查询1，系列2对应查询2。每个查询对应的准确率–召回率曲线差异较大，不便于比较。为了生成一个能够综合反映查询结果的准确率–召回率图，我们对准确率–召回率的值进行平均化。该平均化的过程是将每次查询的准确率–召回率值转化为标准召回率等级对应的准确率。 标准召回率的等级在0到1之间，增量是0.1。为了获取每个增量上的准确率，我们对上图的准确率–召回率数据进行插值，使每个召回率的增量等级上都有数值。在搜索评价体系中，插值法定义在任何标准召回率等级R处的准确率P为： 3.排序靠前文档的质量 在很多搜索系统中，用户更关注排序靠前的相关文档。我们日常浏览页面时，可能不会往后翻很多页，注意力主要集中在第一页或者前两页。对于一些问答类查询，用户更倾向于直接获得一个明确的结果，这种情况下使用召回率并不合适。因此，关注搜索引擎排序结果中靠前文档的质量，在评价搜索结果的时候至关重要。 前文已经讲过计算位置p的准确率，其中p的取值一般是10或者20。这种方法通过多次评价查询结果的质量，然后给出结论，计算简单。但是，其也存在一些问题，即没有对相关文档的顺序做详细区分，因此需要关注文档排列的顺序。 MRR（Mean Reciprocal Rank）是指多个查询的排名的均值，是国际上通用的对搜索结果进行评价的指标。假设有两次查询，第一次查询的第一个相关文档的排序是2，第二次查询的第一个相关文档的排序是5，那个根据这两次查询对搜索结果进行评价，计算方法是：1/2（1/2+1/5）=0.35。MRR方法主要应用于寻址类检索或者问答类检索。MRR的计算如式（6-9）所示。 对于网络搜索评价来说，DCG（Normalized Discounted Cumulative Gain）是较为常用的方法。这种方法基于两个假设：1）高相关性的文档比边缘相关的文档更重要；2）一个相关文档的排序越靠后，对用户的价值就越低，因为它们很少被用户查看。这两个假设产生了一种新的评价方法——相关性设定等级，其作为衡量文档增益的标准。这种增益是从排序靠前的结果开始计算，但靠后的排序位置上的增益会有折扣。DCG方法就是在一个特定的位置p的前提下，计算文档总的增益。在介绍DCG之前，先描述一下CG（Cumulative Gain），其表示前p个位置累计得到的增益，公式如（6-10）所示。 还有一种比较常用的计算方式——增加相关度影响比重，如公式（6-12）所示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第五章--搜索系统中的主要算法","slug":"SearchAndRec-Chapter05","date":"2021-05-11T16:01:24.000Z","updated":"2021-05-11T08:02:57.274Z","comments":true,"path":"2021/05/12/SearchAndRec-Chapter05/","link":"","permalink":"http://renxingkai.github.io/2021/05/12/SearchAndRec-Chapter05/","excerpt":"","text":"1.搜索和机器学习搜索引擎包含两个阶段：召回和排序。搜索系统中所涉及的机器学习的算法会分布在两个部分。第一部分是对Query及文档的理解过程，因为理解过程中使用了自然语言处理等相关算法。第二部分就是排序学习过程，即将机器学习技术应用到排序阶段， 1.1排序学习传统的检索模型靠人工来拟合排序公式，并通过不断地实验确定最佳的参数组合，以此构成相关性打分函数。机器学习排序与传统的检索模型不同，可通过机器学习获得最合理的排序公式，而人只需要给机器学习提供训练数据，如下图所示。 机器学习排序由4个步骤组成：人工标注训练数据、文档特征抽取、学习分类函数、在实际搜索系统中采用机器学习模型。机器学习排序框架如下图所示。 单文档方法（Pointwise）：处理对象是单一文档，将文档转化为特征向量后，将排序问题转化为机器学习中常规的分类或回归问题。CTR方法是单文档方法的典型应用，相对比较成熟，广泛应用于广告、搜索、推荐中。CTR方法的数学表达式：y=f(x)，其中y的范围为[0，1]，y的值越大表示用户点击率越高。 文档对方法（Pairwise）：相比于单文档方法算法，文档对方法将重点转向文档顺序关系，是目前相对比较流行的方法。其输入是文档对，输出是局部的优先顺序，主要是将排序问题归结为二元分类问题。这时，机器学习方法就比较多，比如Boost、SVM、神经网络等。对于同一Query的相关文档集中，任何两个不同标记的文档都可以得到一个训练实例（di，dj），如果di&gt;dj，则赋值+1，反之为-1。于是，我们就得到了二元分类器所需的训练样本。预测时可以得到所有文档的一个偏序关系，从而实现排序。 文档列表方法（Listwise）：与上述两种方法不同，其将每个查询对应的所有搜索结果列表作为一个训练样例。根据训练样例训练得到最优评分函数F，评分函数F对每个文档打分，然后根据得分由高到低排序，得到最终的排序结果。这种方法的输入是文档集合，输出是排好顺序的列表。文档列表方法排序示意图如下图所示。 三者之间的区别就在于训练数据之间的关系对预测目标的影响不同。简单地说，单文档排序算法是点点之间排序，训练数据之间的关系与最终排序无关。换句话说，样本经过模型训练形成的是一种评分方式，而所有样本按照评分结果由大到小排序即可。模型生成后，每个样本的输出结果是固定的、静态的，不会发生变化。这里典型的单文档排序算法有逻辑回归、树模型等。事实上，所有利用二分类问题归纳的排序算法都可以当作单文档排序。其根本逻辑就在于设定两个分类1、0，在训练样本时考虑每个独立样本与当前分类的关系，生成模型参数。在排序过程中，先计算每个样本当前参数耦合后的结果再总体排序即可。 而对于文档对方法，输入的是文档对。比如现在有三个文档D1、D2、D3，排序为D1&gt;D2&gt;D3，那么输入应该是&lt;D1，D2&gt;、&lt;D2，D3&gt;、&lt;D3，D1&gt;，对应的训练目标是“文档1是否应该排在文档2的前面”。这种方法在模型构造上与大部分单文档方法可以共用原始模型，好处在于模型训练出来的是对应文档组的排序关系，在复杂、高维度、不易解析的情况下，有时会比单文档方法的排序结果更接近真实值。但是其缺点也很明显，其中一个缺点是由于模型输出的两个文档之间有排序先后关系，如果靠前的位置出现错误，那么对于整体排序的影响是远远大于单文档方法。另一个缺点是模型较难对输出结果进行评价且训练困难，由于不同情况下文档之间的关系多种多样，而且不同情况下，不同的输入对训练产生的影响不同，因此很难对模型整体输出结果做出评价。同时，文档对方法一方面增加了标注的难度，另一方面增加了训练的时间。 而文档列表方法与前两者都不同，其考虑的是模型整体的排序结果，输入是一个文档列表，且每一个文档的对应位置都已经锁定。例如，输入是[D1，D2，D3]，那么该方法认为单次样本的输入中，排序为D1&gt;D2&gt;D3。该方法的代表模型有Lamda Rank、Ada Rank等。得益于NDCG等新的评价方式，文档列表排序在模型训练的过程中可以有效地迭代数据。而且由于输入的单个样本是一组标注好的序列，模型在迭代的过程中也更容易贴近用户需求。文档列表排序方法的缺点也有很多。首先，在理想情况下，其确实更容易保证模型的排序结果贴近用户需求，但是这需要前期大量的标注工作或者说对于使用场景有着明显的限制。其次，由于独立样本复杂，模型的训练成本大于其他两种方法。最后，在现实生活中，我们往往很难能确定地输入单个样本的排序。我们还是要根据具体场景选择合适的排序方法。 1.2排序学习示例Bagging和Boosting算法对比如下图所示。 Boosting示意图 Bagging示意图 1.3搜索和深度学习1.3.1 深度神经网络（Deep Neural Network，DNN）DNN模型的优缺点。作为深度神经网络模型，其对于非线性问题的处理优势不言而喻，而在搜索乃至推荐系统中，由于很多时候特征与训练目标间的关系并不清晰，所以DNN模型对高维度特征的提取更是一大优势。这也是有些场景下，DNN的表现要比树模型、FM家族模型要好的原因。但是DNN本身也有不足，如对类别特征的支持不好，随着类别特征的增多，甚至可能产生维度爆炸的情况；对系统算力的要求较高，因为为了能更好地提取高维度特征，模型的深度可能会使算力不足的问题进一步加剧；可解释性较差，随着隐藏层的增多，评估不同特征对模型的影响变得尤为困难。 1.3.2 深度结构语义模型（Deep Structured Semantic Models，DSSM）深度结构语义模型（Deep Structured Semantic Models，DSSM）在计算语义相关度方面提供了一种思路。它的思想很简单，就是将Query和Title的海量点击曝光日志用DNN(Deep Nature Networks)表达为低维语义向量，并通过余弦相似度来计算两个语义向量的距离，最终训练出语义相似度模型。DSSM模型既可以用来计算两个句子的语义相似度，又可以获得低维语义向量表达。 DSSM模型从下往上可以分为三层：输入层、表示层、匹配层，如下图所示。 （1）输入层 输入层的作用是把一个句子映射到一个向量空间（中文可以处理为单字的形式），并输入到DNN中，然后计算Bi-gram或者Tri-gram。 （2）表示层 表示层采用BOW（Bag of Words）的方式，将整个句子的词不分先后顺序地放到一个袋子里，如下图所示。 用Wi表示第i层的权值矩阵，bi表示第i层的偏差项，则第一个隐藏层向量l1（300维）、第i个隐藏层向量li（300维）、输出向量y（128维）可以分别表示为： （3）匹配层 最后，在匹配层用三角余弦计算Query和Doc的相似度。 DSSM用字向量作为输入既可以减少对切词的依赖，又可以提高模型的范化能力，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用单词向量化的方式（如Word2Vecor的词向量）或者主题模型的方式（如LDA的主题向量）来直接做词映射，再把各个词向量累加或者拼接起来。但由于Word2Vecor和LDA都是无监督训练，会给整个模型引入误差，而DSSM采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。 DSSM的缺点是采用词袋模型（BOW），导致丧失了语序信息和上下文信息，而且采用弱监督、端到端的模型，使预测结果不可控。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第四章-1--搜索系统框架及原理","slug":"SearchAndRec-Chapter04-2","date":"2021-05-09T22:00:37.000Z","updated":"2021-05-09T14:01:56.344Z","comments":true,"path":"2021/05/10/SearchAndRec-Chapter04-2/","link":"","permalink":"http://renxingkai.github.io/2021/05/10/SearchAndRec-Chapter04-2/","excerpt":"","text":"1.文本分析在搜索过程中需要对文本进行处理，比如对查询的分析以及建立索引时对文档内容的分析，我们将这部分内容称作“Query理解”。其主要包括Query预处理、Query纠错、Query扩展、Query归一、联想词、Query分词、意图识别、term重要性分析、敏感Query识别、时效性识别等。如下图所示，用户输入一个Query，为了缓解后端压力，搜索系统会先去Cache中查询Query是否被命中，如果被命中，则直接返回该Query的结构化数据。如果没被命中，就需要后续对Query进行一系列处理。首先是简单的预处理，大小写、全半角、繁简体转化以及对过长的Query进行截断处理，接着可能需要先对Query进行分词，使用分词的Term结果进行错误检测，然后再对Query分词做重要性分析和紧密度分析，对无关紧要的词汇做丢词等处理。有了分词Term及其对应的权重、紧密度信息后，搜索系统可以进行意图识别。意图识别包括模糊意图识别和精准意图识别。除此之外，部分搜索场景还需要对Query进行敏感识别及时效分析等其他处理，以及对前面各部分处理后的结果进行人工干预，解决相应的负例。 1.1查询处理1.1.1搜索查询中的“术”对文档处理主要是一个词法分析的过程。词法分析的过程是将字符串（文档中的文本内容）转换成词条的过程，这些词条可以作为索引词条。因此，词法分析的主要目的是识别文本中的词条。 在对英文进行分词的过程中，除了空格分隔符，还有几种特殊的情况：数字、连字符、标点符号和字母的大小写。数字一般不适合用作索引词条，因为对于数字来说，如果不参考上下文，它没有明确的含义。对于连字符来讲，目前常用的处理方法是首先采用一定的规则选出那些对词义有影响的连字符，然后将其他连字符过滤掉。对于文本来讲，标点符号将被全部去除，但对于那些成为单词一部分的标点符号来讲，一般不可以去除。对于字母大小写，其处理方法一般是将文本中的所有词条转换成大写或者小写，但在某些特殊情况下，需要对大小写进行区分。 对于中文的词法分析，最关键的就是中文分词。在中文分词的过程中，有两大难题：一是歧义识别，所谓歧义是指同样的一句话可能有两种或者多种切分方法；二是新词识别，“新词”的专业术语为“未登录词”，也就是那些在字典中没有收录过的词。 中文分词的方法单字切分就是按照中文一个字、一个字地进行分词。二分法是指每两个字进行一次切分。词库分词是用一个已经建立好的词的集合去匹配目标，当匹配到集合中已经存在的词时，就将其切分出来。 中文分词算法现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 基于字符串匹配的分词方法。它又叫作机械分词方法，是按照一定的策略将待分的字符串与一个充分大的机器词典中的词条进行匹配，若在词典中找到对应字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，基于字符串匹配的分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法、分词与标注相结合的一体化方法。下面介绍两种机械分词方法，即正向最大匹配法和逆向最大匹配法。正向最大匹配法（Forward Maximum Matching Method，FMM）的算法思想是，选取包含6~8个汉字的符号串作为最大符号串，将最大符号串与词典中的单词条目相匹配，如果不能匹配，就削掉一个汉字继续匹配，直到在词典中找到相应的单词为止。其匹配的方向是从左向右。逆向最大匹配法（Backward Maximum Matching Method，BMM）和正向最大匹配算法相似，只是匹配的方向是从右向左，比正向最大匹配的精确度高一些。下图为正向最大匹配和逆向最大匹配的算法示例。 基于理解的分词方法。通过计算机模拟人对句子的理解，达到识别词的效果。其基本思想是在分词的同时进行句法、语义分析，利用句法和语义信息来处理歧义现象。通常，基于理解的分词包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息，以便对分词歧义进行判断，即模拟人理解句子的过程。 基于统计的分词方法。从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。换句话说，字与字相邻共现的频率或概率能够较好地反映词的可信度。我们可以对语料中相邻共现的各个字组合出现的频率进行统计，计算它们的共现信息。共现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成一个词。这种方法只需对语料中的字组出现频率进行统计，不需要词典，因而又叫作无词典分词法或统计取词方法。 1.1.2搜索系统中的“道”上述内容只是搜索查询中的“术”，要想真正理解搜索引擎，还需要深刻理解搜索系统的内核，即搜索系统的“道”。查询理解便是我们需要理解的“道”，它在搜索系统中是一个比较重要的模块。这个模块的主要目的是推断查询，通过提供建议来引导搜索引擎判断出用户的真实意图，并改进查询以获得更好的结果，下图所示。 (1)查询建议 查询建议，也被称为查询下拉建议、查询下拉推荐或者查询自动补全。它是指搜索引擎系统根据用户当前的输入，自动提供一个查询候选列表供用户选择。查询建议在搜索引擎和广告竞价平台中已经是标配的组件。它可以帮助用户明确搜索意图，减少用户的输入并节约搜索时间，提高用户体验。各个搜索系统的查询建议的处理流程基本相同，不同点主要体现在后台的查询候选产生机制上。 查询建议也有一些常用的算法，能够在查询第一步帮助用户获得满意的结果。常见的下拉推荐算法有：1）基于日志的下拉推荐；2）对页面浏览（Page View，PV）数据进行扩展，基于综合指标的下拉推荐；3）基于用户行为的下拉推荐；4）基于Query Session的下拉推荐。下拉推荐的目的和常用算法如下图所示。 全量日志的自动补全法Most Popular Completion，MPC是最常用的基于日志统计信息的方法。如图4-13所示，基于日志的下拉推荐流程主要分为三步：1）在海量的Query日志中，统计一段时间内每个Query的PV和点击数；2）经过相似度计算，得到与用户输入Query相似的候选Query集合；3）在相似Query候选集中，按照Query的PV和点击数进行排序。为提升匹配效率，我们可以通过对历史搜索Query按PV统计量筛选并预处理，然后分别构建前后缀Trie树，以便对输入Query进行前缀及后缀匹配召回。最后对召回的结果进行排序，如果是仅简单按PV量降序排列，可以在Trie树节点中存放PV信息，并用最小堆等结构进行Top N召回。 全量日志的自动补全法方法也存在一些问题：1）对于Top N Query，该推荐方法效果较好，但是对于长尾Query，可能无法挖掘到相似Query，而现实中长尾Query占了很大比例；2）候选Query语义相同，仅仅是词语顺序不同，可能导致推荐位置的浪费；3）推荐Query可能存在质量问题，如一些质量低的Query由于点击或者PV过高而被推荐，导致质量较高的Query不能被展示。 为了改善上述基于日志的下拉推荐存在的问题、增加高质量Query被推荐的机会、减少作弊行为、给出更加合理的Query排序结果，基于综合统计指标的下拉推荐方法被提出，以代替原始的基于日志的下拉推荐方法。基于综合统计指标的下拉推荐方法包括更多维度，如PV、UV、CTR、转化率等，通过多维度对Query排序能够有效防止作弊行为的发生，挖掘到高质量Query。该方法通过逻辑回归将上述指标拟合成一个实数，与基于日志的下拉推荐方法的不同之处在于：1）第一步计算每个Query的综合统计指标，不仅仅是PV值；2）Query综合统计指标不仅考虑了Query的历史PV/点击信息，而且考虑了用户行为信息，使得质量高的Query获得更多的展现机会。 基于综合统计指标的下拉推荐方法解决了高质量Query展现和排序的问题，但该方法还是和基于日志的方法一样，主要依赖Query自身的特征，比如搜索Query和候选Query之间的联系仅仅是两者的前缀相同。这种简单的动态特征没有将搜索Query和候选Query紧密地结合在一起，同时静态特征和动态特征的组合都是基于线性加权的。为了使两者之间建立动态关联，基于CTR的方法被提出。如果搜索Query和候选Query之间的关联越强，它的CTR就会越高，反之则会比较低。CTR预估是通过逻辑回归模型预估Query的CTR来实现的，使用的特征主要有：1）搜索Query和推荐Query的相关特征；2）搜索Query和推荐Query类目的相关特征；3）候选Query综合统计指标的相关特征；4）搜索Query和推荐Query的词性特征；5）搜索Query和推荐Query对应的结果页面特征等。 只依靠短短的Query信息去准确识别用户的意图是不够的，还需要结合用户的一些信息对用户意图进行推测。根据用户的信息建模，比如用户性别、年龄、学历以及兴趣偏好等，结合初排结果再次进行排序。基于用户行为的下拉推荐方法的主要步骤有：1）计算用户和Query的相关个性化特征；2）建立合适的评价体系对这类特征进行权重学习，这里可以使用逻辑回归模型，比如AUC模型。但是一般情况下，用户的某一个场景下的行为信息较少，需要挖掘其他场景下的行为信息作为补充，同时还存在冷启动问题。 另外，点击的URL也可以作为信息源，即使用用户点击的URL对简短的Query信息进行补充和表示。定义一次完整的Query Session包含搜索Query和点击的URL。基于Query Session的下拉推荐方式的主要步骤是：1）将搜索Query以及对应的点击的URL从日志库中提取出来，预处理后聚类；2）给定一个搜索Query，确定所属聚类类别，计算其在这个类别中的排序；3）返回排序靠前的相关联的Query。候选Query的排序由与搜索Query之间的相似度以及所属类别中的支持度决定，该指标通过点击日志计算得到。Query的热度和用户需要的支持度不一定成正比，因此需要将相似度和支持度归一化，线性组合计算Query的排序结果；或者考虑向用户输出两种方法的推荐列表，并让他们调整每种方法的权重。为了计算两个Query之间的相似度，我们会对每个Query中的每个词语向量化。词典是由点击的URL分词后的结果去掉停用词后组成的集合，词语的权重由词语出现的次数和URL点击的次数决定。给定一个Query(q)和一个URL(u)，令Pop(q，u)表示u在q下的热度；Tf(t，u)是词语t在u内出现的次数；q的表示向量为q，q[i]表示Query中的词语在词袋中所处的位置i，其计算公式是： 总和涵盖所有点击的URL。该表示形式通过经典Tf-idf加权中的点击流行度来更改逆文档频率。使用余弦函数计算相似度，如果两个文档的单词出现比例相似（但长度或单词出现顺序可能不同），则认为它们相似。 即使使用同样点击的URL的日志，Query聚类的方法也不尽相同。图4-14表示搜索Query和点击的URL之间的关联，左侧代表查询qi；右侧表示搜索Query对应的点击的URL；Query和URL之间的连接关系eij表示在qi下点击uj；边的权重wij表示整个日志中，在qi下点击uj的总次数。该方法便于寻找相似的Query，两个Query之间共同点击的URL越多，说明两个Query越相似。 单个Query对用户意图识别的信息量是不够的。例如有用户搜索了“小米”，我们并不能判断用户搜索的是小米这个品牌还是食物，但当我们发现用户在搜索“小米”之前还搜索了“智能手机”，就能大致理解用户的意图。所以，除了需要分析用户当前的Query外，对用户Query上下文的分析也会帮助我们理解用户查询意图。我们根据会话数据挖掘用户Query上下文序列，并结合Query的聚类结果构建概念序列后缀树（Concept Sequence Suffix Tree）。当用户提交Query后，推荐系统就可以根据该后缀树快速给出合适的下拉推荐。具体的下拉推荐方案如下图所示。 (2)查询更正 查询更正主要是指Query纠错，也就是对用户在搜索输入时的错误Query进行检测和更正。用户在使用搜索引擎时，可能由于输入法、手误或者理解偏差等造成输入错误，使返回结果不能满足用户需求或者无返回结果。因此，搜索引擎需要对此进行处理，提高搜索的准确率和召回率，为用户提供更好的使用体验。 根据Query中是否包含不在词典中的词语，可以将Query的错误类型分为两种：Non-word和Real-word。Non-word错误一般出现在带英文单词或数字的Query中，不会存在中文错误的情况。所以，中文Query一般只存在Real-word错误，而带英文、数字的Query则可能存在上述两类错误。下图对常见错误类型进行了归类并给出了相应的例子。 Query纠错可以通过噪声信道模型来理解，假设用户原本想输入Qreal，但是经过噪声信道之后，可能输入到搜索引擎中的是Qnoise，对Qnoise进行去噪处理，最大限度地还原为Qdenoise，使得Qdenoise≈Qreal。 已知Qnoise，求解最大可能的Qreal，公式如下： Query纠错一般包括两个子任务：错误检测和错误纠正。其中，错误检测就是识别出错误的位置。对于Non-word类型的错误，我们可以根据词汇是否在维护的词典中进行判断。不过，该方法的效果取决于维护词典的规模和质量。对于Real-word类型的错误，每个词汇都可作为错误候选词。至于错误纠正，即在检测出Query存在错误的基础上对错误部分进行纠正，主要包括纠错候选召回、候选排序选择两个步骤。在进行候选召回时，没有一种策略能覆盖所有错误类型，一般采用多种策略进行多路候选召回，然后在多路候选召回的基础上通过排序模型进行最终的候选排序。在纠正Non-word类型的错误时，搜索系统可以查找词典中与错误词汇最相近的词语。常见的方法有计算最小编辑距离和最大噪声信道概率。在纠正real-word类型的错误时，搜索系统可以从发音和拼写多个角度，查找与错误词汇最相近的词语集合作为拼写建议。常见的方法有计算最大噪声信道概率和分类。 对于英文错误、多/漏字、颠倒错误，搜索系统可以通过编辑距离度量召回。编辑距离表示一个字符串通过插入、删除、替换操作转化为另一个字符串所需的操作次数，例如hapy转化成happy的编辑距离是1。由于搜索Query数量庞大，如果计算Query两两之间的编辑距离，计算量会非常大，因此一般采用启发式策略，比如首字符相同的情况下将长度小于某一值的Query分到一个桶中，计算桶中的Query两两之间的编辑距离。对于上述方式不能够处理的情况，比如顺序颠倒、漏字多字的情况，还可以利用编辑距离满足两边之和大于第三边的特性对多叉树进行剪枝。首先随机选取一个Query作为根节点，然后自顶向下对所有Query构建多叉树，树的边为两个节点Query的编辑距离。给定一个Query，需要找到与其编辑距离小于等于n的所有Query，并自顶向下计算与相应节点Query的编辑距离d，接着只需递归考虑边值在d–n到d+n范围的子树即可。如下图所示，需要查找所有与“十面埋弧”编辑距离小于等于1的Query，由于“十面埋弧”与“十面埋伏”的编辑距离为1，此时只需考虑边值在1–1到1+1范围的子树，因此不用考虑将“十面埋伏怎么样”作为根节点子树。 据统计，英文中80%的拼写错误的编辑距离是1，大多拼写错误的编辑距离小于等于2，基于此可以减少大量不必要的计算。通过最小编辑距离获取拼写建议候选集（Candidate w），再从候选集中选出概率最大的w作为最终的拼写建议，然后基于噪声信道模型，进一步计算候选词w的概率p(w)和在候选词w出现的情况下x的条件概率p(x|w)，通过对语料库统计，即可得到p(w)、p(x|w)。了采用一元词法模型Unigram，还可以推广到二元词法模型Bigram，甚至三元词法模型Trigram及更高阶，以更好地融入上下文信息。 对于等长的拼音字形错误，我们还可以使用HMM模型召回。例如：连一裙→连衣裙，可以将错误Query“连一裙”作为观测序列，正确Query“连衣裙”作为隐藏状态，映射关系可以通过人工整理的同谐音和形近字混淆词表、编辑距离度量召回的相近英文单词以及挖掘好的纠错片段对得到。通过对搜索行为日志统计得到模型参数，然后采用维特比算法对隐藏状态序列矩阵求解最大纠错概率，得到候选纠错序列。下图就是一个HMM模型处理等长拼音字形错误类型的示例。进一步地，我们还可以尝试利用深度学习模型充分挖掘搜索点击行为及搜索上下文来纠错候选召回，如采用Seq2Seq、Transformer、Pointer-Generator Networks等模型进行端到端的生成改写，通过引入注意力、记忆存储等机制以及结合混淆词表进行优化。 1.2 意图理解用户搜索意图的识别是搜索文本分析的重要部分，也是最具挑战的部分。其存在的难点主要有如下几点。1）用户输入Query不规范。由于不同用户对同一事物认知不同，用户在通过自然语言描述时会存在差异，甚至可能会出现Query表达错误和遗漏的情况。2）歧义性和多样性。搜索Query不能明确表达用户真实意图，可能带来歧义；或者用户搜索的内容本身可能有多种含义，比如：“小米”可能是食物，也可能是品牌。 根据用户信息及搜索上下文可实现个性化意图识别，比如针对不同的用户（年龄、性别等），搜索同一个Query的意图可能不一样，用户当前Query的意图可能和上下文Query相关。按照用户意图明确程度，搜索意图识别可以分为精准意图识别和模糊意图识别。 精准意图识别精准意图识别是指用户所表达的意图已经相当明确，可以根据Query锁定一个资源目标。精准意图识别在垂直搜索领域较为常见，以应用商店搜索为例，用户搜索“下载微信”的意图很明确，就是想下载微信App，那么将微信App展示在第一位即可。一般在排序模型拟合较好的情况下对应的精准资源能够排在首位，但是以下情况可能会引起排序不稳定，进而导致对应的精准资源不能排在首位，具体包括：1）长尾Query数据稀疏，模型学习不充分；2）引入用户个性化特征，排序结果因人而异；3）以商业化为导向的排序影响相关性体验。因此，我们需要通过一定策略识别出精准意图并置顶。对于垂直搜索领域，精准意图一般是在给定Query的情况下，找到与其精准对应的item。我们可以通过文本匹配的方式，对&lt;Query，item&gt;对进行精准二分类。常用的分类模型有TextCNN、LSTM以及DSSM等。对于长尾Query且文本完全包含item的情况，由于用户行为量不够丰富，利用分类模型可能无法召回，直接进行文本匹配提取可能存在歧义，因此可以转化成NER任务，通过BiLSTM-CRF等序列标注模型进行item实体识别。另外，针对问答型任务，比如“姚明的身高是多少？”，可以通过召回“姚明”“身高”字样的页面为用户提供答案，但如果能够直接给出这个Query的答案，用户体验会更好。这类问答型任务一般需要结合知识图谱来实现，传统做法是先对Query进行词法、句法以及语义解析，识别出主要实体，再基于这些主题构造相应的查询逻辑表达式，进而去知识库中查询答案。近年来，业内陆续提出将问题和知识库中的候选答案映射成分布式向量进行匹配，以及利用CNN、LSTM、记忆网络等对问题及候选答案建模来解决。 意图分类在进行Query意图分类前，需要先制定一套意图标签用于全面覆盖用户意图需求。这里需要对Query侧和item侧的标签体系进行统一，以便在预测出某个Query的意图标签分布后直接用标签去倒排索引中召回属于这些标签的item。在电商场景下，根据Query识别该商品的类别，按照类别召回该类别下的商品，可以解决Query无结果情况，扩大召回量。由于搜索Query长度较短且意图存在多种可能，因此意图分类可以归为多文本多标签分类任务。在样本选取上，可以通过关联用户搜索行为分布及理解item获得的标签或站点所属行业分类信息等自动构造样本。对于可能存在的样本类别不平衡的问题，我们需要对样本进行上下采样等处理，或采用主动学习等方法进行高效的样本标注。至于模型方面，传统的文本分类主要采用向量空间模型或特征工程表征文本，然后用贝叶斯、SVM、最大熵等机器学习模型进行训练预测。随着Word2vec、GloVe、Fasttext等分布式词向量技术的兴起，传统自然语言处理任务需要做大量特征工程的局面被打破。通过分布式向量对文本进行表示，然后接入其他网络进行端到端分类训练的做法成为主流，如简单又实用的浅层网络模型Fasttext。Fasttext是从Word2vec衍生出来的，其架构和Word2vec架构类似，核心思想是将整篇文档的词及n-gram向量叠加平均后得到文档向量，然后使用文档向量做softmax多分类。相对于Word2vec，Fasttext是在输入层考虑字符级n-gram特征，这在一定程度上解决了未登录词识别问题，并且在输出层支持有监督的任务学习。Fasttext训练简单，且线上接口性能很高，但因为采用相对简单的浅层网络结构，准确率相对较低。为此，我们进一步尝试了一些深度神经网络模型，如：TextRNN、TextCNN、Char-CNN、BiLSTM+Self-Attention、RCNN、C-LSTM、HAN、EntNet、DMN等。这些模型通过CNN/RNN网络结构提炼更高阶的上下文语义特征以及引入注意力机制、记忆存储机制等，有效提高了模型的分类准确率。其实，对于Query短文本分类来说，采用相对简单的TextRNN/TextCNN网络结构就已经能达到较高的准确率。其中，TextRNN通过使用GRU/LSTM编码单元能更好地捕获词序和较长长度的上下文依赖信息，但训练耗时相对较长。TextCNN主要通过不同尺寸的卷积核捕获不同局部窗口内的n-gram组合特征，然后通过max-pooling或kmax-pooling保留变长文本中一个或多个位置的最强特征，并转换为固定长度的向量再做sigmoid/softmax分类。为进一步提高网络性能、加速模型收敛，我们还可以考虑在网络中加入dropout及batch normalize等防过拟合机制，以及在输入层融入Word2vec、GloVe、Fasttext等模型预训练得到的向量。TextCNN在捕获长程依赖信息方面不如TextRNN，但对于长度相对较短的Query的推荐效果相对较好，而且其训练速度及在线接口性能也都比较符合要求。 1.3 其他文本分析方法1.层次聚类 目前，在文本挖掘领域使用最广泛的聚类算法是层次聚类。层次聚类包括凝聚式层次聚类和分裂式层次聚类。凝聚式层次聚类又称为自底向上的层次聚类，是将集合中的每个对象作为一个单独的类别，然后逐渐合并这些类别形成更大的类别，直到满足聚类终止条件为止。分裂式层次聚类又称为自顶向下的层次聚类，它的聚类过程与凝聚式层次聚类相反：首先将集合中的所有对象置于同一个类别，然后对这个类别不断细分，直至达到聚类终止条件。相比凝聚式层次聚类，分裂式层次聚类在分裂过程中所要依据的规则更难确定，并且细节方面的处理过程也更加复杂，因此其适应用范围比较窄。本节采用的是凝聚式层次聚类。 凝聚式层次聚类的基本流程如下。1）将数据集中的每个点都作为一个独立的类别，类间的距离近似等于相应数据点的距离。2）找到聚类最近的两个类别，然后合并这两个类别。3）计算新合并的类别和原来每个类别之间的距离。4）重复第2步和第3步，直到所有的数据都聚到一个类别中，或者满足聚类终止条件（预先设定的类别个数或者距离的阈值）为止。 类别合并的方法有三种，分别是单连接法、全连接法和平均连接法。 单连接法：也叫最短距离法，类间距离用两个类中最近的两个数据点的距离表示。距离作为度量条件，一旦最近的两个类别之间的距离小于预先设定的距离阈值，算法流程结束。 全连接法：也叫最长距离法。与单连接法选两个类中最近的两个数据点的距离作为类间距离相反，它选择将两个类中距离最远的两个数据点的距离作为类间距离。 平均连接法：无论是单连接法还是全连接法都容易受到极端值的影响，造成聚类的不稳定。与上述两种方法不同，平均连接法选取两个类别中所有对象的平均距离作为类间距离。该方法更加合理，但计算较复杂。 2.K均值聚类 K均值聚类是基于样本集合划分的聚类方法，其事先确定样本集合划分的类别数k，将所有样本分到这k个类中，目标是每个样本到所属类别的中心的距离最小。由于每个样本只能属于一个类别，所以K均值聚类是硬聚类。 K均值聚类的基本流程如下。1）从样本集中随机选取k个样本点作为初始聚类中心。2）计算聚类对象到聚类中心的距离，将每个样本指派到与其最近的聚类中心的类中。3）计算当前各个类中的样本的均值，作为新的聚类中心。4）重复第2步和第3步，直到迭代收敛或者满足聚类终止的条件为止。 K均值聚类属于启发式方法，不能保证收敛到全局最优，且初始聚类中心的选择会直接影响聚类的结果，因此在选择初始聚类中心时可以考虑用层次聚类对样本进行聚类，得到k个类时停止，然后从每个类中选取一个与中心聚类最近的样本点。这里类别数k需要事先指定，但实际中往往最合适的k值是未知的，所以需要不断尝试k值聚类，检验聚类的结果，推断出最优的k值。聚类结果的质量可以使用类的平均直径来衡量。一般情况下，类别数变小时，平均直径会增加；类别数超过某个值，且平均直径不变时，这时的k值就是最优值。 3.LDA主题模型 主题模型广泛应用于自然语言处理领域，用于挖掘文本中潜在的主题信息。它是描述主题信息的一系列数学模型，本质是形式化地刻画出主题信息。下面介绍概率主题模型的发展历程——从潜在语义分析模型LSA到概率潜在语义分析模型PLSA，最后讲述目前应用最广泛的LDA主题模型。 （1）LSA模型 在向量空间模型（Vector Space Model，VSM）中，各个单词之间是完全相互独立的，所以很难解决一义多词和一词多义问题。因此，Deerwester等人提出了潜在语义分析（Latent Semantic Analysis，LSA）模型。LSA和VSM模型的相同之处是，使用向量表示词语和文本之间的关系；不同之处是，LSA将词语和文档映射到潜在语义空间，提出单词和文本之间存在主题层这一语义关系，这比以往将单词视为文本的唯一表示元素有了巨大的进步。LSA模型结构如下图所示。 上图中，A矩阵是一个稀疏矩阵，行表示词汇，列表示文档，矩阵中的元素是词汇在文档中出现的次数；等式右边的三个矩阵也有清晰的物理含义。X矩阵是对词进行分类的结果，行表示词汇，列表示语义类别，矩阵中的元素表示词汇在对应类别下的相关性；Y矩阵是对文档的分类结果，行表示主题，列表示文档，矩阵中的元素表示每篇文档在不同主题中的相关性；B矩阵则表示词的类和文档的类之间的相关性。 LSA模型利用奇异值分解的方式，截取了最大的前K个特征值，从而完成了文本从高维单词空间到低维主题空间的降维映射，这种方式对语料集中噪声过滤起到了重要作用。此外，在低维的主题空间中，语义关系相似的单词的主题相同或者相似，所以该模型改善了原模型中一义多词的缺陷。然而，改善后的LSA模型还是无法完全解决原模型中一词多义的问题，还具有计算量过大的缺点，所以后来有研究者提出了用概率潜在语义分析模型来弥补LSA模型的不足。 （2）PLSA模型Hofmann基于LSA模型构建了概率潜在语义分析（Probability Latent Semantic Cnalysis，PLSA）模型。该模型利用文档、主题和单词三层联合发生的概率获得与文本最贴切的语义关系，并采用迭代算法（一般是EM算法）推断文本与主题之间的语义关系，然后采用主题的语义信息表示文本信息。PLSA模型结构如下图所示。 其中，方框代表重复次数，d表示文档，z代表主题，w代表单词，M代表该语料库中文档的总数，N是当前操作的文档所包含的单词数目。PLSA模型不仅达到了降维的目的，而且刻画出了主题层和单词层的关系。但该模型仍不是一个完备的概率生成模型，因为它没有使用概率来描述文本。并且PLSA模型中的模型参数和语料集中的文本数量呈正相关，这就造成了模型的过拟合。 （3）LDA模型 在2003年，Blei等人基于PLSA模型构建了一种三层贝叶斯结构的模型——隐狄利克雷分布（Latent Dirichlet Allocation，LDA）模型。LDA模型是一个真正意义上完备的概率生成模型，是在PLSA模型的基础上扩展了狄利克雷先验超参数α和β，构建出文档层、主题层、单词层的三层贝叶斯结构模型，解决了PLSA模型中过拟合的问题。LDA模型在文本——主题维度中利用狄利克雷分布来完成抽样，并且一致的先验超参数使模型参数不会因语料集中文本数量的改变而受到影响，具有良好的泛化能力。此外，LDA模型建立在严格的贝叶斯理论基础之上，因此具有良好的扩展性。目前LDA模型已广泛地运用于科学研究和实际生产中。 LDA模型假设总体语料库中的所有文本都服从主题空间的多项分布，同时每个主题都服从词项空间的多项分布。LDA模型结构如下图所示。 上图中，θ和φ分别表示文本——主题分布与主题——单词分布，其先验分布都是狄利克雷分布，而α和β则分别为其狄利克雷分布的参数，也称为超参数。一个语料库D中共有M篇文档，每篇文档d由N个数量不定的单词w组成。此外，每篇文档由K个主题的多项分布表示，而每个主题z又由V个单词的多项分布表示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第四章-2--搜索系统框架及原理","slug":"SearchAndRec-Chapter04-1","date":"2021-05-08T15:50:14.000Z","updated":"2021-05-09T14:02:33.722Z","comments":true,"path":"2021/05/08/SearchAndRec-Chapter04-1/","link":"","permalink":"http://renxingkai.github.io/2021/05/08/SearchAndRec-Chapter04-1/","excerpt":"","text":"1.搜索系统的框架1.1基本框架 搜索系统通常由信息收集、信息存储、信息扩展及搜索计算4部分组成。搜索系统的具体结构上图所示。 （1）信息收集该部分包括但不限于爬虫和线下导入。对于网络爬虫部分，该组件的主要功能是收集全网实时/延时数据（这取决于搜索引擎的应用场景），并对收集的数据进行简单处理，如去重、停用敏感信息、过滤垃圾信息以及生成信息存储部分可以接收的数据，以便存储。线下导入的信息有时是作为爬虫信息的补充，如作为对缺乏规范描述的定义、定理等进行补充的备检信息；有时是作为推理或关系信息的主要依据，如作为对网络爬虫收集的信息进行关系处理的依据。 （2）信息存储该部分包括但不限于文件信息保存及索引保存等。信息存储的主要功能是提供高效的、泛化的、文档间可联动的搜索内容。所以在文件信息保存上，有时会因为场景的原因，将所有文件内容放置在内存中，所以会建立庞大的倒排索引和并发集群以提高搜索的响应效率。对于推理性搜索，为了提高搜索的响应效率，存储组件需要对应用于推理的节点（关键词、字段或者短语描述等）进行特殊处理。 （3）信息扩展除响应效率外，为了提高搜索的准确性，有时我们需要对搜索系统召回的内容进行相应扩展。扩展的目的是尽量避免无结果的搜索。在信息扩展部分，我们可以在爬虫程序中定义文档间关系，甚至定义推理逻辑实现，在信息存储部分可以屏蔽停用词、定义同义词，并在建立索引的时候就支持泛化能力。 （4）搜索计算为了理解搜索输入（通常称为Query）和召回排序，我们需要对Query进行生成和解析并提供召回的评分策略，以便对结果进行排序。 生成Query：搜索系统的输入通常是人类自然语言的输入，其表达形式包括但不限于关键词、短语、句子、文章段落，甚至图片等。所以，在生成Query时，通常会加入意图理解模块，以尽可能理解用户当前检索内容的真实意图。同时，受制于人类语言表达的缺陷，在自然语境下所表述的内容被计算机理解时可能会产生歧义，这也是意图理解模块试图解决的问题。通过意图理解模块，我们通常会得到当前搜索内容的主题或者检索要素。对于检索要素，可能需要进行拓展，这部分拓展主要依赖当前用户的行为和近期全网热度等。最后将整理的全部检索要素生成计算组件可理解的Query并输入计算模块。 解析Query：计算模块将搜索信息从信息存储组件中按照既定的评分标准排序召回。首先，搜索系统需要将Query导入信息延展组件进行拓展。信息拓展的主要目的是加大召回的覆盖率和提高召回的准确度。这里常用的手段与NLP技术相关，除了可以对最基本的同义词、近义词进行打分，还可以对所有相关检索要素的重要程度进行打分，比如检索要素是Java工程师，显然Java的重要程度远远大于工程师，此时就需要对这两个不同的检索要素赋予不同的权重。 信息存储组件负责实时或延时地将搜集到的数据存储起来。对于数据的时效性，我们需要考虑搜索系统的性能及应用场景，而最终选择采取哪种方式进行数据存储，则是由读者实际工作决定。在存储信息时，为了保证信息的高效存储，通用的存储方式有内存存储与索引存储，具体的构建方式会在相关章节详细介绍。在接收到Query之后，信息存储组件会将结果返回到信息延展组件。返回的信息包括但不限于：单个召回结果中检索要素命中的位置信息、单个召回结果中检索要素命中的频次信息、整体召回结果的统计与分布等。这里之所以需要返回大量的信息，是因为我们在考虑搜索系统整体准确度的时候，还需要对结果进行排序。而排序阶段的特征数据或者评分数据主要由信息存储组件提供。所以，有时候信息存储组件返回的数据量可能过大。在结构上，一些搜索系统倾向于将部分信息延展组件与信息存储组件甚至计算组件整合到一起，从而在返回结果的同时结束部分计算结果的统计。部分情况下，信息存储组件也会将部分结果返回至信息延展组件，主要是考虑到更为精确、细粒度的搜索结果可能导致召回不足。这就需要在信息延展组件中，对可能发生的召回不足的情况制定补救措施，重复进行搜索、收集搜索结果的流程，直至召回满足或无法召回为止。最后将搜索结果返回至计算组件，进行最终的排序并将结果展示给用户。 1.2搜索引擎的工作方式搜索引擎分为4部分，搜索器、索引器、检索器及用户接口，如下图所示。 搜索器。搜索器的功能是日夜不停地在互联网中漫游，搜集信息。它要尽可能多、尽可能快地搜集各种类型的新信息，还要定期更新已经搜集的旧信息，以免出现死链。搜索器有两种搜集信息的策略：第一种策略是从一个起始URL集合开始，顺着这些URL中的超链接（Hyperlink），以宽度优先、深度优先或启发式循环地在互联网中搜集信息。它会沿着任何一个网页中的URL集合“爬”到其他网页，重复这个过程，并把搜集到的所有网页存储起来。第二种策略是按照域名、IP地址划分Web空间，每个搜索器负责一个子空间的穷尽搜索。 索引器。索引器的功能是理解搜索器所搜索的信息，通过分析索引程序对收集的网页进行分析，提取相关网页信息（包括网页所在URL、编码类型、页面内容包含的关键词、关键词位置、生成时间、大小、与其他网页的链接关系等），根据相关度算法进行大量复杂计算，得到每一个页面的页面内容及超链接中每一个关键词与页面内容的相关度（或重要性），然后用这些相关信息建立网页索引数据库。 检索器。检索器的功能是根据用户查询在索引库中快速检出文档，对文档内容与查询的相关度进行评估，并对将要输出的结果进行排序，实现某种用户相关性反馈机制。检索主要过程如下：检索器对用户接口提出的查询要求进行递归分析，在用户接口中一般采用基本语法来设置检索条件。 用户接口。用户接口的作用是输入用户查询、显示查询结果、提供用户相关性反馈机制。其主要目的是方便用户使用搜索引擎，高效率、多方式地从搜索引擎中得到有效、及时的信息。用户接口的设计和实现使用了人机交互的理论和方法，以充分适应人类的思维习惯。用户输入接口可以分为简单接口和复杂接口两种。 2.数据收集及预处理2.1爬虫网络爬虫是搜索系统中很关键也很基础的组件，负责数据收集。通用的爬虫框架如下图所示。其工作原理是首先从互联网网页中选出一部分质量较高的网页的链接作为种子URL，把这些种子URL放到待抓取URL队列中，由爬虫从待抓取URL队列中依次读取，并通过DNS解析URL，把链接地址转化为网站服务器对应的IP地址。然后将IP地址和网页相对路径名称交给网页下载器，由网页下载器把网页内容下载到本地。这样做一方面可以将页面内容存储到网页库中，等待索引建立等后续处理；另一方面将下载页面的URL放到已经抓取的URL队列中，对已经爬取的页面进行记录，可以防止重复爬取。对于刚下载的网页，抽取其中的URL链接，并检查已抓取的URL队列，将没有抓取过的URL放入待抓取URL队列中，不断循环上述步骤直至待抓取的URL队列为空，即爬虫系统完成一轮完整的爬取流程。 该框架适用于绝大多数的爬虫系统，但不能代表全部。根据应用场景的不同，网络爬虫大概分为以下4种：1）通用型网络爬虫，负责爬取全网的资源，一般适用于大型搜索引擎；2）批量型网络爬虫，按照预先设置好的抓取目标和范围爬取资源，当爬虫完成目标后，就停止爬取；3）增量型网络爬虫，针对互联网中有变化的网页，比如新增、修改或者删除的网页，及时响应更新到本地网页库中；4）垂直型网络爬虫，关注特定主题或者特定行业的网页，难点是如何识别网页内容是否为特定主题或行业。 优秀的爬虫应该具备的特性主要有3点。1）高性能。此处的性能主要是指网页的下载速度，常用的评估爬虫性能的方式是以爬虫每秒能够下载的网页数量为指标，单位时间内下载的网页数量越多，爬虫的性能越高。2）健壮性。爬虫在遇到突发情况时，要能够做出正确的处理。当服务器宕机时，健壮的爬虫应该能够在服务器重启后，恢复之前抓取的内容和数据结构。3）友好性。友好性主要包括两个方面：一方面是保护网站私密性；另一方面是减少被抓取网站的网络负载。对于部分网站所有者不想被爬取的网页，网站所有者需要设置协议告诉爬虫哪些内容不能够抓取。常用的方法有：设置爬虫禁抓协议和网页禁抓标记。爬虫禁抓协议是指由网站所有者生成一个文件robot.txt，将其放在网站服务器根目录下，这个文件指明了哪些目录下的文件不允许抓取。友好的爬虫在抓取该网站的内容之前，应该先读取robot.txt文件并遵守协议。如果是某个网站不想被抓取，可以使用网页禁抓标记，即在网页的HTML代码里加入meta name=“robots”标记，其中content字段指出允许或者不允许爬虫做哪些行为，主要分为两种情形：一种是告诉爬虫不要索引该网页内容，以noindex标记；另一种是告诉爬虫不要抓取网页包含的链接，以nofollow标记。 评估爬虫的质量标准一般有三个：抓取网页的覆盖率、抓取网页的时效性和抓取网页的重要性。1）覆盖率，是指抓取网页的数量占互联网内网页总数量的比例，比例越高，覆盖率越高。2）时效性，是指爬虫对修改、删除的网页的反应速度，尤其是针对已经过期的网页，及时更新网页库。3）重要性直接影响搜索的准确率。 上图中待抓取的URL序列在爬虫系统中是关键部分。那么，URL序列顺序是如何确定的呢？将新下载页面中包含的链接直接追加到队列末尾，即宽度优先遍历策略，这是一种常见方式，但并不是唯一方式，还有非完全Pagerank策略以及大站优先策略。1）宽度优先遍历策略虽然没有明显地考虑网页重要性，但实验表明该方法效果很好，对很多链入的网页较为友好，并且链入的网页质量也较高。2）非完全Pagerank策略。Pagerank是一种著名的链接分析方法，最早应用于网页重要度排序，因此可以利用其对URL进行排序。由于其是一个全局算法，而爬虫下载的网页只是部分，所以由其计算的得分不一定可靠，但我们仍然可以借鉴它的思想：将已经下载的网页和待抓取的URL集中起来形成网页集合，在该集合内计算Pagerank，计算完成后，按照结果对待抓取的URL排序，然后依次抓取。3）大站优先策略。以网站为单位衡量网页的重要性，我们认为大站往往是知名企业，其网站质量一般比较高，所以一般优先搜索大站。 上文讲了如何对待抓取的URL排序，我们还需要考虑如何对已抓取的内容快速更新。常见的策略有：历史参考策略、用户体验策略和聚类抽样策略。1）历史参考策略。假设过去更新频繁的网页以后更新也会频繁，所以可以通过其历史更新情况，预估网页何时更新。2）用户体验策略。由于用户一般只对排名靠前的网页感兴趣，所以即使网页过期，如果不影响用户体验，那么晚些更新也是可以的。所以，判断一个网页什么时候更新，取决于网页的内容变化给排序结果带来的影响。3）聚类抽样策略。前两种策略都比较依赖网页历史信息，但在现实中，保存历史信息会增加搜索引擎负担，同时首次抓取的网页并不存在历史信息，因此以上两种方法都有缺陷。聚类抽样策略认为：网页具有一些属性，如网页内容、链接深度以及Pagerank值等，根据这些属性可以预测更新周期，而且，具有相似属性的网页也具有相似的更新周期，因此可以根据这些属性进行网页归类。同一类别的网页具有相同的更新频率。 2.2数据清洗爬虫在下载了网页之后，需要对这些数据进行清洗，其中最关键的部分就是对网页的去重处理（据统计，完全相同的页面大约占全部页面的20%）。处理相似网页的好处：1）可以节省存储空间，留出充足的空间存储有效网页；2）通过对以往数据的分析可以预先发现重复网页，在之后的网页收集中避开这些网页，提高网页收集效率；3）如果用户点击了死链接，可以将用户引导到一个内容相同的页面，进而有效提高用户体验。 在实际工作中，网页相似检验一般是在爬虫阶段完成的，具体流程如下图所示。 通用的网页去重流程如下图所示。对于给定的文档，首先通过一定的特征抽取方法，从文档中抽取一系列表示文档主旨的特征集合，之后通过算法直接查找相似文档。对于搜索引擎中数以万计的文档，算法的执行速度是需要重点关注的，因此很多高效的算法会在特征集合的基础上，对信息进一步压缩，如采用信息指纹相关算法将特征集合压缩成新的数据集合。（新的数据集合包含的元素数量远小于特征集合的数量，但同时也造成了信息丢失，所以需要衡量压缩率和准确度之间的平衡。）把文档压缩成信息指纹后，就可以进行相似度计算了。 经实验证明，SimHash算法是应用最广泛且表现最好的去重算法之一，是一种局部敏感哈希框架的实现特例。该算法流行的原因是：两个文档越相似，其对应的两个哈希值也就越接近，且哈希值的计算明显比文本计算快很多，同时也节省空间。 SimHash算法步骤如下。 对需要判断的文本分词形成该文本的特征单词，然后生成去掉噪声词的单词序列，并为每个词加上权重。 通过哈希函数将每个特征单词映射成固定长度的二进制表示。 利用权重改写特征的二进制向量，将权重融入向量中，形成一个实数向量。假设某个特征单词的权重是5，对二进制向量进行改写：若比特位数值是1，则改写成5；若是0，则改写成–5。改写完的特征累加后获得一个表示文档整体的实数向量。 再次将实数向量转换成二进制向量，规则如下：如果对应位置的数值大于0，设成1；如果小于等于0，则设置成0。得到两个文档的二进制数值后，利用海明距离来计算文档的相似度。 具体例子可以看下图。 2.3存储空间及分布设计搜索系统中最重要的数据结构是索引结构，索引结构越合理意味着查询越快捷。设想如果我们直接对一篇篇文本进行扫描，费时费力。如果我们建一个前向索引，依然是依次扫描每一个文档，对于多次出现的字符串不一一扫描，对于同一文档内的字符串查找采用二分查找的方式，这样可加快匹配速度，但这远远不够，还需要一个倒排索引的数据结构，让查询面向词和文档集，使搜索任务更快捷和简单。对于单个查询词，搜索就是字典查找的过程，不需要扫描所有文档。倒排索引的过程如下图所示。 这里说明一下排序过程。排序就是扫描每篇文档产生的文档号、单词号、出现位置这个三元组。按照单词号重新排序，单词号相同则按照文档号排序，单词号和文档号都相同则按照出现位置排序。 Trie树，又称单词查找树或键树，是一种树形结构，也是哈希树的变种形式。其典型应用是统计和排序大量的字符串，但不限于字符串。Trie树的核心思想是用空间换时间，利用字符串的公共前缀来降低查询时间的开销，以达到提高效率的目的。其包含三个特性：1）根节点不包含字符，除根节点以外，每一个节点都只包含一个字符；2）从根节点到某一个节点，路径上经过的字符连接起来为该节点对应的字符串；3）每个节点的所有子节点包含的字符都不相同。字符串and，as，bee，bus，cn，com构建的Trie树如下图所示。 在上图所示的Trie树中，root表示根节点，不代表任何字符，从第二层开始，白色表示分支节点，灰色表示叶子节点。从根节点到叶子节点，把路径上经过的字符连接起来，会构成一个词。而叶子节点内的数字代表该词在字典中所处的链路（字典中有多少个词就有多少条链路），具有共同前缀的链路称为串。树中的词只可共用前缀，不能共用词的其他部分。树中的任何一个完整词，一定是从根节点开始到叶子节点结束。所以，检索是从根节点开始到叶子节点结束，这种检索方式由于公共前缀的词都在同一个串中，能够大幅度缩小查询词汇的范围，同时从一个字符到下一个字符比对，不需要遍历该节点下所有子节点。因此，Trie树的时间复杂度仅和检索词的长度m有关：O(m)。综上可知，Trie树主要利用词的公共前缀缩小查词范围，通过状态间的映射关系避免所有字符的遍历，从而达到高效检索的目的。这种优势同样是需要付出代价的：1）由于结构需要记录更多的信息，因此Trie树的实现稍显复杂；2）Trie树词典不仅需要记录词，还需要记录字符之间、词之间的相关信息，因此在构建字典时必须对每个词和字逐一进行处理，而这无疑会减慢词典的构建速度；3）公共前缀虽然可以减少一定的存储空间，但Trie树相比普通字典还需表达词、字之间的各种关系，实现更加复杂，因此实际空间消耗相对更大。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第三章--知识图谱","slug":"SearchAndRec-Chapter03","date":"2021-04-26T21:44:20.000Z","updated":"2021-05-14T07:14:03.235Z","comments":true,"path":"2021/04/27/SearchAndRec-Chapter03/","link":"","permalink":"http://renxingkai.github.io/2021/04/27/SearchAndRec-Chapter03/","excerpt":"","text":"仅用于学习!!! 1.介绍搜索引擎是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，将用户检索到的相关信息展示给用户，为用户提供检索服务。搜索引擎包括4个接口，分别是搜索器、索引器、检索器和用户接口。 搜索器的功能是在互联网中漫游，负责发现和搜集信息。 索引器的功能是理解搜索器所搜索的信息，从中抽取出索引项，输出用于表示文档以及生成文档库的索引表。 检索器的功能是根据用户的查询在索引库中快速检出文档，并进行文档与查询的相关度评价，对将要输出的结果进行排序，实现某种用户相关性反馈机制。 用户接口的功能是输入用户查询、显示查询结果、提供用户相关性反馈机制。 具体的搜索引擎架构示意图如图2-1所示。 2.搜索引擎的分类搜索引擎可以分为以下4类：全文搜索引擎、元搜索引擎、垂直搜索引擎和目录搜索引擎。下面对这4类搜索引擎进行具体介绍。 全文搜索引擎。计算机通过扫描文章中的每个词，对每个词建立索引，记录词汇在文章中出现的次数和位置信息。当用户进行查询时，计算机按照事先建立好的索引进行查找，并将结果反馈给用户。按照数据结构的不同，全文搜索可以分为结构化数据搜索和非结构化数据搜索。对于结构化数据，全文搜索一般是通过关系型数据库的方式进行存储和搜索，也可以建立索引。对于非结构化数据，全文搜索主要有两种方法：顺序扫描和全文检索。顺序扫描，顾名思义，按照顺序查询特定的关键字，这种方式耗时且低效；全文检索需要提取关键字并建立索引，因此，搜索到的信息过于庞杂，用户需要逐一浏览并甄别所需信息。在用户没有明确检索意图情况下，全文检索方式效率稍显不足。Google和百度都是典型的全文搜索引擎。 元搜索引擎。按照功能划分，搜索引擎可以分为元搜索引擎（Meta Search Engine）和独立搜索引擎（Independent Search Engine）。元搜索引擎是一种调用其他独立搜索引擎的搜索引擎，其能对多个独立搜索引擎进行整合、调用并优化结果。独立搜索引擎主要由网络爬虫、索引、链接分析和排序等部分组成；元搜索引擎由请求提交代理、检索接口代理、结果显示代理三部分组成，不需要维护庞大的索引数据库，也不需要爬取网页。元搜索引擎具体实现逻辑如图2-2所示。请求提交代理就是将请求分发给独立搜索引擎。元搜索引擎可以按照用户需求和偏好请求实际需要调用的独立搜索引擎，该方式能够有效提升用户查询的准确率和响应效率。检索接口代理是将查询内容转化成独立搜索引擎能够接受的模式，并且保证不会丢失必需的语义信息。结果显示代理是元搜索引擎按照用户的需求采用不同的排序方式对结果进行去重、排序。元搜索引擎常用的排序方式有：相关度排序、时间排序、搜索引擎排序等。元搜索引擎的整体工作流程如下：用户通过网络访问元搜索引擎并向服务器发出查询，服务器接收到查询内容后，先访问结果数据库，查询近期记录中是否存在相同的查询，如果存在，返回结果；如果没有，将查询进行处理后分发到多个独立搜索引擎，并集中各搜索引擎的查询结果，结合排序方式对结果进行排序，生成最终结果并返给用户，同时保存现有结果到数据库中，以备下次查询使用。保存的查询结果有一定的生存期，超过一定时间的记录就会被删除，以保证查询结果的时效性。 垂直搜索引擎。垂直搜索引擎是针对某个行业的专业搜索引擎，是搜索引擎的细分和延伸，对特定人群、特定领域、特殊需求提供服务。它的特点是专业、精确和深入。垂直搜索引擎将搜索范围缩小到极具针对性的具体信息。垂直搜索引擎的结构与通用搜索系统类似，主要由三部分构成：爬虫、索引和搜索。但垂直搜索的表现方式与Google、百度等搜索引擎在定位、内容、用户等方面存在一定的差异，所以它不是简单的行业搜索引擎。用户使用通用搜索引擎时，通常是通过关键字进行搜索，该搜索方式一般是语义上的搜索，返回的结果倾向于文章、新闻等，即相关知识。垂直搜索的关键字搜索是放到一个行业知识的上下文中，返回的结果是消息、条目。对于有购房需求的人来说，他们希望得到的信息是供求信息而不是关于房子的文章和新闻。 目录搜索引擎。目录搜索引擎是网站常用的搜索方式，类似于书本章节目录。该搜索方式是对网站信息整合处理并分目录呈现给用户，整合处理的过程一般需要人工维护，更新速度较慢，而且用户需要事先了解网站的基本内容，熟悉主要模块，所以应用场景越来越少。 3.推荐系统协同过滤目前，基于协同过滤的推荐是推荐系统中应用最广泛、最有效的推荐策略。它于20世纪90年代出现，促进了推荐系统的发展。协同过滤的基本思想是聚类。比如，如果周围很多朋友选择了某种商品，那么自己大概率也会选择该商品；或者用户选择了某种商品，当看到类似商品且其他人对该商品评价很高时，则购买这个商品的概率就会很高。协同过滤又分为三种：基于用户的协同过滤、基于项目的协同过滤和基于模型的协同过滤。 1）基于用户的协同过滤的基本思想是首先找到与目标用户兴趣相似的用户集合，然后找到这个集合中用户喜欢并且没有听说过的物品推荐给目标用户。下图是基于用户的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户C喜欢商品A、商品C和商品D，用户A和用户C具有相似的兴趣爱好，因此把商品D推荐给用户A。 2）基于项目的协同过滤的基本思想是基于所有用户对推荐对象的评价的推荐策略。如果大部分用户对一些推荐对象的评分较为相似，那么当前用户对这些推荐对象的评价也相似。然后，将相似推荐对象中用户未进行评价的商品推荐给用户。总之，基于项目的协同过滤就是根据用户对推荐对象的评价，发现对象间的相似度，根据用户的历史偏好将类似的商品推荐给该用户。图2-8是基于项目的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户B喜欢商品A、商品B和商品C，用户C喜欢商品A，通过这些用户的喜好可以判定商品A和商品C相似，喜欢商品A的用户同时也喜欢商品C，因此给喜欢商品A的用户C也推荐了商品C。 3）基于模型的协同过滤的基本思想是基于样本用户的喜好信息训练一个推荐模型，然后根据实时的用户喜好信息进行推荐。其和上述两种协同推荐的不同点在于先对已有数据应用统计和机器学习的方法得到模型，再进行预测。常用的方法有机器学习方法、统计模型、贝叶斯模型和线性回归模型等。 基于协同过滤推荐的优点有：1）可以使用在复杂的非结构化对象上；2）能够发现用户新的兴趣爱好，给用户带来惊喜；3）以用户为中心的自动推荐，随着用户数量的增加，用户体验也会越来越好。缺点在于：1）存在冷启动问题，即在没有大量用户数据的情况下，用户可能不满意获得的推荐结果；2）存在稀疏性问题，即用户大量增长的同时，评价差异性会越来越大，推荐对象也越来越多，导致大量的推荐对象没有经过用户评价，部分用户无法获得推荐结果，部分推荐对象无法被推荐。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第二章--搜索系统","slug":"SearchAndRec-Chapter02","date":"2021-04-26T21:44:20.000Z","updated":"2021-04-26T13:51:58.043Z","comments":true,"path":"2021/04/27/SearchAndRec-Chapter02/","link":"","permalink":"http://renxingkai.github.io/2021/04/27/SearchAndRec-Chapter02/","excerpt":"","text":"仅用于学习!!! 1.介绍搜索引擎是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，将用户检索到的相关信息展示给用户，为用户提供检索服务。搜索引擎包括4个接口，分别是搜索器、索引器、检索器和用户接口。 搜索器的功能是在互联网中漫游，负责发现和搜集信息。 索引器的功能是理解搜索器所搜索的信息，从中抽取出索引项，输出用于表示文档以及生成文档库的索引表。 检索器的功能是根据用户的查询在索引库中快速检出文档，并进行文档与查询的相关度评价，对将要输出的结果进行排序，实现某种用户相关性反馈机制。 用户接口的功能是输入用户查询、显示查询结果、提供用户相关性反馈机制。 具体的搜索引擎架构示意图如图2-1所示。 2.搜索引擎的分类搜索引擎可以分为以下4类：全文搜索引擎、元搜索引擎、垂直搜索引擎和目录搜索引擎。下面对这4类搜索引擎进行具体介绍。 全文搜索引擎。计算机通过扫描文章中的每个词，对每个词建立索引，记录词汇在文章中出现的次数和位置信息。当用户进行查询时，计算机按照事先建立好的索引进行查找，并将结果反馈给用户。按照数据结构的不同，全文搜索可以分为结构化数据搜索和非结构化数据搜索。对于结构化数据，全文搜索一般是通过关系型数据库的方式进行存储和搜索，也可以建立索引。对于非结构化数据，全文搜索主要有两种方法：顺序扫描和全文检索。顺序扫描，顾名思义，按照顺序查询特定的关键字，这种方式耗时且低效；全文检索需要提取关键字并建立索引，因此，搜索到的信息过于庞杂，用户需要逐一浏览并甄别所需信息。在用户没有明确检索意图情况下，全文检索方式效率稍显不足。Google和百度都是典型的全文搜索引擎。 元搜索引擎。按照功能划分，搜索引擎可以分为元搜索引擎（Meta Search Engine）和独立搜索引擎（Independent Search Engine）。元搜索引擎是一种调用其他独立搜索引擎的搜索引擎，其能对多个独立搜索引擎进行整合、调用并优化结果。独立搜索引擎主要由网络爬虫、索引、链接分析和排序等部分组成；元搜索引擎由请求提交代理、检索接口代理、结果显示代理三部分组成，不需要维护庞大的索引数据库，也不需要爬取网页。元搜索引擎具体实现逻辑如图2-2所示。请求提交代理就是将请求分发给独立搜索引擎。元搜索引擎可以按照用户需求和偏好请求实际需要调用的独立搜索引擎，该方式能够有效提升用户查询的准确率和响应效率。检索接口代理是将查询内容转化成独立搜索引擎能够接受的模式，并且保证不会丢失必需的语义信息。结果显示代理是元搜索引擎按照用户的需求采用不同的排序方式对结果进行去重、排序。元搜索引擎常用的排序方式有：相关度排序、时间排序、搜索引擎排序等。元搜索引擎的整体工作流程如下：用户通过网络访问元搜索引擎并向服务器发出查询，服务器接收到查询内容后，先访问结果数据库，查询近期记录中是否存在相同的查询，如果存在，返回结果；如果没有，将查询进行处理后分发到多个独立搜索引擎，并集中各搜索引擎的查询结果，结合排序方式对结果进行排序，生成最终结果并返给用户，同时保存现有结果到数据库中，以备下次查询使用。保存的查询结果有一定的生存期，超过一定时间的记录就会被删除，以保证查询结果的时效性。 垂直搜索引擎。垂直搜索引擎是针对某个行业的专业搜索引擎，是搜索引擎的细分和延伸，对特定人群、特定领域、特殊需求提供服务。它的特点是专业、精确和深入。垂直搜索引擎将搜索范围缩小到极具针对性的具体信息。垂直搜索引擎的结构与通用搜索系统类似，主要由三部分构成：爬虫、索引和搜索。但垂直搜索的表现方式与Google、百度等搜索引擎在定位、内容、用户等方面存在一定的差异，所以它不是简单的行业搜索引擎。用户使用通用搜索引擎时，通常是通过关键字进行搜索，该搜索方式一般是语义上的搜索，返回的结果倾向于文章、新闻等，即相关知识。垂直搜索的关键字搜索是放到一个行业知识的上下文中，返回的结果是消息、条目。对于有购房需求的人来说，他们希望得到的信息是供求信息而不是关于房子的文章和新闻。 目录搜索引擎。目录搜索引擎是网站常用的搜索方式，类似于书本章节目录。该搜索方式是对网站信息整合处理并分目录呈现给用户，整合处理的过程一般需要人工维护，更新速度较慢，而且用户需要事先了解网站的基本内容，熟悉主要模块，所以应用场景越来越少。 3.推荐系统协同过滤目前，基于协同过滤的推荐是推荐系统中应用最广泛、最有效的推荐策略。它于20世纪90年代出现，促进了推荐系统的发展。协同过滤的基本思想是聚类。比如，如果周围很多朋友选择了某种商品，那么自己大概率也会选择该商品；或者用户选择了某种商品，当看到类似商品且其他人对该商品评价很高时，则购买这个商品的概率就会很高。协同过滤又分为三种：基于用户的协同过滤、基于项目的协同过滤和基于模型的协同过滤。 1）基于用户的协同过滤的基本思想是首先找到与目标用户兴趣相似的用户集合，然后找到这个集合中用户喜欢并且没有听说过的物品推荐给目标用户。下图是基于用户的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户C喜欢商品A、商品C和商品D，用户A和用户C具有相似的兴趣爱好，因此把商品D推荐给用户A。 2）基于项目的协同过滤的基本思想是基于所有用户对推荐对象的评价的推荐策略。如果大部分用户对一些推荐对象的评分较为相似，那么当前用户对这些推荐对象的评价也相似。然后，将相似推荐对象中用户未进行评价的商品推荐给用户。总之，基于项目的协同过滤就是根据用户对推荐对象的评价，发现对象间的相似度，根据用户的历史偏好将类似的商品推荐给该用户。图2-8是基于项目的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户B喜欢商品A、商品B和商品C，用户C喜欢商品A，通过这些用户的喜好可以判定商品A和商品C相似，喜欢商品A的用户同时也喜欢商品C，因此给喜欢商品A的用户C也推荐了商品C。 3）基于模型的协同过滤的基本思想是基于样本用户的喜好信息训练一个推荐模型，然后根据实时的用户喜好信息进行推荐。其和上述两种协同推荐的不同点在于先对已有数据应用统计和机器学习的方法得到模型，再进行预测。常用的方法有机器学习方法、统计模型、贝叶斯模型和线性回归模型等。 基于协同过滤推荐的优点有：1）可以使用在复杂的非结构化对象上；2）能够发现用户新的兴趣爱好，给用户带来惊喜；3）以用户为中心的自动推荐，随着用户数量的增加，用户体验也会越来越好。缺点在于：1）存在冷启动问题，即在没有大量用户数据的情况下，用户可能不满意获得的推荐结果；2）存在稀疏性问题，即用户大量增长的同时，评价差异性会越来越大，推荐对象也越来越多，导致大量的推荐对象没有经过用户评价，部分用户无法获得推荐结果，部分推荐对象无法被推荐。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"No Answer is Better Than Wrong Answer A Reflection Model for Document Level Machine Reading Comprehension 笔记","slug":"emnlp2020-NoAnswerisBetter","date":"2021-04-19T17:29:46.000Z","updated":"2021-04-22T07:03:45.608Z","comments":true,"path":"2021/04/20/emnlp2020-NoAnswerisBetter/","link":"","permalink":"http://renxingkai.github.io/2021/04/20/emnlp2020-NoAnswerisBetter/","excerpt":"","text":"论文链接AbstractNatural Questions（NQ）数据集给机器阅读理解带来了新的挑战：答案不仅具有不同的粒度（长、短），而且具有更丰富的类型（包括无答案、是/否、单span和多span）。本文针对这一挑战，系统地处理了各种类型的答案。特别是，我们提出了一种新的方法称为Reflection Net，它利用两步训练过程来识别无答案和错误答案的情况。通过大量实验验证了该方法的有效性。在撰写论文时（5月）。2020年12月20日），我们的方法在长答案和短答案排行榜*上均获得前1名，F1得分分别为77.2和64.1。 1 IntroductionNatural Questions(NQ) 数据集的答案提供了两级粒度：长答案和短答案；因此需要模型去在文档和段落级别寻找答案，除了长短答案，还有无答案样本，多span短答案样本，YES/NO答案类型。一些研究人员提出了pipeline方式去抽取短答案，先对长答案进行抽取，然后再抽取短答案。虽然这种方法是合理的，但由于长答案和短答案是分开建模的，因此可能会失去它们之间固有的相关性。还有其他使用联合训练长短答案的方法，以前的方法已经被证明能有效地提高NQ任务的性能，但是很少有工作关注这个QA集合中丰富答案类型的挑战。我们注意到，51%的问题在NQ集中没有答案，因此，模型准确预测何时输出答案是至关重要的。对于其他答案类型，如多span答案或yes/no答案，尽管它们在NQ集合中的百分比很小，但不应忽略它们。相反，在实践中，更倾向于一种能够很好地处理各种答案类型的系统设计。 本文中，我们着重处理无答案类型，我们首先训练所有答案类型的MRC模型，然后，利用训练好的MRC模型对所有训练数据进行推理，训练出第二个模型，称为Reflection model，以预测的答案、上下文和MRC头部特征作为输入，预测出更准确的置信度，从而区分正确答案和错误答案。使用二阶段训练有三个原因： 首先，MRC置信度计算的常用方法是基于logits的启发式方法，这种方法不规范，不同问题之间的可比性不强 其次，在训练长文档MRC模型时，由于负实例比正实例多，所以对负实例进行了大量的下采样。但在预测时，MRC模型需要对所有实例进行推理。这种训练数据分布差异和预测结果表明，MRC模型可能会被一些负面实例所迷惑，并用高置信度的分数预测错误答案。 第三，MRC模型学习了问题的表示、类型和答案之间的关系，而答案不知道预测答案的正确性。 我们的第二阶段模型解决了这三个问题，类似于成为其名称来源的反射过程。通过大量实验验证了该方法的有效性。在撰写论文时（5月）。2020年12月20日），我们的方法在长答案和短答案排行榜*上均获得前1名，F1得分分别为77.2和64.1。 2 ApproachReflection模型结构如图1所示，由MRC模型和Reflection模型构成，分别用于答案预测和答案置信度预测。 2.1 MRC Model使用预训练模型作为MRC模型，滑动窗口用于处理长文本文章，然后将问题与文章片断组成一起去构造一个样本，正样本片段中包含答案，负样本片段中不含答案，由于长文本中负样本较多，我们对负样本使用下采样。 MRC模型的输出为span和答案类型，包含$l=(t,s,e,ms)$，t是答案类型，s和e是答案开始结束位置，答案类似是多span时，我们使用BIO去标注多答案(ms)，使用Transformer作为Encoder，最终得到$h(x)$ 答案类型分类使用[CLS]进行分类，单答案span使用最小化开始和结束的位置进行，多答案类型使用序列标注思想，直接将隐状态通过线性层进行分类BIO，并没使用CRF解码。最终MRC模型的loss: 除了预测答案，MRC模型需要输出置信分数， xs,xe,x1是被预测的开始、结束和[CLS]字符 2.2 Reflection ModeReflection Mode的目标是一个更精确的置信度得分，区分正确答案和两种错误答案（见第3.4节）。第一种方法是预测一个has-ans问题的错误答案，第二种方法是预测一个no-ans问题的任何答案 Training Data Generation为了生成Reflection Model的训练数据，我们使用训练后的MRC 模型去推理全量数据 对于属于每一个问题的所有实例(feature)，我们只根据其置信度得分选择预测答案前1名的实例。 所选实例、MRC预测答案、其对应的如下头部特征和正确性标签（如果预测答案与真实答案相同，则标签为1；否则为0）一起成为反射模型的训练案例。 Model Training 如图1(a)所示，我们初始化反射模型使用MRC训练好的参数，学习率比MRC模型的学习率小几倍。为了收到重要的MRC模型的状态信息，我们从MRC模型的top层抽取顶层特征当它预测答案时，如表2所示。 顶层特征与[cls]标记的隐藏表示连接，然后是用于最终置信度预测的隐藏层。 反射模型将所选实例x和预测答案作为输入。具体地说，我们创建了一个以答案类型和答案位置标记为元素的字典Ans。我们将应答类型标记添加到[cls]字符中，将位置标记添加到相应的位置字符中，并将空标记添加到其他字符中。 反射模型的隐藏层表示为： 然后和顶层特征拼接[CLS]字符表示$h^r(x_1)$，如下式： 最后获得的置信度分数和二分类损失如下： 式中，如果MRC模型的预测答案（基于x）正确，则y=1，否则为0。对于推理，MRC模型需要为每个问题预测一个文档的所有滑动窗口实例，而反射模型只需要推理一个包含MRC模型预测的最终答案的实例。因此反射模型的计算量很小 3 ExperimentsNQ 训练集307,373；验证集7,830;测试集7842 for lb. 3.1 Implementation先在SQuAD 2.0上ft一遍，再ft NQ 3.2 Baselines DocumentQA DecAtt + DocReader BERTjoint 4 Ablation Study 5 Related WorkMRCAnswer Verifier6 Conclusion本文提出了一种系统的方法来处理MRC中的丰富答案类型。特别是，我们开发了一个Reflection Model 来处理无答案/错误答案的情况。其关键思想是根据预测答案的内容、上下文和状态，训练第二阶段模型，预测预测答案的置信度。实验表明，该方法在NQ集上达到了最新的结果。由F1和R@P=90在长、短的回答上，我们的方法都超过了之前的SOTA。消融研究也证实了我们的方法的有效性。","categories":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"}],"tags":[{"name":"MRC NQ","slug":"MRC-NQ","permalink":"http://renxingkai.github.io/tags/MRC-NQ/"}],"author":"CinKate"},{"title":"Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge 笔记","slug":"paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge","date":"2021-04-19T17:12:54.000Z","updated":"2021-04-19T09:15:17.033Z","comments":true,"path":"2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/","link":"","permalink":"http://renxingkai.github.io/2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/","excerpt":"","text":"论文链接Abstract答案选择和知识库问答（KBQA）是问答（QA）系统的两个重要任务。现有方法分别分开解决这两个任务，这需要大量的重复工作，而忽略了任务之间的丰富的相关信息。在本文中，我们基于以下动机，通过多任务学习（MTL）同时解决答案选择和KBQA任务。首先，答案选择和KBQA都可以视为排序问题，一个在文本级别，另一个在知识级别。其次，这两个任务可以互惠互利：答案选择可以结合知识库（KB）中的外部知识，而KBQA可以通过从答案选择中学习上下文信息来进行改进。为了实现共同学习这两个任务的目标，我们提出了一种新颖的多任务学习方案，该方案利用从各个角度学习到的多视图注意力来使这些任务能够相互交互以及学习更全面的句子表示形式。在多个真实的数据集上进行的实验证明了该方法的有效性，并提高了答案选择和KBQA的性能。同样，从不同的表示角度来看，多视图注意力方案在组合注意力信息方面被证明是有效的。 Introduction多任务学习在NLP领域是广泛的，然而QA中的MTL研究较少。本文中，我们探索了利用MTL去同时处理答案选择和KBQA，这两种任务都可以视为排序任务，一个在答案文本级别，另一个是知识级别。答案选择是在多个答案片段中选出一个最合适的答案，KBQA专注于在KB中抽取出相关的知识，去回答问题。大多多任务学习模块，划分共享层和特殊任务层，共享层在各个任务中共享信息，特殊任务层对不同任务是不同的。大多模型忽视了共享层和特殊任务层之间的交互，并且忽视了不同任务之间的交互。因此，本文提出了MTL从不同方面去学习多视角的attention，能够使不同的任务互相交互。具体而言，我们从任务特定的层中收集注意力信息，以在共享层中学习更全面的句子表示形式。 此外，多视图注意力机制通过结合单词级和知识级信息来增强句子的表示学习。 也就是说，通过使用多视图注意力机制，可以在不同任务之间共享和传输单词级和知识级的注意力信息。 根据实验，与单独答案选择和KBQA任务相比，联合学习可以显着提高每个任务的性能。 实验结果还表明了多视图注意力方案的有效性，并且每个视图的注意力都做出了贡献。本文主要贡献如下： 我们探索用于选择答案和知识库问题解答的多任务学习方法。 知识级别的KBQA任务可以改善答案选择任务，而词汇级别的答案选择任务可以增强KBQA任务。 我们提出了一种新颖的多任务学习方案，该方案利用多视图注意力机制来桥接不同的任务，该任务将特定于任务的层的重要信息集成到共享层中，并使模型能够交互式地学习单词级别和知识 级表示。 实验结果表明，答案选择和KBQA的多任务学习优于最新的单任务学习方法。 此外，基于多视图注意力的MTL方案进一步提高了性能。 Multi-Task Learning for Question Answering 图一为多任务QA网络的架构(包含AS和KBQA)。 Task-specific Encoder Layer 首先将预处理后的句子编码成向量表示。不同的QA任务在数据分布和底层表示上应该是不同的。因此，每个任务都配备了一个用于问答的特定任务编码器，每个特定任务编码器都包含一个单词编码器和一个知识编码器来学习完整的句子表示，如图2所示 Word Encoder. 输入为词向量，使用BiLSTM去捕获全文信息，得到Hw。 Knowledge Encoder. 由于知识序列是由一系列标记化的实体或关系名称组成的，因此后一种学习过程需要基于高级知识的表示。针对这一问题，我们将CNN应用到知识序列上，其中大小为n的filters在知识嵌入矩阵上滑动以捕获局部n-gram特征，每次移动都会计算一个隐藏层向量。由于实体长度不确定，因此，会使用不同大小的卷积核来计算，然后使用Dense层来获取知识表示Hk 将Hk和Hw进行拼接，$H_q=[H_{W_q}:H_{K_q}]$，$H_a=[H_{W_a}:H_{K_a}]$，分别为问题和答案的隐藏表示。 Shared Representation Learning Layer与任务特定编码层的输入相比，整句表示具有更丰富的语义，与其他任务的分布更相似。因此，我们整合来自所有任务的编码向量，并通过一个高级共享的Siamese-Bi-LSTM来生成最终的QA表示$S_q$和$S_a$，然后进行平均池化，并且使用了一些词级别特征$x_{ol}$，最终表示为$x=[s_q,s_a,x_{ol}]$ Task-specific Softmax Layer 使用softmax最终分类。 Multi-Task Learning 最小化以上目标函数，$\\lambda$为不同任务的权重 Multi-Task Model with Multi-View Attention为了增强潜在表征空间中不同QA任务之间的交互作用，我们提出了一种多视角注意机制，从任务特定层和共享层提取重要信息。 Multi-View Attention Scheme如图3所示，与其他注意共享方案不同的是，我们不仅从任务特定层提取注意力，而且还将来自共享层的信息进行组合。此外，我们从词汇和知识两个角度获得注意信息，因为词汇水平和知识水平的信息可能共同促进表示学习。具体来说，我们计算了五种注意观，包括词、知识、语义、知识语义和共同注意力。 Semantic View &amp; Knowledge Semantic View使用mean/max池化去获取句子的语义表示 ExperimentDatasets &amp; PreprocessingAS: YahooQA,TREC QAKBQA: SimpleQuestions,WebQSP Multi-Task Learning Results Related WorkAnswer SelectionMulti-Task LearningConclusion本文研究了同时解决答案选择和知识库问答问题的多任务学习方法。我们提出了一种新的多任务学习方案，该方案利用从不同角度学习的多视角注意，使这些任务能够相互作用，并学习更全面的句子表示，包括词视角、知识视角、语义视角、知识语义视角和共注意力视角。在几个广泛使用的QA基准数据集上进行的实验表明，答案选择和知识库问答的联合学习方法明显优于单任务学习方法。同时，多视角注意策略能有效地从不同的表征视角收集注意信息，提高整体表征学习效果。","categories":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"}],"tags":[{"name":"MTL MRC","slug":"MTL-MRC","permalink":"http://renxingkai.github.io/tags/MTL-MRC/"}],"author":"CinKate"},{"title":"Text Style Transfer via Learning Style Instance Supported Latent Space 阅读笔记","slug":"ijcai2020-StyIns","date":"2021-02-06T21:41:25.000Z","updated":"2021-02-06T13:47:23.342Z","comments":true,"path":"2021/02/07/ijcai2020-StyIns/","link":"","permalink":"http://renxingkai.github.io/2021/02/07/ijcai2020-StyIns/","excerpt":"","text":"本文为biendata视频笔记，仅用来学习，侵删。 开源地址 Background风格转换：在不改变原来句子语义情况下，将原句子x和目标风格sj输入，生成含有目标风格sj的句子 Applications 文本润色，将非正式句子转为正式句子 评论、对话的自动生成 风格特征写作 Previous Paradigms风格转换，需要表示句子内容和target style。 解耦方式(Distentage)。将encoder之后的隐状态，拆出风格表示特征c和文本表示特征h，将h和target style同时输入到decoder中，生成y。这种方式的缺点：内容保留不太好；优点：风格表示容易控制、灵活的。 基于Attention的Seq2Seq。使用Seq2Seq结构，使用attention将x和y去对齐，并且在生成y时候，使用目标风格style embedding去监督生成对应风格的句子。cycle loss:先用目标风格sj监督x生成y，然后再用先前风格si将y再生成回去x’，x’和x计算相似loss。可以保证迁移之后的句子仍然保留原来的语义。同时使用判别器去判别迁移后的y是否真的有目标风格，促进风格迁移生成！优点：保留词级别的信息，几乎没有语义丢失。缺点：风格的信号比较差，风格迁移效果不太好。 Multi-Generator。通过不同generator来实现不同风格上的style生成。相当于每个decoder就代表了各自的风格。为了保留内容，在使用另一个decoder重新构造生成x’。优点：效果很好！缺点：资源需求比较多，需要多个seq2seq。 Locate and Replace。找出句子中和风格相关的词，进行替换，保留和风格无关的词。优点：较为精确，能够实现较好的风格转换和内容保留。缺点：需要一个风格词表；对于超出词义的风格迁移，效果不好，上升到更深的语义级别效果不好！ Motivation结合Attention-based Seq2Seq和潜在风格空间Latent Style Space，去构建更好的风格迁移和内容保留。 Methodology之前的VAE方法，假设每个句子是独立的，每个句子都去单独输出隐状态风格z，不能充分考虑全局信息 K为K个句子都包含同样的风格j。一个原句子encoder编码原句子x为H，一个风格encoder(将K个句子都包含同样的风格j的句子集)进行风格编码为z，一个decoder对H和z进行解码。 style encoder使用生成式流模型，假设初始为高斯分布z0，经过多次复杂变换之后，生成更为复杂的分布zt，能更充分表示风格。 TrainingUnsupervised Training Cycle Consistency Loss:先将x转为y再转回去x’，计算x’和x的交叉熵。 Reconstruction Loss:给定原句x和原句风格s，希望能重新生成原句x Adversarial Style Loss:提供style相关的判别对抗lossSemi-Supervised Training ExperimentDataset Metrics Results 人工评测 Cases Conclusion","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"CoCon A Self Supervised Approach for Controlled Text Generation 阅读笔记","slug":"CoCon","date":"2021-02-01T16:30:22.000Z","updated":"2021-02-01T08:31:57.396Z","comments":true,"path":"2021/02/02/CoCon/","link":"","permalink":"http://renxingkai.github.io/2021/02/02/CoCon/","excerpt":"","text":"论文链接Abstract基于Transformer的LM展现了显著的自然语言生成能力。由于其巨大的潜力，控制这些LMs的生成文本正受到关注。虽然已经有了不少研究去控制被生成文本的高级属性(情感、主题等)，但仍然缺少对内容上词级别、短语级别的准确控制。本文中，我们提出了Content-Conditioner(CoCon)在细粒度级别，去使用目标内容控制一个语言模型的输出。在我们的自我监督的方法中，CoCon学习帮助LM完成一个部分观察到的文本序列，通过调节LM中保留的内容输入。通过实验，我们证明了CoCon可以自然地将目标内容合并到生成的文本中，并以Zero-shot的方式控制高级文本属性。 1 Introduction基于Transformer的LM在海量语料上进行训练，然后去预测下一个token通过对数似然损失(log-likehood)。控制LM的输出逐渐吸引了人们的注意。像从零开始训练一个修改过的LM来合并目标文本属性[这样的方法成本很高，而对特定属性的预训练LM进行微调则限制了文本控制的范围。PPLM没有修改LM的架构，通过属性来控制被生成的文本。尽管在控制诸如主题和情感之类的高级文本属性方面是有效的，但是相同的目标属性可能会生成在单词和短语级别具有显著不同的内容文本样本，从而为对LM生成的文本的内容进行更细粒度的控制留下一个gap。为了举例说明情绪控制的例子，一篇以“彼得”这样的名字的提示文本开头的文章后面可以是描述彼得性格的文本，关于他在某个特定时间正在做什么，甚至是关于“彼得广场”这样的非人类实体，以积极的方式。对LMs输出内容的更精细控制可以为生成符合事实或没有不适当内容的文本铺平道路。 我们提出的Content-Conditioner （CoCon），以缩小这一差距，通过指导预训练的LMs的文本输出同时包含目标内容。基本上，CoCon包括三个部分：1）编码器，2）解码器和3）CoCon块。CoCon采用预训练LM作为编码器和解码器，通过CoCon块将目标内容合并到编码文本表示中，然后将内容条件表示传递给解码器进行生成。为了训练CoCon块，我们提出了一种自监督学习方法，其中训练数据由预训练LM本身生成的文本样本组成。通过将每个文本序列分成两段（[xa；xb]），CoCon学会了通过将xb本身作为内容输入来帮助LM重建丢失的后段（xb）。 我们提出了CoCon的损失函数和content masking，在产生高质量文本的同时，对来自不同来源的内容进行限制。由于CoCon块的大小是LM的一小部分，并且没有对LM的权重进行微调，因此训练成本明显低于从头开始训练LM。对比于strong baselines，CoCon也可以进行高等级的文本属性控制(例如主题和情感)，通过zero-shot的方式。此外，CoCon在同化多个目标内容方面具有通用性，其内容调节的强度可以在推理过程中通过内容偏差项进行灵活的调整。本文中，我们使用GPT-2作为LM。我们主要贡献如下： 我们提出CoCon的基于条件的内容语言生成方法。 我们引入了一种自我监督学习方法，在这种方法中，CoCon在给定未来token的信息时学习完成文本序列。 通过消融试验和与PPLM和CTRL等强基线的比较，我们研究了CoCon如何有效地影响文本生成的内容，以及如何有竞争性地控制主题和情感等高级文本属性。 我们展示了CoCon在整合多种内容方面的多功能性，文本生成中内容调节的灵活性，以及它与其他控制方法（如PPLM）的互补性。 2 Related Work之前的属性控制的文本生成方法大多基于RL和GAN进行训练。与CoCon不同，这些方法中对预定属性的要求限制了生成文本的可能类型。 CTRL是最近的一种方法，它通过使用控制代码生成受控的流畅文本，这些控制代码是在生成过程中预先添加到文本中的元数据。虽然它使用GPT-2结构生成高质量的文本，但是它的控制代码在训练过程中也是预先确定的。 最接近我们的工作是PPLM，它试图通过相对较小的“可插拔”属性模型来控制已经预训练的LM上的文本。尽管PPLM的灵活设计也支持受控生成，而无需像CoCon中那样对LM进行再培训或微调，但我们的方法旨在将生成控制在更局部的内容级别，而不是高级的文本属性。另一个核心区别在于训练，CoCon的自监督学习免除了对标记数据的需要，例如用于训练PPLM的属性鉴别器模型的数据。PPLM中的加权解码试图通过在解码步骤中提高目标词的概率来控制输出文本，但已被证明产生不连贯文本。 文本风格转换是通过将文本从一种风格转换到另一种风格来控制文本属性的相关领域。一些这样的研究使用自动编码器来分离文本的风格和非风格的潜在表征。这种分离使得文本在保留大部分内容的同时，能够在潜在空间改变样式。另一项工作是识别与文本语料库中特定风格相关的n个属性标记，并通过替换它们来编辑文本的风格。从本质上说，风格转换改变现有的文本，而不是生成文本，需要预定义的属性。 3 Content Conditioner (CoCon)Motivation在基于语言模型的文本生成任务中，通常使用自回归的方式进行生成： 之前的控制文本生成任务中，p(x)可以通过目标属性或者控制代码进行控制文本的情感或者主题： 虽然这些方法生成是流畅的，并且可以很好地与目标属性对齐，但是输出文本${x_t,...,x_l}$是在全局属性（例如情绪/主题）级别上控制的，而不是在更局部的内容（例如单词/短语）级别上控制的。由于存在大量可能的${x_t,...,x_l}$候选项，这些候选项将与先验文本和目标属性很好地对齐，因此在随机字符采样过程中，生成的文本样本包含非常不同的内容。这激发了一种对输入目标内容c进行条件处理的方法，以便对文本生成进行更细粒度的控制： Model Architecture 我们提出的CoCon（图1）通过在我们的实验中加入一个基于预训练Transformer的语言模型（LM）GPT-2，在保持流畅性的同时控制生成文本的内容。 LM的生成可以分为两个独立的部分：编码器（enc）和解码器（dec）。编码器充当一个特征提取器，接收输入序列的嵌入并在断点处输出其中间表示，即$h_{:t−1}=enc(x_{:t−1})$。随后，解码器接收该表示并输出下一字符的logit，即$o_t＝dec(h_{:t−1})$，从而产生 从等式4中，我们可以看到表示（h）是控制下一个字符logits（o）的表示。实际上，我们通过CoCon块用目标内容输入（c）对h进行条件化，从而转换h 我们参数化CoCon模块将其作为一个单层的Transformer模块，其中包含一个attention层和 FFN层。与典型的LM attention层类似，Query（Q）、Key（K）、Value（V）矩阵通过表示$h_{:t−1}$上的线性变换来计算，其中$Q,K,V\\in R^{(t-1)xd}$，d是表示的维数。为了关注内容表示$h_{l_c}^{(c)}$，还计算内容键和值$K^{(c)},V^{(c)}\\in R^{l_cxd}$，并在计算CoCon注意输出之前concat到原始注意矩阵： 最后的CoCon输出通过位置前馈层进行计算。通过连接到t−1之前的表示并将其传递给解码器（dec），下一个logit以及随后的字符$\\hat x_t$现在由c进行调节： Multiple Content Inputs对于多个目标内容的输入，处理方法较为简单，将多个目标内容的K和V进行拼接再计算即可。 Strength of Content Conditioning在CoCon的注意机制中，我们可以通过偏置W（等式6）中与内容输入（c）相对应的注意权重来改变内容对输出文本的调节程度。更具体地说，可以通过注意对内容值$(V^{(c)})$的softmax加权来改变c对输出文本的影响。在生成期间，可以选择向内容注意权重$W_{:,:l_c}\\in R^{(t-1)xl_c}$添加正偏项（τ内容），以增加V（c）的影响，促进内容调节，而负偏项可以相反地降低内容调节效果。我们讨论了变化τ含量的例子在4.4节中。 3.1 Self-Supervised Learning我们可将一个序列$x=\\{x_1,...,x_{t-1},x_t,...,x_l\\}$分为两部分：$x^a=\\{x_1,...,x_{t-1}\\}$和$x^b=\\{x_t,...,x_{l}\\}$，即$x=[x^a;x^b]$。在现实世界中，$x^b$的替代品可能有很多，可以流利地从$x^a$开始。再加上文本采样的随机性，这意味着，如果没有关于$x^b$的信息，仅用LM从$x^a$重建完整x的概率可能很低。 Self Reconstruction Loss 自重构损失$L_{self}$就是让控制文本$c=x^b$，之后要生成的就是$x^b$自己，这一步是让模型能够学习结合控制文本的内容。 Null Content Loss 无内容损失$L_{null}$，即令$c=null$，这时候CoCon就退化为一个简单的语言模型，为了能够生成流畅的文本。 Cycle Reconstruction Loss 循环重构损失$L_{cycle}$通过两个不同文本互为控制文本来生成质量更高的文本。假设现在有两个不同的文本：$x=[p;q]$和$x&#39;=[p&#39;;q&#39;]$，进行如下操作： 首先将p’作为引导文本，将q作为控制文本，生成新文本$q^1=f(p&#39;,q)$。显然，q1的目的是在和p’保持语言流畅，且尽可能包含q的内容。 再将p作为引导文本，将q1作为控制文本，生成新文本$q^2=f(p,q^1)$，这是让q2和p保持流畅，同时尽可能包含q1的内容。 既然q2包含q1的内容，并且q1包含q的内容，要么就是q2包含q的内容，且要和p保持流程，此时就是q本身。 因此，循环重构损失就是以q为真值去优化q2，反过来也可以以q’为真值去优化q’2 Adversarial Loss 最后用到常用的对抗损失$L_{adv}$，让生成的文本更接近真实的文本。 Full Training 组合以上四个loss即为最终需要优化的目标： 4 Experiments本文在三个可控文本生成任务上进行实验，情感可控生成，主题可控生成，文本可控生成。 文本可控生成 文本引导的文本生成评估指标有BLEU、NIST、METEOR、PPL和Dist-1/2/3。由于没有现成的该任务的数据集，故本文随机从预训练的GPT-2中采样3000个样本，均匀分为三组，每组的控制文本c的长度为5,10,20个BPE长度，所生成的句子x长度都是100。把它们作为测试集。训练集则是随机从GPT-2所产生的句子中获取。此外，CoCon还在大小为250K的Webtext.上训练，以探究不同训练数据来源的影响。结果如下表所示。可以看到，CoCon在结合控制文本c.上比GPT2好太多。在这个实验。上，不加一些损失会有更低的PPL以及更高的BLEU、NIST、METEOR等值，比如去掉$L_{null}$或者$L_{adv}$模型的BLEU值会更高，这是因为这两个损失相比之下更关注生成文本的流畅度而不是与控制文本c相结合。 主题可控文本生成 在主题可控任务上，比较的基线模型有PPLM和CTRL，都是当前很强的模型。为验证主题相关性，训练一个分类器用于评估主题程度。下表是实验结果： 情感可控文本生成 情感可控生成任务采用二分类情感，下表是实验结果，和主题任务类似，CoCon在情感相关度上显著优于其他模型，这说明CoCon能够更加紧密地控制文本c。 实例分析 下图是不同的$t_{content}$生成的不同文本，显然，值越大就越和控制文本相关，但是越容易生成不相关的内容。 多个控制文本输入的结果实例： 5 Conclusion我们提出了CoCon为了细粒度的控制基于神经网络的生成文本。CoCon可以被有效训练通过一个自监督的方式，并且它与已经生成高质量文本的预训练语言模型兼容。通过我们的实验，CoCon被证明能够顺利地将目标内容合并到生成的文本中，并控制高级文本属性。这种新的控制方式为更真实、更安全的文本生成带来了希望，促进了神经文本生成在实际应用中的应用。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"PLUG AND PLAY LANGUAGE MODELS A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION","slug":"iclr-2020-PPLM","date":"2021-01-28T21:44:04.000Z","updated":"2021-01-31T14:27:13.189Z","comments":true,"path":"2021/01/29/iclr-2020-PPLM/","link":"","permalink":"http://renxingkai.github.io/2021/01/29/iclr-2020-PPLM/","excerpt":"","text":"论文链接Abstract在大型文本语料库上训练的大型基于transformer的语言模型（LMs）已经显示出无与伦比的生成能力。然而，如果不修改模型结构或对特定属性数据进行微调，并且需要大量的再训练成本，那么控制生成语言的属性（例如，切换主题或情感）是困难的。我们提出了一个简单的可替代方案：可控制语言生成的即插即用语言模型（PPLM）。它将预训练的语言模型与一个或多个简单的属性分类器相结合，这些属性分类器无需对语言模型进行任何进一步的训练就可以指导文本生成。在我们提出的规范场景中，属性模型是简单的分类器，由用户指定的bag of words或单个学习层组成，其参数比LM少100000倍。采样需要前向和后向传递，其中来自属性模型的梯度推动LM的隐藏激活，从而指导生成。模型样本展示了对一系列主题和情感风格的控制，广泛的自动化和人工注释评估显示属性对齐和流畅性。PPLMs的灵活性在于，可以使用不同属性模型的任何组合来指导文本生成，这将允许本文给出的示例之外的多种创造性应用。 1 INTRODUCTION基于Transformer的PLMs在大量无标注的语料中训练，然后使用对数似然损失函数进行训练。然而，一旦这样的模型被训练，在不修改模型结构以允许额外的输入属性或对属性特定的数据进行微调的情况下，控制生成文本的属性就变得很困难。 可控的文本生成需要对$p(x|a)$进行建模，a是想要的可控属性，x是需要被生成的文本。然而，生成模型只需要去学习$p(x)$。在计算机视觉领域，Nguyen et al.（2017）的即插即用生成网络（PPGN）开发了一种生成具有不同属性的图像的机制，方法是将判别器（属性模型）$p(a|x)$与基本生成模型$p(x)$插在一起，并从结果的$p(x|a)\\propto p(a|x)p(x)$中采样，有效地创建条件生成模型从任何提供的属性模型动态建模。以类似的方式，我们提出了用于条件语言生成的即插即用语言模型（PPLM），它将一个或多个简单的属性模型p（a|x）组合在一起，可以是一个Bag-of-Words（BoW）或单层分类器的形式，也可以是一个预训练的无条件语言模型p（x）。我们从得到的组合模型中取样，在潜在的表示空间中遵循梯度，这种方式受到了（MALA）的启发。 优化是事后在激活空间中执行的，因此不需要重新训练或微调。文本控制是细粒度的，强度参数决定属性影响多大的强度；强度为0则完全恢复原始模型p（x）。这种设计允许极大的灵活性：用户可以将最先进的生成模型与任意数量的属性控制器结合起来，生成模型可能很大，而且很难训练。属性模型可能更易于训练或未经训练，并且在推理过程中可以灵活地组合多个控制器。在本文中，我们演示了使用GPT-2 345M参数模型作为通用语言模型p（x）的PPLM方法，但是该方法适用于任何基于transformer的文本生成器的任何表示空间，并且允许与任何属性模型p（a|x）组合。 我们演示了使用多个属性控制器的受控生成，这些控制器在生成过程中组装和组合，每个控制器具有不同的强度，充当一组“控制旋钮”，将生成的文本调整为所需的属性（参见表1中的示例），我们关键的贡献如下： 我们提出了用于控制语言生成的即插即用语言模型，讨论了它与现有工作的关系，以及如何从PPLM中进行采样。 我们证明了对一系列属性的文本生成控制，包括7个主题，每个主题使用一个BoW定义，以及1个简单的情感判别器。我们使用自动评估（分别训练的困惑和情绪模型）和人类评估（属性相关性和流畅性）来量化有效性。所有的评估都指向PPLMs生成属性控制的流畅文本的能力 我们对比了CTRL和GPT-2，我们的方法，没有任何LM训练，在属性相关性和流利性方面都是标杆，并且经常优于基线 我们表明，PPLM方法可以用来文本消除毒性，遵循负梯度模型训练，生成有毒内容是可能的，同时也可以用其检测毒性。我们还展示了PPLM如何用于结构受限的故事写作 2 RELATED WORKControlled generation当前的控制文本生成方法大多涉及到使用RL方法ft预训练模型，训练GAN或者训练条件生成模型。与我们方法不同之处，这些方法并不是即插即用，因为整个模型需要针对每个特定属性分别进行微调。我们的方法不需要对任何条件生成模型进行再训练，语言模型和条件模型都可以灵活组合。 Noisy Channel Modeling不少人使用香农噪声信道理论为了提升Seq2Seq建模，他们的方法翻译源语言句子y到一个目标语言句子x通过首先对前向模型的采样$p_{forward}(x|y)$，然后基于概率$p_{backward}(x|y) \\propto p(x)p(y|x)$重排样本。PPLM对样本进行打分使用相同的基本公式，但由于我们没有前向模型$p_{forward}(x|a)$，我们依赖于潜在空间的更新。作为一个基线，我们使用$p(x)$作为前向模型，然后重排序。我们将看到，在某些场景中工作得相当好，而在其他场景中工作得很差 Weighted decoding有的研究者使用判别器或者BoW去控制语言生成，为了考虑用于解码的评分函数，对解码过程进行了修改。See等人（2019）注意到，使用加权解码（WD）进行控制是困难的，通常会导致牺牲流畅性和连贯性。此外，Ghazvininejad et al.（2017）强烈依赖于从特定主题的一组关键字中取样，并且不允许以不必包括一组关键字的方式偏向于主题的生成。类似的，Baheti等人提出了一种解码策略用于在对话系统中生成感兴趣的回复，使用BoW和词向量，随机采样方法可以被使用去受限于模型的生成到确定的关键词和主题。我们使用带权重的解码作为基线。 Text Style Transfer在语言建模之外，语篇风格迁移是一个相关的研究课题。有的研究者训练VAE for 风格迁移依赖于学习区分风格和内容的潜在表示。Li et al.证明了一种基于条件生成模型的简单方法的有效性，该方法将与属性相关的n-gram替换为与所需属性对应的n-gram。我们的方法与上面方法一个关键的不同之处在于，我们使用了一个线下的判别器，基于此鉴别器执行优化。最近，Lample等人（2019年）采用了一种从无监督语言翻译到风格转换的方法，在这种方法中，去噪自动编码器的训练目标包括重建损失和回译损失的加权组合。虽然上述方法在风格转换任务上取得了令人印象深刻的成功，但主要的焦点是不受控制的语言生成，而且这些方法也不是即插即用的。 3 PLUG AND PLAY LANGUAGE MODELS 3.1 LANGUAGE MODELING WITH TRANSFORMERS给定一个句子$X={x_0,...,x_n}$，语言模型被训练去计算序列$p(X)$的无条件概率。这个概率可以应用递归链式法则进行表示为乘积形式： 本文中，我们使用Transformer进行语言分布的建模。令$H_t=[(K_t^{(1)},V_t^{(1)}),...,(K_t^{(l)},V_t^{(l)})]$为由key-value组成的历史矩阵，从第i层0到t的时间序列。 3.2 STEERING GENERATION: ASCENDING $log p(a|x)$为了去控制语言模型的输出，在每个生成步t，我们转移历史$H_t$在两个梯度方向上，一种倾向于在条件属性模型$p(a|x)$下，更高的对数似然属性a；另一种是倾向于未修改语言模型p（x）的更高对数似然。将这些因素与可变乘数结合起来，为我们提供了一个可控的“旋钮”，以指定的强度在给定的情感方向上引导生成。参数更新仅限于Ht，而不限于其他激活模型，因为未来的预测仅通过Ht依赖于过去（注意，Ht由在时间t之前生成的所有Tramsformer的key-value组成）。在Ht空间经过每一步会导致模型激活的逐渐变化——这可能被认为是对过去的逐渐重新解释——从而引导下一代朝着理想的方向发展。 将∆Ht作为对Ht的更新，这样，使用（Ht+∆Ht）生成的文本会改变生成文本的分布，从而更可能拥有所需的属性。∆Ht初始化为零，并使用属性模型的梯度进行更新，该属性模型测量生成的文本具有所需属性的程度（例如正性）。我们重写属性模型$p(a|x)$为$p(a|Ht+∆Ht)$然后基于以下公式去更新∆Ht： 3.3 ENSURING FLUENCY: ASCENDING $log p(x)$上一节中描述的方法能够生成针对特定判别器的文本，但如果不加以检查，当文本进入低概率区域时，将很快导致不切实际的敌对或愚蠢的示例（Szegedy et al.，2013；Nguyen et al.，2015）。为了解决这个问题，我们从两个方面使用了无条件语言模型，以确保流利度保持在或接近无条件语言模型的水平（这里是GPT-2）。 Kullback–Leibler (KL) Divergence除了上述步骤之外，我们还更新∆Ht以最小化修改和未修改语言模型的输出分布之间的KL差异。实际中，这是通过在使用渐变之前将数量相加来实现的，尽管可以将其可视化为两个单独的步骤，如图2所示。我们用一个标量λKL来缩放KL系数，在实践中，将这个超参数设置为0.01通常可以很好地在各种任务工作。 Post-norm Geometric Mean Fusion除了KL散度，我们还是用了后范数融合方法，这并不直接影响∆Ht；相反，它只是将生成的文本与无条件的p（x）LM分布联系起来。 3.4 SAMPLING AND RANKINGPPLM中的属性模型$p(a|x)$提供两种功能：首先，基于对数似然提供一个能排序想要样本的分数；其次，在潜在空间中执行更新的梯度上升方向。前者可用于生成r个样本，并对其进行排序，以选择最佳样本。除了使用更新的采样外，这还可以作为属性控制的附加方法。此外，为了避免重复性、低质量文本的问题（Holtzman et al.，2018），我们计算Dist-1、Dist-2和Dist-3分数的平均值（对于生成的文章），这是重复性的指标（Li et al.，2015），然后丢弃平均分数低于阈值τ的样本。 4 EXPERIMENTS, RESULTS, AND EVALUATION4.1 EVALUATION METHODS AND ABLATION STUDY我们评估两个属性：PPLM是否生成满足所需属性（主题或情感）的文本，以及当我们加强对属性的控制时，其文本质量是否恶化。注：我们可以随时将控制旋钮调低到零，以禁用属性控制，并达到原始模型的流畅性。如果需要，用户可以在推理时调整旋钮，直到在属性强度和流利性之间达到所选择的折衷。我们使用自动方法和人工注释器进行评估 4.2 BoW ATTRIBUTE MODELSBoW的一些结果 4.3 DISCRIMINATOR ATTRIBUTE MODELS当仅用BoW一些词难以表示的属性词，就可以使用判别器来进行属性控制。我们在输入句子x和相应标签yx的数据集上训练鉴别器，以下是结果。 4.4 LANGUAGE DETOXIFICATION使用大量互联网数据训练的语言模型反映了数据中存在的偏见和歧视。Wallace et al.（2019）最近的一篇论文进行了对抗性攻击，当给定一个经过仔细优化的触发器字符串作为前缀时，GPT-2会产生种族主义输出。他们还发现，当简单地使用“黑人”作为前缀时，2%的GPT-2样本包含明显的种族歧视。其他前缀（如“亚洲人”或“犹太人”）被提及，但没有报告百分比。我们进行实验并报告基线毒性百分比为10%（“亚洲人”）、12%（“犹太人”）和8%（“黑人”）。Wallace等人（2019年）发布的代码库产生了对抗性触发，平均毒性百分比为63.6%。更多细节见第S13节。 通过引入毒性分类器作为属性控制模型，并用负梯度更新潜在空间，PPLMs可以很容易地适应语言解毒。我们对来自毒性评论分类挑战（Jigsaw）的毒性数据训练了一个单层分类器，并表明与其他PPLM-Discrim方法具有相似的超参数设置，它在自然提示和对抗性触发条件下都能很好地工作。自然诱发的毒性百分比分别为6%、4%和10%，而对抗性诱发的毒性百分比则急剧下降到平均4.6%，具有统计学意义。注释程序和百分比和p值的完整表格的详细信息见表S23和第S13节。请注意，解毒语言的模型也可能被恶意用于生成有毒语言，这是我们在第S6节中简要讨论的主题 4.5 CONTROLLED STORY WRITING我们探索辅助性故事写作的受控生成（Peng et al.，2018；Luo et al.，2019；Yao et al.，2019；Fan et al.，2018）。使用不受控制的LMs辅助艺术创作可能很困难。为了有助于结构，我们使用了预先定义的故事骨架，通常用于即兴创作。我们用PPLM填充这些前缀之间的空白。见表S20和表S21中的示例。 5 CONCLUSION我们提出了PPLM，一个即插即用的方法可控的语言生成，容易组合到一个巨大的预训练LM和一个BoW模型，或者一个轻量级的容易去训练的判别器。PPLM实现了对属性的细粒度控制通过一个简单的基于梯度的采样机制。由于PPLMs在保持流利性的同时能够灵活地控制生成，因此它在支持下一代语言模型方面具有很大的潜力。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"Style Transfer from Non-Parallel Text by Cross-Alignment 阅读笔记","slug":"nips2017-style-transfer","date":"2021-01-26T09:09:18.000Z","updated":"2021-05-20T06:59:30.247Z","comments":true,"path":"2021/01/26/nips2017-style-transfer/","link":"","permalink":"http://renxingkai.github.io/2021/01/26/nips2017-style-transfer/","excerpt":"","text":"论文链接Abstract本文主要研究基于非平行文本的文体转换。这是一个包括机器翻译、破译和情感改写在内的一系列问题的例子。关键的挑战是将内容与风格等其他方面分开。我们假设一个共享的潜在内容分布在不同的文本语料库中，并提出了一种利用潜在表达的精确对齐来进行风格转换的方法。从一个语体转换过来的句子应该和从另一个语体转换过来的例句相匹配。我们证明了这种交叉比对方法在三个任务上的有效性：情感改写、单词替换密码的破译和词序恢复。 1 Introduction机器翻译、文本摘要类似的任务需要大量的平行语料进行训练，但是在一些NLP生成任务中，我们仅有非平行或单语言的数据。类似于文本破译或者风格迁移，在这些任务中，我们必须保留源语句的内容，但要使语句与所需的表示约束（例如，样式、明文/密文）保持一致。 我们的任务较有挑战性：我们只假设访问两个句子的语料库，尽管呈现的风格不同，但内容分布相同。我们的目标是证明这种内容的分布等价性，如果仔细加以利用，就足以让我们学会将一个文体中的句子映射到一个文体无关的内容向量，然后将其解码成一个内容相同但文体不同的句子。 本文中，我们提出了一个精细的句子级表示对齐方法在文本语料之间。我们通过学习一个编码器，它以一个句子和它的原始样式指示符作为输入，并将其映射到与样式无关的内容表示。然后将其传递给与样式相关的解码器进行渲染。实际上，更丰富的潜在内容表示更难在整个语料库中对齐，因此它们提供了更多的信息内容约束。此外，我们从交叉生成（风格转换）的句子中获取额外的信息，从而得到两个分布对齐约束。例如，风格转换成否定句的肯定句，作为一个总体，应该与给定的否定句相匹配。我们在图1中说明了这种交叉对齐 我们使用三个NLP任务去证明我们方法的有效性：情感改写、文本破译和词序恢复。在这三个任务中，模型都是使用非平行语料进行训练。 2 Related WorkStyle transfer in vision 抽取内容和风格特征等，使用GANS network(CoupledGANs、CycleGAN)。虽然我们的方法具有类似的高级体系结构，但自然语言的离散性不允许我们重用这些模型，因此需要开发新的方法。 Non-parallel transfer in natural language在NLP中的生成任务大多需要平行语料，类似于(机器翻译、摘要生成等)。我们的方法很接近于不使用平行语料，但是使用训练中的非直接信号进行辅助。(VAEs，操作隐状态的表示以控制对应的情感生成等)。虽然我们的模型建立在分布式交叉对齐的基础上，以实现风格转换和内容保留，但可以以相同的方式添加约束。 Adversarial training over discrete samples最近离散样本中的对抗训练，大多使用RNN。在我们的工作中，我们采用了Professor-forcing算法（Lamb等人，2016年），该算法最初是为了弥补教师在训练过程中的强迫和测试过程中的自我反馈之间的差距。这样的设计符合我们的风格迁移场景which called cross-alignment。 3 Formulation 这个命题基本上是说不同风格生成的X应该足够“独特”，否则风格之间的转换任务就没有很好的定义。虽然这看起来微不足道，但即使对于简单的数据分布，它也可能不适用。下面的例子说明了在不同的模型假设下，转移（和恢复）是如何变得可行或不可行的。正如我们将看到的，对于某一风格的Y，z的分布越复杂，恢复传递函数的可能性就越大，寻找传递函数就越容易。 3.1 Example 1: Gaussian 3.2 Example 2: Word substitution这里考虑另一个例子，当z是一个双语语言模型，风格y是一个正在使用的词汇表，它将每个“内容词”映射到它的表面形式（词汇形式）。如果我们观察同一语言z的两个实现x1和x2，那么转移和恢复问题就变成了推断x1和x2之间的单词对齐。 这就是一个简单的语言破译或者翻译版本。除此之外，回复问题仍然是很困难的。为了解释这一问题，假设$M_1$和$M_2$是$X_1$和$X_2$的概率估计，寻找单词对齐与去寻找一个排列矩阵P，满足$P^TM_1P≈M_2$是相似的，可以表示为一个优化问题： 同样的公式适用于给定M1和M2作为两个图的邻接矩阵的图同构（GI）问题，表明确定P的存在性和唯一性至少是GI困难的。幸运的是，如果M作为一个图足够复杂，那么搜索问题可能更容易处理。例如，如果作为一个集合的每个顶点的关联边的权重是唯一的，那么可以通过简单地匹配边集合来找到同构。这个假设在很大程度上适用于我们的场景，其中z是一个复杂的语言模型。我们在结果部分以经验证明了这一点。 上述例子表明，作为潜在内容变量的z应该承载最复杂的数据x，而作为潜在风格变量的y应该具有相对简单的效果。我们将在下一节中相应地构建模型。 4 Method由于图像中的数据是连续的，所以可以直接进行迁移学习，由于自然语言的离散性，我们不能直接进行训练；需要我们在潜在空间进行操作。由于x1和x2是给定潜在内容变量z之后条件独立。 这样表示建议我们使用一个自编码模型，特别地，一个风格迁移任务将x2迁移到x1涉及到两步：编码步和解码步。编码步骤负责推断x2的内容$z-p(z|x_2,y_2)$：解码步骤负责：生成对应的迁移事务从$p(x_1|y_1,z)$,本文中，我们使用神经网络近似学习和训练$p(z|x,y)$和$p(x|y,z)$。 正如我们的生成框架所设想的那样，为了使翻转风格成为一个有意义的迁移，$X_1$和$X_2$的内容空间必须是连续的。为了限制x1和x2是从相同的潜在内容分布p（z）生成的，一种选择是应用可变自动编码器（Kingma和Welling，2013）。VAE先使用一个先验的概率密度$p(z)$，例如$z-N(0,I)$，然后使用KL散度去标准化后验$p_E(z|x_1,y_1)$和$p_E(z|x_2,y_2)$ 然而，正如我们在上一节中所讨论的，将z限制为简单且均匀的分布，并将大部分复杂性推给解码器，对于非并行数据的风格迁移可能不是一个好的策略。相比之下，标准的自动编码器只是将重建误差最小化，鼓励z携带尽可能多的x信息。另一方面，它降低了p（x|y,z）中的熵，这有助于在y1和y2之间切换时产生有意义的风格转换。在不显式建模p（z）的情况下，仍然可以强制p（z|y1）和p（z|y2）的分布对齐。为此，我们介绍了自动编码器的两种受约束的变体。 4.1 Aligned auto-encoder省去了对p(z)作显式假设并使后验值与p(z)一致的VAEs，我们将$p_E(z|y_1)$和$p_E(z|y_2)$相互对齐，从而得到如下约束优化问题。 在实践中，拉格朗日松弛的原始问题，而不是优化。我们引入了一个对抗性鉴别器D来校准不同类型z的聚集后验分布（Makhzani et al.，2015）。D旨在区分这两种分布： 我们使用带有GRU单元的单层RNNs来实现编码器E和生成器G。E获取一个初始隐藏状态为y的输入句子x，并输出最后一个隐藏状态z作为其内容表示。G生成一个以潜在状态(y,z)为条件的句子x。为了对齐$z_1=E(x_1,y_1)$和$z_2=E(x_2,y_2)$的分布，鉴别器D是具有单个隐藏层和sigmoid输出层的前馈网络。 4.2 Cross-aligned auto-encoder第二种变体是交叉对齐自动编码器，它直接将一种风格的迁移样本与另一种风格的真实样本对齐。 在生成性假设下，$p(x_2|y_2)=\\int_{x_1}p(x_2|x_1;y_1;y_2)p(x_1|y_1)dx_1$，因此x2（从左侧取样）应表现出与被迁移的x1（从右侧取样）相同的分布，反之亦然。与我们的第一个模型类似，第二个模型使用两个判别器D1和D2来排列群体。D1的任务是区分真实x1和被迁移的x2，D2的任务是区分真实的x2和被迁移的x1。 对抗训练由G生成的离散样本由于梯度阻断无法进行反向传播，虽然很多人使用强化学习，但由于高方差的采样梯度导致收敛并不稳定。首先，我们不使用单个采样词作为生成器RNN的输入，而是使用词上的softmax分布。具体地说，在从G(y1,z2)迁移x2的生成过程中，假设在时间步t处输出logit向量是vt。我们将其峰值分布softmax（vt/γ）作为下一个输入，其中γ是温度参数。 其次，我们使用Professor Forcing（Lamb et al.，2016）来匹配隐藏状态序列，包含输出信息且平滑分布的输出词。也就是说，到判别器D1的输入是由实例x1强制的（1）G（y1；z1）教师的隐藏状态序列，或者由先前的软分布自馈的（2）G（y1；z2）教师的隐藏状态序列。 交叉对齐自动编码器的运行过程如图2所示。注意，交叉对齐加强了潜在变量z在生成器G的递归网络上的对齐。通过对齐整个隐藏状态序列，它可以防止z1和z2的初始不对齐在递归生成过程中传播，因此转移的句子可能会结束在远离目标的某个地方目标域。 我们使用卷积神经网络实现D1和D2序列分类（Kim，2014）。算法1给出了训练算法。 5 Experimental setup Sentiment modification 我们的第一个实验是以改变潜在情绪为目标的文本改写，这可以看作是否定句和肯定句之间的风格转换。我们在Yelp餐厅评论上进行实验，利用与每个评论相关的现成用户评分。按照标准惯例，评分高于三分的评审被认为是积极的，低于三分的评审被认为是消极的。当我们的模型在句子层次上运行时，数据集中的情感注释在文档层次上提供。我们假设文档中的所有句子都有相同的情感。这显然过于简单化了，因为有些句子（例如背景）是情绪中立的。考虑到这样的句子在长评论中更为常见，我们过滤掉超过10句的评论。我们进一步过滤剩下的句子，去掉那些超过15个单词的句子。结果数据集有25万个否定句和35万个肯定句。将出现少于5次的单词替换为“”标记后，词汇量为10K。作为基线模型，我们与Hu et al.（2017）的对照control-gen模型进行了比较。 为了定量评估转换的句子，我们采用了一种基于模型的评估标准，类似于用于图像传输的评估标准（Isola et al.，2016）。具体地说，我们根据预先训练好的情感分类器来衡量一个转移句有多少正确的情感。为此，我们使用Kim（2014）中描述的TextCNN模型。在我们简化的风格转换数据集上，它几乎达到了97.4%的完美准确率。 虽然定量评估提供了一些传输质量的指标，但它并没有涵盖这一代任务的所有方面。因此，我们还对从测试集2中随机选取的500个句子进行了两次人类评估。在第一次评估中，评委们被要求根据句子的流畅性和情感程度对生成的句子进行排名。流利程度从1分（不可读）到4分（完美），而情感类别是“积极”、“消极”或“两者都不是”（可能是矛盾的、中性的或无意义的）。在第二个评价中，我们对转移过程进行了比较评价。以随机顺序向注释者展示源句和系统的相应输出，并询问“哪一个转移句在语义上等同于具有相反情感的源句？”？”. 它们可以都令人满意，A/B更好，也可以都不令人满意。我们为每个问题收集两个标签。标签协议和冲突解决策略可在补充材料中找到。请注意，这两个评估不是多余的。例如，一个系统总是独立于源句生成语法正确、情感正确的句子，在第一个评估设置中得分较高，但在第二个评估设置中得分较低。 Word substitution decipherment我们的第二组实验涉及字词替换密码的破译，这在NLP文献中已有探讨（Dou和Knight，2012；Nuhn和Ney，2013）。这些密码将明文（自然语言）中的每个单词根据1:1替换密钥替换为密码令牌。解密任务是从密文中恢复明文。如果我们能够访问并行数据，这任务是微不足道的。然而，我们有兴趣考虑一个非并行破译方案。为了训练，我们选择200K个句子作为X1，并对不同的200K个句子集应用替换密码f得到X2。虽然这些句子是非平行的，但它们是从评论数据集中的相同分布中提取的。开发集和测试集有100K个平行句D1和D2。我们可以使用Bleu分数定量比较D1和转移（解密）D2（Papineni et al.，2002）。 显然，这个破译任务的难度取决于替换词的数量。因此，我们根据替换词汇表的百分比来报告模型性能。注意，转移模型不知道f是一个词替换函数。他们完全从数据分发中学习。 除了有不同的迁移模型外，我们还引入了一个简单的基于词频的解码基线。具体来说，我们假设X1和X2之间共享的单词不需要翻译。其余的单词根据频率进行映射，任意断开连接。最后，为了评估任务的难度，我们报告了在平行语料库上训练的机器翻译系统的准确性（Klein et al.，2017）。 Word order recovery我们最后的实验集中在单词排序任务，也称为bag翻译（Brown et al.，1990；Schmaltz et al.，2016）。通过学习原始英语句子X1和混叠英语句子X2之间的语体迁移函数，该模型可以用来恢复混叠句子的原始语序（或者反过来随机排列句子）。非平行训练数据和平行测试数据的构造过程与单词替换破译实验相同。同样，传输模型不知道f是一个随机函数，完全从数据中学习 6 Results 7 Conclusion之前的迁移学习都是使用平行语料，本文工作中，我们将其定义为翻译问题，并使用非平行的语料。我们的方法使用强制的分布表示对齐去优化神经网络，我们使用情感迁移任务、文本破译、单词排序等任务证明我们方法的有效性。本文也引出了一个问题：when can thejoint distribution p(x1; x2) be recovered given only marginal distributions?，我们认为解决此问题可以促进情感迁移在CV和NLP中的发展。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"Towards Fine-grained Text Sentiment Transfer 笔记","slug":"FGST","date":"2021-01-12T23:53:55.000Z","updated":"2021-01-19T08:27:34.718Z","comments":true,"path":"2021/01/13/FGST/","link":"","permalink":"http://renxingkai.github.io/2021/01/13/FGST/","excerpt":"","text":"论文链接Abstract本文专注于细粒度的情感迁移(FGST)，该任务的目标是在保留原始语义内容的同时，修改输入序列以满足给定的情感强度。不同于传统的情绪传递任务，它只反转文本的情绪极性（正/负），FTST任务需要更细致和细粒度的情绪控制。为了解决这个问题，我们提出了一个新的Seq2SentiSeq模型。具体地，通过高斯核层将数字情感强度值并入解码器以精细地控制输出的情感强度。此外，针对并行数据不足的问题，提出了一种循环强化学习算法来指导模型训练。在这个框架中，精心设计的奖励可以平衡情感转换和内容保存，同时不需要任何ground truth的输出。实验结果表明，该方法在自动评价和人工评价方面均优于现有方法。 1 Introduction为了对文本生成进行更细致、更精确的情感控制，我们转向细粒度文本情感转移（FTST），它在保持语义内容不变的情况下，修改一个序列以满足给定的情感强度。例子如下： FTST任务有两个主要挑战。首先，在生成句子时很难实现情感强度的细粒度控制。以前关于粗粒度文本情感迁移的工作通常对每个情感标签使用单独的解码器（Xu et al.，2018；Zhang et al.，2018b）或将每个情感标签嵌入单独的向量（Fu et al.，2018；Li et al.，2018）。然而，这些方法对于细粒度文本情感迁移是不可行的，因为目标情感强度值是一个实际值，而不是离散的标签。第二，平行数据在实践中是不可用的。换句话说，我们只能访问标有细粒度情绪评级或强度值的语料库。因此，在FTST任务中，我们不能通过ground truth输出来训练生成模型。 针对上述两大挑战，我们提出了两个相应的解决方案。首先，为了控制生成句子的情感强度，我们提出了一种新的情感强度控制序列对序列（Seq2Seq）模型Seq2SentiSeq。它通过高斯核层将情感强度值引入到传统的Seq2Seq模型中。通过这种方式，该模型可以鼓励在解码过程中生成情感强度接近给定强度值的词。其次，由于缺乏并行数据，我们不能直接用极大似然估计（MLE）对模型进行训练。因此，我们提出了一种循环强化学习算法来指导模型训练，而不需要任何并行数据。设计的奖励可以平衡情感迁移和内容保存，同时不需要任何地面真相输出。 本文主要贡献如下： 提出了一种情感强度控制的生成模型Seq2SentiSeq，通过高斯核层引入情感强度值，实现对生成句子的细粒度情感控制。 为了适应非并行数据，我们设计了一种循环强化学习算法CycleRL来指导模型的无监督训练。 实验表明，该方法在自动评价和人工评价两方面均优于现有系统。 2 Proposed Model2.1 Task Definition给定一个输入序列x和一个目标情绪强度值$v_y$，FTST任务的目标是生成一个序列y，该序列y不仅表达了目标情绪强度$v_y$，而且保留了输入x的原始语义内容。在不损失通用性的前提下，我们将情绪强度值$v_y$限制在0（最负）到1（最负）之间阳性。 2.2.1 Encoder使用双向RNN对句子进行编码，获取每个词的表示$h_i$。 2.2.2 Decoder给定输入句子的隐层表示$\\{h_i\\}_{i=1}^m$和目标情感强度值$v_y$，decoder目标去生成序列y,不仅与x描述相同的语义，同时表达出了情感$v_y$。 为了在解码过程中达到控制情感的目的，我们首先在原有的语义表示之外，在每个词中嵌入一个额外的情感表示。语义表征表征词的语义内容，情感表征词表征情感的强度。形式上，解码器在时间步t的隐藏状态st计算如下： 与传统的Seq2Seq模型（Bahdanau et al.，2014）类似，整个词汇表的语义概率分布计算如下 情绪概率度量生成序列的情绪强度与目标vy的接近程度。通常，每个词都有特定的情感强度。例如，单词“好”的强度约为0.6，“好”的强度约为0.7，“好”的强度约为0.8。然而，当涉及到之前生成的词时，当前生成词的情感强度可能完全不同。例如，短语“不好”的负强度约为0.3，而“非常好”的负强度约为0.9。也就是说，每个词在时间步t的情感强度应该由情感表示$E_s$和当前解码器状态$s_t$两者决定。因此，我们定义了一个情感预测的函数$g(E_s,s_t)$直观地说，为了实现情感的细粒度控制，情感强度更接近目标情感强度值$v_y$的词应该被赋予更高的概率。 受Luong et al.（2015）和Zhang et al.（2018a）的启发，为了支持情感强度接近vy的词，我们引入了一个高斯核层，该层以vy为中心放置高斯分布。具体而言，情绪概率表示为： 为了平衡情感转换和内容保存，将整个词汇表的最终概率分布pt定义为两个概率分布的混合 2.3 Training: Cycle Reinforcement LearningFTST任务的一个严重挑战是缺乏并行数据。由于ground truth输出y是不可观测的，因此不能直接用极大似然估计（MLE）进行训练。为此，我们设计了一种循环强化学习（CycleRL）算法。算法1概述了训练过程。两个奖励旨在鼓励改变情绪，但保留内容，而不需要平行数据。下面介绍Seq2SentiSeq模型的两个奖励和相应的梯度的定义。 2.3.1Reward Design我们为FTST的两个目标(sentiment transformation and content preservation)分别设计两个rewards，然后使用一个全局reward r去平衡两个目标和指导模型训练。 Reward for sentiment transformation.一个预先训练的情感评分器用来评估样本句子$\\hat{y}$与目标情感强度值$v_y$的匹配程度。具体而言，情绪转化的回报公式如下： Reward for content preservation.直观地说，如果模型在内容保存方面表现良好，则很容易对源输入x进行反向重构。因此，我们将内容保存的报酬设计为基于生成的文本$\\hat{y}$和源情感强度值$v_x$的模型重构x的概率。 Overall reward.最终整体的reward。 3 Experimental Setup3.1 Dataset我们在Yelp数据集5上进行实验，该数据集包含大量的产品评论。每一篇评论都被分配了一个从1到5的情绪等级。由于细粒度评分中人与人之间的标签不一致更为严重，因此我们对Jaccard相似度大于0.9的句子进行平均评分。然后，将平均评级标准化为0到1之间的情绪强度。其他数据预处理同沈等（2017）。最后，我们得到了总共64万个句子。我们随机分为630k训练，10k验证，500k测试。尽管训练数据集的情绪强度分布不均匀，但该框架由一个均匀的数据扩充组成，该扩充生成的句子强度来自区间[0，1]，步长为0.05，以指导模型训练（算法1中的步骤6）。 3.3 Baselines细粒度系统旨在修改输入句子以满足给定的情感强度。Liao et al.（2018）构建伪平行语料库来训练一个模型，该模型是一个修正的VAE和一个耦合组件的组合，该组件对伪平行数据进行建模，并具有三个额外的损失L。此外，我们还考虑了SCSeq2Seq（Zhang等人，2018a），这是在对话生成中提出的一种特异性控制的Seq2Seq模型。为了适应这种无监督的任务，提出的CycleRL训练算法被用来训练SC-Seq2Seq模型。 粗粒度系统旨在反转输入的情绪极性（正/负），这可以被视为情绪强度设置为低于平均值（负）或高于平均值（正）的特殊情况。我们将我们提出的方法与以下最先进的系统进行了比较：CrossAlign（Shen et al.，2017）、MultiDecoder（Fu et al.，2018）、DeleteRetrieve（Li et al.，2018）和Unpaired（Xu et al.，2018）。 3.4 Evaluation Metrics3.4.1 Automatic Evaluation Content (BLEU) Fluency (PPL) Sentiment 3.4.2 Human Evaluation4 Results and Discussion 5 Related Work近年来，关于无监督情绪传递的文献越来越多。这项任务的目的是翻转一个句子的情感极性，但保持其内容不变，没有平行数据。然而，对于情感的细粒度控制的研究却很少。Liao et al.（2018）通过启发式规则利用伪并行数据，从而将此任务转化为监督任务。然后提出了一种基于变分自动编码器（VAE）的模型，首先将内容因子和源情感因子分离，然后将内容因子和目标情感因子结合起来。然而，伪并行数据的质量并不理想，这严重影响了VAE模型的性能。与之不同的是，我们在训练过程中通过反译（Lample等人，2018b）动态更新伪并行数据（等式12）。 NLP的其他一些任务也对控制文本生成的细粒度属性感兴趣。例如，Zhang et al.（2018a）和Ke et al.（2018）提出控制对话生成的特异性和多样性。我们从这些文章中借鉴了一些想法，但我们的工作动机和提出的模式却与之相去甚远。主要区别在于：（1）由于情感依赖于局部语境，而特异性独立于局部语境，因此我们的模型中有一系列的设计来考虑局部语境（或先前生成的词）st（例如，公式1，公式3）。（2） 由于缺乏并行数据，我们提出了一种循环强化学习算法来训练所提出的模型（第2.3节）。 6 Conclusion我们提出了一个Seq2SentiSeq模型来控制生成句子的细粒度情感强度。为了在没有任何并行数据的情况下训练所提出的模型，我们设计了一种循环强化学习算法。我们将所提出的方法应用于Yelp评论数据集，获得了自动评估和人工评估的最新结果。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"图神经网络相关学习","slug":"graph-embedding","date":"2021-01-09T10:00:56.000Z","updated":"2021-01-11T03:09:40.943Z","comments":true,"path":"2021/01/09/graph-embedding/","link":"","permalink":"http://renxingkai.github.io/2021/01/09/graph-embedding/","excerpt":"","text":"只用于记录学习，图侵删。 图学习算法分类结构图 Deep Walk &amp; Node2vec GraphSAGE","categories":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"}],"tags":[{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"}],"author":"CinKate"},{"title":"Style Transformer：Unpaired Text Style Transfer without Disentangled Latent Representation 笔记","slug":"style-transfomer-paper","date":"2021-01-06T21:31:33.000Z","updated":"2021-01-11T03:19:15.909Z","comments":true,"path":"2021/01/07/style-transfomer-paper/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/style-transfomer-paper/","excerpt":"","text":"论文链接Abstract在未配对的文本风格转换中，对潜空间中的文本内容和风格进行消解是一种普遍现象。然而有两个主要的问题存在现在的大多数神经网络中：1)从一个句子的语义中完全剔除风格信息是很困难的；2)基于递归神经网络（RNN）的编码器和解码器，在潜在表示的中介下，不能很好地处理长期依赖的问题，导致非文体语义内容保存不好。在本文中，我们提出了Style Transfomer，它不需要假设源句的潜在表示，而是在Transformer中配置注意机制，以实现更好的风格转换和内容保留。 1.Introduction语篇风格转换的任务是改变语篇的风格属性（如情感），同时在语境中保留与风格无关的内容。由于文本风格定义比较模糊，构建相同内容但文本风格不一样的句子对是比较困难的。因此文本风格迁移研究主要关注在未成对的文本句子。 神经网络成为了主流的文本风格迁移工具，主要使用Seq2Seq结构，encoder负责编码句子到一个vector，decoder生成不同风格的文本但是保留了原来的内容。这些方法都在专注于如何分清潜在语义空间中的content和style。由于没有成对的句子，对抗loss被使用到潜在空间去促进潜在空间中的风格信息。 现在面临的一些困难： 在潜在语义空间中，区分content和style是困难的， 区分content和style是没有必要的，一个好的风格迁移器，其实不用区分content和style，最好进行直接改写并输出。 受限的vector表示，会使得较长句子编码损失信息 为了理清潜在空间中的content和style，所有的现存方法都假设输入句子被编码到一个固定大小的潜在空间。这些方法因此不能使用attention机制去加强输入句子中的信息。 大多数模型使用RNN去作为encoder和decoder,对于较长句子编码效果并不好。 本文使用Transfomer作为基础模块。贡献如下： 我们提出一个创新的训练算法，不假设需要理清输入句子的content和style，因此，模型可以使用attention机制去提升性能。 首次使用Transfomer在风格迁移任务中。 在两个数据集中我们模型效果都很好，特别地，在content保留中，Style Transfomer取得了较大性能的提升。 2.Related Work3.Style Transformer3.1 Problem Formalization假设有K种不同的数据集Di,可以定义K种不同的风格，文本风格迁移目标是给一个随机的自然语言句子x和一个想要的风格s，需要让机器重写句子x成为新的句子y,句子y包含新的风格s,同时又保留了x的文本信息(语义)。 3.2 Model Overview本文目标是学习一个映射函数f(x,s) ，x是一个自然语言句子，s是一个风格控制变量。函数f的输出是迁移之后的句子y。 3.3 Style Transformer NetworkTransformer是一个标准的encoder-decoder结构，对于一个输入句子x=(x_1,x_2,...,x_n) 编码器encoder将输入映射到连续表示z=(z_1,z_2,...,z_n) 解码器根据条件概率和自回归的方式生成输出句子：p_\\theta(y|x)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 在每一时间步t，下一个token通过softmax分类器来计算：p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;)=softmax(o_t) ot是Transfomer decoder的输出。 为了能够控制风格，我们在Transformer Encoder中加入了一个额外的风格向量:Encoder(x,s,\\theta_E) 因此，网络的输出变为:p_\\theta(y|x,s)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 我们定义网络的预测输出为f_(x,x) 3.4 Discriminator Network因为大部分情况下缺少并行语料来进行有监督训练，我们提出了使用判别网络去从非平行语料中学习监督信息。我们构建了两种方法 为了保留内容信息，当我们将迁移语句$\\hat{y}=f_\\theta(x,\\hat{s})$送到带有原始标签s的Style Transfromer中时，我们训练网络重构原始输入语句x 对于风格控制，我们训练了一个判别网络去更好地控制被生成的句子。 判别网络是另一个TRM encoder,学习区分不同句子的风格。Style TRM网络接收风格监督信息从这个判别网络中。我们实验了两种判别网络： Conditional Discriminator 一个句子x和一个风格s被输入到判别网络$d_{\\phi}(x,s)$判别器需要输出句子x是否含有相关的风格。在判别器训练阶段，数据集x中的真是句子和重构句子$y=f_{\\theta}(x,s)$被标记为正样本，迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为负样本。在Style TRM训练阶段，网络$f_\\theta$被训练最大化正样本概率当输入$\\hat{y}=f_\\theta(x,\\hat{s})$和$\\hat s$到判别器中。 Multi-class Discriminator 仅有一个句子x被输入到判别器$d_{\\phi}(x)$，判别器的目标是去回答此句话的风格。更具体地说，判别器是具有K+1类的分类器。前K类代表K种不同的风格，最后一类代表$f_\\theta(x,\\hat s)$生成的数据，也常被称为假样本。在判别器训练阶段，我们标记真实句子x和重构句子$y=f_{\\theta}(x,s)$为相关的风格，至于被迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为类别0。在Style TRM学习阶段，我们训练网络$f_\\theta(x,\\hat{s})$最大化代表风格$\\hat s$的概率。 3.5 Learning Algorithm该模型的训练算法可分为两部分：判别器学习和风格变换网络学习。架构图如图2所示 3.5.1 Discriminator Learning我们训练我们的判别器来区分真实句字x和重构句子$y=f_{\\theta}(x,s)$与迁移句子$\\hat{y}=f_\\theta(x,\\hat{s})$。损失函数为交叉熵算法细节： 3.5.2 Style Transformer Learning根据$s=\\hat s$和$s!=\\hat s$分为两种情况。 Self Reconstruction 对于$s=\\hat s$，输入句子x和风格s都是来于相同数据集，可以直接训练Style TRM，去重构输入句子，通过最小化-log likehood: L_&#123;self&#125;(\\theta)=-p_&#123;\\theta&#125;(y=x|x,s) 对于$s!=\\hat s$，没有监督数据，不能直接获得loss，因此提出两种不同的训练loss： Cycle Reconstruction为了鼓励生成的句子保留输入句子x中的信息，我们将生成的句$\\hat{y}=f_\\theta(x,\\hat{s})$以x的风格输入给Style TRM，并训练我们的网络通过最小化负对数似然来重构原始输入句子： Style Controlling如果我们只训练我们的Style TRM从转移句$\\hat{y}=f_\\theta(x,\\hat{s})$重构输入句x，网络只能学习将输入复制到输出。为了处理这个退化问题，我们进一步为生成的句子添加了风格控制损失。即将网络生成的句子$\\hat y$输入到判别器中，以最大限度地提高$\\hat s$的概率。对于conditional discriminator，Style TRM的目标是在使用样式标签$\\hat s$输入到判别器时最小化类1的负对数似然性： 在multi-class discriminator的情况下，Style TRM以最小化对应类型的风格$\\hat s$的负对数似然： 组合loss之后，Style TRM学习过程如下： 3.5.3 Summarization and Discussion与GANs的训练过程类似（Goodfellow et al.，2014），在每个训练迭代中，我们首先执行$n_d$步鉴别器学习以获得更好的鉴别器，然后训练我们的Style TRM$n_f$步以提高其性能。算法3对训练过程进行了总结 由于自然语言的离散性，对于生成的句子$\\hat{y}=f_\\theta(x,\\hat{s})$，我们不能通过离散样本直接传播来自判别器的梯度。在我们的实验中，我们还观察到Gumbel-Softmax技巧会减慢模型的收敛速度，并且没有给模型带来太多的性能改进。基于以上原因，我们将fθ生成的softmax分布视为一个“软”生成句子，并将该分布输入给下游网络，以保持整个训练过程的连续性。当使用这种近似时，我们也将我们的解码器网络从贪婪解码切换到连续解码。也就是说，在每一个时间步，我们将整个softmax分布（等式（2））输入到网络，而不是将在先前预测步骤中具有最大概率的令牌馈送到网络。解码器利用这个分布，从输入的嵌入矩阵计算加权平均嵌入。 4 Experiment4.1 DatasetsYelp Review Dataset (Yelp) Yelp数据集是由Yelp数据集挑战提供的，由带有情绪标签（负面或正面）的餐馆和商业评论组成。在之前的工作之后，我们使用了Li等人（2018）提供的数据集。此外，它还为测试集提供人类参考句。 IMDb Movie Review Dataset (IMDb) IMDb数据集由在线用户撰写的影评组成。为了获得高质量的数据集，我们使用了Maas等人（2011）提供的极性电影评论。基于此数据集，我们通过以下步骤构建了一个高度极性的句子级风格转换数据集：1）在原始训练集上微调一个BERT（Devlin et al.，2018）分类器，在测试集上达到95%的准确率；2）将原始数据集中的每个评论分成几个句子；3） 通过我们微调的BERT分类器过滤掉置信阈值低于0.9的句子；4）删除不常见单词的句子。最后，这个数据集包含366K、4k和2k个句子，分别用于训练、验证和测试. 4.2 Evaluation目标转移句应该是一个流利的、内容完整的、具有目标风格的句子。为了评估不同模型的表现，我们在前人工作的基础上，比较了生成样本的三个不同维度：1）风格控制，2）内容保留(BLEU)，3）流畅性(perplexity)。 4.3 Training Details在所有的实验中，对于编码器、解码器和鉴别器，我们都使用了四层TRM，每层有四个注意头。Transformer中的隐藏大小、嵌入大小和位置编码大小都是256维。另一个包含256个隐藏单元的嵌入矩阵用来表示不同的风格，作为输入语句的额外标记输入到编码器中。并且位置编码不用于样式标记。对于鉴别器，类似于Radford et al.（2018）和Devlin et al.（2018），我们进一步在输入中添加标记，并将相应位置的输出向量馈送到表示鉴别器输出的softmax分类器中。 4.4 Experiment Results实验结果一些迁移例子 5.Conclusions and Future Work本文我们提出了Style Transfomer，实验结果在两个数据集中展示出了我们模型比之前的SOTA模型有竞争力的结果，特别地是，由于我们提出的方法没有假设一个分离的潜在表示来操纵句子的风格，我们的模型可以在两个数据集上得到更好的内容保留。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"常见的评估指标","slug":"metrics","date":"2021-01-06T19:29:52.000Z","updated":"2021-01-11T03:10:07.812Z","comments":true,"path":"2021/01/07/metrics/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/metrics/","excerpt":"","text":"准确率，精确率，F1值，ROC，AUC，P-R，mAP(图来自网络，侵删)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"}],"tags":[{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"}],"author":"CinKate"},{"title":"搜索中的NLP技术","slug":"searchandnlp","date":"2020-12-26T10:25:23.000Z","updated":"2021-01-06T11:33:26.871Z","comments":true,"path":"2020/12/26/searchandnlp/","link":"","permalink":"http://renxingkai.github.io/2020/12/26/searchandnlp/","excerpt":"","text":"搜索技术除了涉及基础的搜索算法，也涉及到很多NLP技术，本文转载于，只是做个学习记录，侵删。 推荐系统被捧为目前算法领域的主流，推荐系统不需要用户主动进行操作就能获取自己喜欢的东西，但是实际上，搜索系统在很长一段时间占据了重要位置，大到百度的大搜，小到音乐、视频、电商、应用商店等，都有各种各样的搜索引擎，这些搜索搜索能更为精准直接的满足用户需求，即使是推荐系统如日中天，目前也仍会有搜索的一席之地。今天我来为大家介绍，搜索系统中涉及的算法问题，也让大家了解，搜索中需要什么算法。 数据预处理模块这个很好理解吧，用户query进来，一般要做如下处理: 大小写转化。 删除标点符号（当然有的分析会保留标点，但是建议在这个场景下还是去掉更好）。 繁体转简体。 数字处理，转中文，转阿拉伯数字，甚至罗马字等，部分情况要删除数字。 长度截断。（总不能让用户输入一本西游记，然后你还在整吧？） 理解模块query理解其实是一个非常重要的模块，好的query理解能为后续的工作提供很大的支持。这部分往往不直接应对下游，而是按需使用，有点像辅助吧。 分词。基操。 关键词抽取。会涉及丢词、排序等功能。 命名实体识别。一方面协助用户识别，另一方面可能会涉及数据库查询匹配的内容。在垂搜中比较常见，大搜也有但是相比之下没那么精准和常见。 紧密度分析，避免由于切词出现错误导致词义偏移的问题，这个其实并不少见，尤其是在人名上，这个是别的精准度会很低，近期的如“武大靖”，会被分为“武大 靖”，“曾舜晞”直接被分为了3个字，挺头疼的。 非法信息过滤。例如黄色、暴力血腥、反动等。 改写模块改写模块其实非常关键，这是连接用户query和数据库底层数据的桥梁，数据库的存储有特定的形式，但是用户不会按照你的底层数据结构去写，例如，用户不见得会输入和平精英，而是吃鸡，数据库里可不见得会存吃鸡对吧，所以这块很重要。 同义词改写。上面的吃鸡就要改写为和平精英，这个需要通过同义词挖掘等方式去构造词典实现。 拼音改写。数据库是罗密欧与朱丽叶，但是用户输入的是罗密欧与朱莉业，拼音改写其实颇为常见，用户经常由于找不到需要的结果或者不知道应该需要哪个，于是直接输入后开始搜索。 前缀补全。非常常见，用户输入射雕，射雕英雄传就要出了，这个一般的方法也是构造词典，另外有一个很重要的需要了解的就是前缀树，这个能让你查询的时间非常低的水平（只和query长度本身有关）。 丢词和留词。结合上述关键词提取和命名实体识别完成，有些不必要的词汇需要被删除，例如“李小璐到底怎么了”，整个句子只有李小璐是关键词，其他词如果也通过and逻辑召回，就没有信息召回了，这时候其实可以直接删除或者将降级到or逻辑。留词和丢词相反，丢词如果是做减法，留词就是做加法。 近义词召回。这个召回不是从数据库中召回，而是召回近义词，具体的方法是通过embedding方法转化词汇，然后通过ball tree、simhash的方式召回与之意思相近的词汇，该模式虽然比较激进，但是能一定程度增加召回，有一定效果。 意图识别。与其说是意图，我更喜欢理解为这是直接针对底层数据结构产生的需要解决的问题，query这是一条，但是数据库有好几个，我们要去哪个数据库搜，这是需要识别的，而这个数据库的设计往往和品类、意图有关，找酒店、找景点都是不同的，所以此时就要进行意图识别，一般地方法是抽象为文本分类，但是很多时候语义本身是无法体现出真实意图的，例如少年的你，语义上其实很难分析出，有时候更复杂的会夹带一些实体识别、词性、词典之类的信息。 召回模块结合命名实体识别、改写结果，然后开始召回，模式比较多，包括但不限于下面形式： Elastic Search，著名的搜索平台ES，有些时候甚至简单的搜索平台直接用它管整个搜索引擎但是要精做，就只能把它当做底层的数据库和搜索平台。 关系数据库。MySQL之类的，但是在我的理解速度和并发性都不是特别好。 Redis。KV形式的数据库，速度很快，但是Key匹配必须是精确匹配，这需要改写模块具有很强的能力。 说白了，就是把用上面流程处理过的query放到数据库里面查，这个其实就是召回。 排序模块内容是召回回来了，其实怎么展现给用户呢？这里是需要深度挖掘用户的实际需求的，很多时候甚至需要做个性化，但是更多时候是我们能够得到的只有短短的一句用户query，那么，我们就要好好利用好这个query，来做好排序，让用户喜欢的（当然，还有广告商喜欢的哈哈哈）东西放在前面，实际上就是一个学习排序的问题了，这个我之前也做过一些简单的介绍，那么在特征层面，可以考虑如下的信息： query层面： 本身的embedding，后续迭代可以走elmo后逐渐形成的pre-train的模式。 词性、实体、词权重、offset等序列标注得到的结果。 document层面。 如果文档本身有文本，最好是标题类的，也把embeding引入，和query层面那种一样。如果只有query文本和document文本，其实就是个文本相似度模型了对吧。 综合层面： query和document的ctr、cqr、bm25，句向量余弦相似度等匹配信息。 其他层面： 意图识别结果。 模型上，DSSM系列似乎是比较流行的方法，但是提取一些特定的特征，有点的时候简单的LR、XGBOOST就能一定程度把问题解决好了。 而在排序模块中，还会涉及一些硬规则等。 搜索引导模块query suggestion是一个上述搜索过程非常类似的模块，只不过处理的结果大部分是放在离线，在线是指一个查字典的过程，那么离线部分，其实是做了一整个搜索过程的：预处理、query理解、改写、召回、排序。 预处理：和上面的预处理类似，不赘述。 query理解，和上述内容类似，但是有的时候会简化，直接进入改写模块。 改写模块，会进行容忍度更高的前缀匹配，去找回一些用户可能会喜欢的内容，这时候往往还会带上统计特征，从整体层面上看用户喜欢的内容，毕竟不同时间用户喜欢的可能不同，例如对热点新闻的偏好。 召回和排序模块。召回模块不是召回数据了，而是召回相似的doc信息，也可以是相似的历史用户query，尤其是后者，如果是后者，则要确认历史用户query是有结果的。 其他辅助模块显然，整个搜索系统远远不止这些内容，在算法视角下，其实还需要很多辅助模块协助我们进行算法开发。 日志模块。无论是线上的运行日志，还是线下的模型实验和离线模型运行，都需要日志协助，对于线上运行和离线模型运行而言，出现错误可以方便追溯，找到ERROR出现的时间和具体问题，而线下的模型试验能方便我们计算运行时间、找到bug，而非每次训练模型的时候功亏一篑才来找问题。 实验模块。由于算法策略存在很多不确定性，无论是算法内存占用和时间，还是算法实际结果，因此需要利用AB实验的方式来验证，快速进行分析，对有问题的算法及时下线检查原因。 数据分析和报表模块。结合实验模块，需要对日常用户数据进行分析，这个比较好理解，不赘述。 特征生产模块。特征生产涉及两块，一个是线上的实时计算，对于一些实时数据，是需要即时生成的，例如微博热搜里面就需要一些诸如5分钟搜索量之类的特征，这个只能从实时计算模块中获取；另一个是离线模块，为了进行离线模型训练，需要定时将生肉（原始数据）转化为特征，方便进行下一步计算，如果这个能变成日常任务，那开发人员就不需要临时造轮子执行，还要长时间等待。 定时任务模块。很多任务其实是定时开始的，报表生成、特征生产，甚至是一些实时任务（说白了就是短时间内的计算），因此有定时任务模块去管理这些定时任务，将会非常方便。 模型管理模块。首先一个项目中可能存在大量模型，然后因为各个模型训练需要花费大量资源，还要结合AB试验，另外还有模型的更新和保存机制（防止模型加载失败导致的无模型可用），因此需要构建模型模块统一管理模型，这个不是每个部门都有，或者说每个模型都是各自管理，但是有一个比较好管理模型平台是非常高效的。 数据平台。额，简单地说就是写SQL的地方，但是也有类似ETL之类的内容，和特征生产模块很接近。 小结搜索作为一个完整地系统，难度甚至比搜索要困难，在更多的场景下，搜索系统只能针对短短几个字的query进行分析，从而在海量数据中找到用户需要的东西，而由于是用户主动输入的，所以用户的预期非常高，但因为是用户主动的行为，所以不会太过复杂，甚至可能会有各种各样的歧义，只有详细分析和挖掘才能得到，因此在我看来，搜索这个事情非常具有挑战性，即使这个东西已经发展多年，年龄比推荐系统更大，还是很多可挖掘的地方。","categories":[{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"}],"tags":[{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"}],"author":"CinKate"},{"title":"Git常用复习","slug":"git-learn","date":"2020-12-23T20:55:42.000Z","updated":"2020-12-23T12:57:43.570Z","comments":true,"path":"2020/12/24/git-learn/","link":"","permalink":"http://renxingkai.github.io/2020/12/24/git-learn/","excerpt":"","text":"版本控制的分类 本地版本控制，如RCS。最常见，最简单的版本控制方法。 集中版本控制，如SVN。所有的版本数据都保存在服务器上，协同开发者从服务器上同步更新或上传自己的修改。必须联网。。。 分布式版本控制，如Git。所有版本信息仓库全部同步在本地的每个用户中，可以在本地查看所有版本历史，可以在本地离线提交，有网了之后push到服务器。但是每个人都有全部代码，存在安全隐患。 Git原理 git add files 将本地文件添加到暂存区stage，git commit 将暂存区文件提交到本地的git仓库，git push将本地仓库文件提交到远程仓库，如下图左边，右边对应反向操作 Workspace：工作目录，平时存放代码的位置 Index/Stage：暂存区，用于临时存放你的改动，事实上只是一个文件，保存即将提交到文件列表信息 Repository：仓库区(或本地仓库)，就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本 Remote：远程仓库，托管代码的服务器，可以简单地认为是你项目组中的一台电脑用于远程数据交换 Git提交时忽略的文件在主目录下建立”.gitinore”文件，此文件以下规则常用#为注释*.txt #忽略所有.txt结尾的文件,这样的话，上传就不会被选中!lib.txt #但lib.txt除外/temp #仅忽略项目根目录下的TODO文件，不包括其他目录tempbuild/ #忽略build/目录下的所有文件doc/*.txt #会忽略 doc/notes.txt 但不包括 doc/server/arch.txt Git分支#列出所有本地分支git branch#列出所有远程分支git branch -r#新建一个分支git branch [branch-name]#虽然新建了但是默认还是在master分支下显示#新建一个分支并切换到该分支git checkout -b [branch-name]#合并指定分支到当前分支git merge [branch]#删除分支git branch -d [branch-name]#删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 多个分支如果并行执行，就会导致代码不会冲突，会同时存在多个版本！ web-api -A组开发 web-admin -B组开发 B会调用A web-app -C组开发 C会调用B和A的代码 如果冲突了要进行协商！ 如果同一个文件在合并分支时都被修改了，则会引起冲突：解决的办法是我们可以修改冲突文件后重新提交！选择要保留他的代码还是你的代码！ master主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分值代码稳定后可以合并到主分支master上来。","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"}],"author":"CinKate"},{"title":"Docker基础学习","slug":"docker-learning","date":"2020-12-01T11:07:19.000Z","updated":"2020-12-22T09:10:06.146Z","comments":true,"path":"2020/12/01/docker-learning/","link":"","permalink":"http://renxingkai.github.io/2020/12/01/docker-learning/","excerpt":"","text":"Docker的常用命令帮助命令docker version #显示docker的版本信息docker info #显示docker的系统信息，包括镜像和容器的数量docker 命令 --help #帮助命令 帮助文档的地址：https://docs.docker.com/engine/reference/commandline/docker/ 镜像命令docker images #查看所有本地主机上的镜像docker images #查看所有本地主机上的镜像[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 11 months ago 13.3kB#解释REPOSITORY #镜像的仓库TAG #镜像的标签IMAGE ID #镜像的IDCREATED #镜像的创建时间SIZE #镜像的大小#可选项Options: -a, --all #列出所有镜像 --digests Show digests -q, --quiet #仅显示镜像的ID docker search #搜索镜像[root@VM-0-15-centos ~]# docker search tensorflowNAME DESCRIPTION STARS OFFICIAL AUTOMATEDtensorflow/tensorflow Official Docker images for the machine learn… 1799 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ … 248 tensorflow/serving Official images for TensorFlow Serving (http… 102 rocm/tensorflow Tensorflow with ROCm backend support 58 xblaster/tensorflow-jupyter Dockerized Jupyter with tensorflow 54 [OK]floydhub/tensorflow tensorflow 26 [OK]bitnami/tensorflow-serving Bitnami Docker Image for TensorFlow Serving 14 [OK]opensciencegrid/tensorflow-gpu TensorFlow GPU set up for OSG 12 docker pull #下载镜像#docker pull mysql 镜像名[:tag][root@VM-0-15-centos ~]# docker pull mysqlUsing default tag: latest #不写tag，默认latestlatest: Pulling from library/mysql852e50cd189d: Pull complete #分层下载,docker 镜像的核心 联合文件系统29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete a880ba7c411f: Pull complete 984f656ec6ca: Pull complete 9f497bce458a: Pull complete b9940f97694b: Pull complete 2f069358dc96: Pull complete Digest: sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093 #签名Status: Downloaded newer image for mysql:latestdocker.io/library/mysql:latest #真实地址#指定版本docker pull mysql:5.7[root@VM-0-15-centos ~]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Already exists a43f41a44c48: Already exists 5cdd802543a3: Already exists b79b040de953: Already exists 938c64119969: Already exists 7689ec51a0d9: Already exists 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7 docker rmi #删除镜像#按docker id镜像删除[root@VM-0-15-centos ~]# docker rmi -f ae0658fdbad5 Untagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Deleted: sha256:a2cf831f4221764f4484ff0df961b54f1f949ed78220de1b24046843c55ac40fDeleted: sha256:0a7adcc95a91b1ec2beab283e0bfce5ccd6df590bd5a5e894954fcf27571e7f5Deleted: sha256:0fae465cbacf7c99aa90bc286689bc88a35d086f37fd931e03966d312d5dfb10Deleted: sha256:23af125b9e54a94c064bdfacc2414b1c8fba288aff48308e8117beb08b38cb19#递归删除所有的镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: mysql:latestUntagged: mysql@sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093Deleted: sha256:dd7265748b5dc3211208fb9aa232cef8d3fefd5d9a2a80d87407b8ea649e571cDeleted: sha256:aac9a624212bf416c3b41a62212caf12ed3c578d6b17b0f15be13a7dab56628dDeleted: sha256:1bf3ce09276e9e128108b166121e5d04abd16e7de7473b53b3018c6db0cf23ffDeleted: sha256:24c6444cea460c3cc2f4e0385e3e97819a0672a54a361921f95d4582583abd59Deleted: sha256:77585ebe3eaa035694084b3c5937fe82b8972aae1e6c6070fc4d7bc391d10928Deleted: sha256:1cfd539163ceb17f7bb85a0da968714fe9258b75dbf73f5ad45392a45cfd34b7Deleted: sha256:c37f414ac8d2b5e5d39f159a6dffd30b279c1268f30186cee5da721e451726eaDeleted: sha256:955b3c214bccf3ee2a7930768137fd7ed6a72677334be67a07c78a622abd318aDeleted: sha256:a2e35a0fdb20100365e2fb26c65357fcf926ac7990bf9074a51cbac5a8358d7eDeleted: sha256:8c3a028fc66f360ce6ce6c206786df68fac4c24257474cbe4f67eda0ac21efd6Deleted: sha256:0a6d37fabaceb4faa555e729a7d97cb6ee193cb97789a213907a3d3c156d7e35Deleted: sha256:579519c51de1afe1e29d284b1741af239a307975197cf6ce213a70068d923231Deleted: sha256:f5600c6330da7bb112776ba067a32a9c20842d6ecc8ee3289f1a713b644092f8Untagged: hello-world:latestUntagged: hello-world@sha256:e7c70bb24b462baa86c102610182e3efcb12a04854e8c582838d92970a09f323Deleted: sha256:bf756fb1ae65adf866bd8c456593cd24beb6a0a061dedf42b26a993176745f6b 容器命令有了镜像才能创建容器，linux，下载一个centos镜像来测试 [root@VM-0-15-centos ~]# docker pull centos 新建容器并启动 docker run [可选参数] image#参数说明--name=&quot;Name&quot; #容器名字-d #使用后台方式运行，类似nohup-it #使用交互方式运行，进入容器查看内容-p #指定容器端口 -p 8080:8080 -p 主机端口:容器端口 (常用) -p 容器端口 容器端口-P #随机指定端口 启动并进入容器 [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@1faa1a0b849e /]# [root@1faa1a0b849e /]# lsbin etc lib lost+found mnt proc run srv tmp vardev home lib64 media opt root sbin sys usr#退出容器[root@1faa1a0b849e /]# exitexit[root@VM-0-15-centos ~]# ls 列出所有运行中的容器 #列出当前在运行的容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES#列出所有运行过的容器[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 4 minutes ago Exited (0) 55 seconds ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov#列出最近创建的容器[root@VM-0-15-centos ~]# docker ps -n=1CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) About a minute ago frosty_cori#只显示容器的编号[root@VM-0-15-centos ~]# docker ps -aq1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36 退出容器 exit #直接容器停止并退出CTRL+P+Q #容器不停止并退出[root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@e5a4a59f28ff /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 21 seconds ago Up 20 seconds agitated_ritchie 删除容器 docker rm 容器id #删除指定的容器,不能删除正在运行的容器，如要删除，加-f#递归删除所有docker容器docker rm -f $(docker ps -aq) #删除所有容器docker ps -a -q|xargs docker rm[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 5 minutes ago Up 5 minutes agitated_ritchiec4fd9794c39a centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) 5 minutes ago dreamy_lumiere1faa1a0b849e centos &quot;/bin/bash&quot; 16 minutes ago Exited (0) 12 minutes ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov[root@VM-0-15-centos ~]# docker rm e5a4a59f28ffError response from daemon: You cannot remove a running container e5a4a59f28ffc678ab9ea5ed7a9af825ff053e1816cd94c90880b3d2a87997df. Stop the container before attempting removal or force remove[root@VM-0-15-centos ~]# docker rm -f $(docker ps -aq)e5a4a59f28ffc4fd9794c39a1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36[root@VM-0-15-centos ~]# docker ps -aq[root@VM-0-15-centos ~]# 启动和停止容器的操作 docker start 容器id #启动 docker restart 容器id #重启docker stop 容器id #停止当前运行的容器docker kill 容器id #强制停止当前容器 常用其他命令后台启动容器 docker run -d centos #后台启动 #docker ps 时候，发现centos会停止，因为容器后台运行，就必须要一个前台进程，docker发现没有应用，就会自动停止 查看日志 docker logs -f -t --tail 10 容器id #发现没有日志 查看容器中的进程信息 docker top 容器id [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@ba393184f315 /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 9 seconds ago Up 8 seconds wizardly_cerf[root@VM-0-15-centos ~]# docker top ba393184f315UID PID PPID C STIME TTY TIME CMDroot 25856 25840 0 09:46 pts/0 00:00:00 /bin/bash 查看镜像的元数据 docker inspect 容器id [root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 3 minutes ago Up 3 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker inspect ba393184f315[ &#123; &quot;Id&quot;: &quot;ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73&quot;, &quot;Created&quot;: &quot;2020-12-01T01:46:02.700180427Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 25856, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2020-12-01T01:46:02.955068318Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;, &quot;Image&quot;: &quot;sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73-json.log&quot;, &quot;Name&quot;: &quot;/wizardly_cerf&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: &#123; &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: &#123;&#125; &#125;, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: &#123;&#125;, &quot;RestartPolicy&quot;: &#123; &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 &#125;, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Capabilities&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] &#125;, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;Mounts&quot;: [], &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;ba393184f315&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: true, &quot;AttachStdout&quot;: true, &quot;AttachStderr&quot;: true, &quot;Tty&quot;: true, &quot;OpenStdin&quot;: true, &quot;StdinOnce&quot;: true, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;Image&quot;: &quot;centos&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123; &quot;org.label-schema.build-date&quot;: &quot;20200809&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot; &#125; &#125;, &quot;NetworkSettings&quot;: &#123; &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;913e925d94cac8aaa4ead2ed29197af7d07d4fbfd35625642019116a42cff156&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: &#123;&#125;, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/913e925d94ca&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08&quot;, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] 进入当前运行的容器 通常容器都是后台运行的，有时我们需要进入容器修改一些配置 docker exec -it 容器id bashShell [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos \"/bin/bash\" 11 minutes ago Up 11 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker exec -it ba393184f315 /bin/bash[root@ba393184f315 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 01:46 pts/0 00:00:00 /bin/bashroot 14 0 0 01:57 pts/1 00:00:00 /bin/bashroot 27 14 0 01:57 pts/1 00:00:00 ps -ef docker attach 容器id [root@VM-0-15-centos ~]# docker attach ba393184f315[root@ba393184f315 /]# docker exec 进入容器后，开启新的终端，可以在里面操作docker attach 进入容器正在执行的终端 从容器拷贝文件到主机 docker cp 容器id:容器内路径 目的的主机路径 [root@VM-0-15-centos home]# docker attach b67b64394534[root@b67b64394534 /]# cd /home [root@b67b64394534 home]# ls[root@b67b64394534 home]# touch rx.java[root@b67b64394534 home]# read escape sequence[root@VM-0-15-centos home]# [root@VM-0-15-centos home]# docker cp b67b64394534:/home/rx.java /home[root@VM-0-15-centos home]# lsrx.java rxk.java Test1-Docker部属nginx#下载Nginx[root@VM-0-15-centos ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx852e50cd189d: Pull complete 571d7e852307: Pull complete addb10abd9cb: Pull complete d20aa7ccdb77: Pull complete 8b03f1e11359: Pull complete Digest: sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest#查看目前运行镜像[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest bc9a0695f571 6 days ago 133MBcentos latest 0d120b6ccaa8 3 months ago 215MB#以后台方式运行镜像，并且将nginx的80端口映射到本地3344端口，同时给此镜像一个名字--name nginx01[root@VM-0-15-centos ~]# docker run -d --name nginx01 -p 3344:80 nginx5a3e8a8e16e5e1f46b8bbe6c23662aebd3c13dc5cacfd63fb24d0703c1235819[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 2 minutes ago Up 2 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland#请求本地3344端口，显示nginx，成功运行[root@VM-0-15-centos ~]# curl localhost:3344&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;#进入容器[root@VM-0-15-centos ~]# docker exec -it nginx01 /bin/bashroot@5a3e8a8e16e5:/# whereis nginxnginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx#关闭容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 33 minutes ago Up 33 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop nginx01nginx01[root@VM-0-15-centos ~]# docker stop centosError response from daemon: No such container: centos[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop b67b64394534b67b64394534 Test2-Docker部属tomcat#下载并启动tomcat (加了--rm 意味着用完就会删除)[root@VM-0-15-centos ~]# docker run -it --rm tomcat:9.0Unable to find image 'tomcat:9.0' locally9.0: Pulling from library/tomcat756975cb9c7e: Pull complete d77915b4e630: Pull complete 5f37a0a41b6b: Pull complete 96b2c1e36db5: Pull complete 27a2d52b526e: Pull complete a867dba77389: Pull complete 0939c055fb79: Pull complete 0b0694ce0ae2: Pull complete 81a5f8099e05: Pull complete c3d7917d545e: Pull complete#正常下载[root@VM-0-15-centos ~]# docker pull tomcat:9.09.0: Pulling from library/tomcatDigest: sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaStatus: Image is up to date for tomcat:9.0docker.io/library/tomcat:9.0#将tomcat 8080端口映射到外部3355端口[root@VM-0-15-centos ~]# docker run -d -p 3355:8080 --name tomcat01 tomcataf2d7437747112b51f83c556b1790503f41e2632bbbe965e00a8bff3d3268c92#进入tomcat[root@VM-0-15-centos ~]# docker exec -it tomcat01 /bin/bashroot@af2d74377471:/usr/local/tomcat# lsBUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.distCONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps work Test3-Docker部属ES+kibanaES暴露的端口较多，十分耗内存，ES的数据一般挂载在安全目录 # --net somenetwork网络配置docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:tag#下载并启动ESdocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.6.2#启动之后 服务器会较为卡顿#查看CPU和内存状态#docker stats #增加内存限制 -e环境配置修改docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms64m -Xmx512m\" elasticsearch:7.6.2CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDSdb5428acc88d elasticsearch02 0.77% 358.6MiB / 1.795GiB 19.51% 656B / 0B 6.41MB / 729kB 43^C[root@VM-0-15-centos ~]# curl localhost:9200&#123; \"name\" : \"db5428acc88d\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"MqrmYUWDTT2jYZJHxsVU0A\", \"version\" : &#123; \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 可视化启动portainer [root@VM-0-15-centos ~]# docker run -d -p 8088:9000 --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainerUnable to find image 'portainer/portainer:latest' locallylatest: Pulling from portainer/portainerd1e017099d17: Pull complete 717377b83d5c: Pull complete Digest: sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Status: Downloaded newer image for portainer/portainer:latestcb19cef5927877f6e10e15013b81fffdaefd5cb2e426f34819fc869c109ab7b3 commit镜像#先启动tomcat 并确保在运行[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 55 seconds ago Up 54 seconds 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#进入tomcat[root@VM-0-15-centos ~]# docker exec -it 675a7261e3b7 /bin/bash#将webapps中的东西复制上去root@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsroot@675a7261e3b7:/usr/local/tomcat/webapps# cd ..root@675a7261e3b7:/usr/local/tomcat# cp -r webapps.dist/* webappsroot@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsROOT docs examples host-manager managerroot@675a7261e3b7:/usr/local/tomcat/webapps# exit[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 7 minutes ago Up 7 minutes 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#commit一个镜像,-a是作者,-m是描述[root@VM-0-15-centos ~]# docker commit -a=\"cinkate\" -m=\"add webapps\" 675a7261e3b7 tomcat02:1.0sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74eb[root@VM-0-15-centos ~]# docker imgaesdocker: 'imgaes' is not a docker command.See 'docker --help'#查看是否commit成功[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat02 1.0 80b6126c75b9 8 seconds ago 654MBnginx latest bc9a0695f571 8 days ago 133MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB docker容器数据卷即使删除了容器，数据还是存在的，类似于数据持久化容器之间应该有一个数据共享的技术！Docker容器中产生的数据，同步到本地！ 这就是卷技术！目录的挂载，将容器的目录，挂载到Linux上面！ 总结：为了容器的持久化和同步操作~，容器间也是可以数据共享！ 使用数据卷 方式一：直接使用命令来挂载 #-v 主机目录，容器内目录#-p 主机端口，容器内端口[root@VM-0-15-centos ~]# docker run -it -v#将centos /home下的目录 挂载到 主机下/home/ceshi下[root@VM-0-15-centos home]# docker run -it -v /home/ceshi:/home centos /bin/bash[root@081342dcbd4f /]# #另一个terminal(本机的home目录)[root@VM-0-15-centos ~]# cd /home[root@VM-0-15-centos home]# lsceshi rx.java rxk.java#查看元信息，挂载成功 Mounts[root@VM-0-15-centos home]# docker inspect 081342dcbd4f[ &#123; \"Id\": \"081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee\", \"Created\": \"2020-12-03T13:39:28.974545204Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 1314, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T13:39:29.238082253Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566\", \"ResolvConfPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hostname\", \"HostsPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hosts\", \"LogPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee-json.log\", \"Name\": \"/clever_lovelace\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": [ \"/home/ceshi:/home\" ], \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/home/ceshi\", \"Destination\": \"/home\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" &#125; ], \"Config\": &#123; \"Hostname\": \"081342dcbd4f\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"centos\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"7cc48bb3849fdd8ca238a8ba1f09409e01e7c50856abbccf23d59caaf5997c8d\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/7cc48bb3849f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;]#容器的home目录下[root@081342dcbd4f home]# touch test.java[root@081342dcbd4f home]# lstest.java#主机挂载的ceshi目录下[root@VM-0-15-centos home]# cd ceshi/[root@VM-0-15-centos ceshi]# lstest.java 关闭容器之后，在主机中挂载的目录中创建，修改文件，都是可以同步到容器中的目录。 Test4–安装MySQL#获取镜像[root@VM-0-15-centos home]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7#运行容器，需要做数据挂载# -d后台启动# -p端口映射# -v 挂载配置文件 和 数据文件# -e 修改环境变量，修改密码# --name 重命名[root@VM-0-15-centos home]# docker run -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.72f81782daf7d49e73365da7def107120a8f79c71b9c5bb21a4fc082f2ebef664#启动成功后，使用本地数据库管理软件可以进行正常连接 假设将容器删除，挂载在本地的数据卷不会丢失，实现了容器的持久化技术。 具名和匿名挂载#匿名挂载-v不写本地路径，会匿名进行挂载docker run -d -P --name nginx01 -v /etc/nginx nginx#具名挂载docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx#通过-v 卷名:容器内路径#查看这个卷所有docker容器内的卷，没有指定目录的情况下：都是在/var/lib/docker/volumes/xxxx/_data通过具名挂载，可以方便的找到一个卷，大多数情况使用具名挂载 如何确定是具名还是匿名挂载？还是指定路径挂载？ -v 容器内路径 #匿名挂载-v 卷名:容器内路径 #具名挂载-v /宿主机路径:容器内路径 #指定路径挂载 拓展 #通过 -v 容器内路径:ro rw 可以改变读写权限#ro readonly#rw readwrite#一旦设定了容器权限，容器对我们挂载出来的内容就有限定了！#ro 说明这个路径只能通过宿主机来操作，容器内部无法操作！docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginxdocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx 初始Dockerfile 方式二：Dockerfile就是用来构建docker镜像的构建文件！通过这个脚本，可以生成镜像，脚本为一个个命令，每个命令都是一层。 #构建一个rxk/centos[root@VM-0-15-centos home]# cd docker-test-volume/[root@VM-0-15-centos docker-test-volume]# ls[root@VM-0-15-centos docker-test-volume]# pwd/home/docker-test-volume[root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# nano dockerfile1#文件中的内容[root@VM-0-15-centos docker-test-volume]# cat dockerfile1 FROM centosVOLUME [\"volume01\",\"volume02\"]CMD echo \"-----end-------\"CMD /bin/bash[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t /rxk/centos .invalid argument \"/rxk/centos\" for \"-t, --tag\" flag: invalid reference formatSee 'docker build --help'.[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t rxk/centos .Sending build context to Docker daemon 2.048kBStep 1/4 : FROM centos ---&gt; 0d120b6ccaa8Step 2/4 : VOLUME [\"volume01\",\"volume02\"] ---&gt; Running in 634e59a80996Removing intermediate container 634e59a80996 ---&gt; 02a6fde9ba35Step 3/4 : CMD echo \"-----end-------\" ---&gt; Running in 3fe4feb278dfRemoving intermediate container 3fe4feb278df ---&gt; d64085501c38Step 4/4 : CMD /bin/bash ---&gt; Running in 09d704b706feRemoving intermediate container 09d704b706fe ---&gt; 7735159af50cSuccessfully built 7735159af50cSuccessfully tagged rxk/centos:latest#查看当前镜像[root@VM-0-15-centos docker-test-volume]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZErxk/centos latest 7735159af50c 5 minutes ago 215MBtomcat02 1.0 80b6126c75b9 7 hours ago 654MBnginx latest bc9a0695f571 8 days ago 133MBmysql 5.7 ae0658fdbad5 12 days ago 449MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB#进入自己[root@VM-0-15-centos docker-test-volume]# docker run -it 7735159af50c /bin/bash#可以看到挂载的两个volume卷[root@d6463f570ce4 /]# ls -ltotal 56lrwxrwxrwx 1 root root 7 May 11 2019 bin -&gt; usr/bindrwxr-xr-x 5 root root 360 Dec 3 15:21 devdrwxr-xr-x 1 root root 4096 Dec 3 15:21 etcdrwxr-xr-x 2 root root 4096 May 11 2019 homelrwxrwxrwx 1 root root 7 May 11 2019 lib -&gt; usr/liblrwxrwxrwx 1 root root 9 May 11 2019 lib64 -&gt; usr/lib64drwx------ 2 root root 4096 Aug 9 21:40 lost+founddrwxr-xr-x 2 root root 4096 May 11 2019 mediadrwxr-xr-x 2 root root 4096 May 11 2019 mntdrwxr-xr-x 2 root root 4096 May 11 2019 optdr-xr-xr-x 91 root root 0 Dec 3 15:21 procdr-xr-x--- 2 root root 4096 Aug 9 21:40 rootdrwxr-xr-x 11 root root 4096 Aug 9 21:40 runlrwxrwxrwx 1 root root 8 May 11 2019 sbin -&gt; usr/sbindrwxr-xr-x 2 root root 4096 May 11 2019 srvdr-xr-xr-x 13 root root 0 Dec 3 15:21 sysdrwxrwxrwt 7 root root 4096 Aug 9 21:40 tmpdrwxr-xr-x 12 root root 4096 Aug 9 21:40 usrdrwxr-xr-x 20 root root 4096 Aug 9 21:40 vardrwxr-xr-x 2 root root 4096 Dec 3 15:21 volume01drwxr-xr-x 2 root root 4096 Dec 3 15:21 volume02#这个卷Volume一定和外部有一个同步的目录[root@d6463f570ce4 volume01]# touch container.txt[root@d6463f570ce4 volume01]# lscontainer.txt[root@d6463f570ce4 volume01]# [root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd6463f570ce4 7735159af50c \"/bin/bash\" 7 minutes ago Up 7 minutes zen_faraday[root@VM-0-15-centos docker-test-volume]# docker inspect d6463f570ce4[ &#123; \"Id\": \"d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881\", \"Created\": \"2020-12-03T15:21:34.913373526Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 15638, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T15:21:35.202804066Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24d\", \"ResolvConfPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hostname\", \"HostsPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hosts\", \"LogPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881-json.log\", \"Name\": \"/zen_faraday\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"volume\", \"Name\": \"4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a\", \"Source\": \"/var/lib/docker/volumes/4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a/_data\", \"Destination\": \"volume01\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125;, &#123; \"Type\": \"volume\", \"Name\": \"0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1\", \"Source\": \"/var/lib/docker/volumes/0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1/_data\", \"Destination\": \"volume02\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125; ], \"Config\": &#123; \"Hostname\": \"d6463f570ce4\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"7735159af50c\", \"Volumes\": &#123; \"volume01\": &#123;&#125;, \"volume02\": &#123;&#125; &#125;, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"cf151107d8fa56fd220695eed265b431618350222be583761b13354867477e39\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/cf151107d8fa\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;] 数据卷容器不同容器之间同步数据 #docker01已经启动了[root@VM-0-15-centos _data]# docker run -it --name docker02 --volumes-from docker01 rxk/centos#--volumes-from类似于 docker02继承docker01#即使docker01被删了，docker02中卷的文件也是存在的，类似硬链接 DockerFiledockerfile是用来构建docker镜像的文件！命令参数脚本！ 构建步骤： 1.编写一个dockerfile文件 2.docker build构建成为一个镜像 3.docker run运行镜像 4.docker push发布镜像(DockerHub、阿里云镜像仓库！) 很多官方镜像都是基础包，很多功能没有，通常会自己搭建自己的镜像。 DockerFile的构建过程基础知识 1.每个保留关键字(指令)都必须是大写字母 2.执行从上到下顺序执行 3.# 表示注释 4.每一个指令都会创建提交一个新的镜像层 并提交 DockerFile:构建文件，定义了一切的步骤，源代码。 DockerImages:通过DockerFile构建生成的镜像，最终发布和运行的产品 Docker容器：容器就是镜像运行起来提供服务的 DockerFile的指令FROM # 基础镜像，一切从这里开始构建MAINTAINER #镜像是谁写的，姓名+邮箱RUN #镜像构建的时候需要运行的命令ADD #步骤：构建基于tomcat的镜像，进行添加！ADD为添加内容WORKDIR #镜像的工作目录VOLUME #挂载的容器卷EXPOSE #指定暴露端口CMD #指定这个容器启动时候要运行的命令，只有最后一个会生效，可被替代ENTRYPOINT #指定这个容器启动的时候要运行的命令，可以追加命令ONBUILD #当构建一个被继承DockerFile， 这个时候就会运行ONBUILD命令。触发指令COPY #类似ADD命令 ，将文件拷贝到镜像中ENV #构建时候设置环境变量 构建自己的centos 构建自己的centos #1.编写dockerfile的文件FROM centosMAINTAINER kuangshen&lt;179049243@qq.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo \"----end----\"CMD /bin/bash#2.通过文件构建镜像# docker build -f dockerfile文件路径 -t 镜像名:[tag].#docker build -f my-dockerfile -t mycentos:0.1 .[root@VM-0-15-centos dockerfile]# docker build -f my-dockerfile -t mycentos:0.1 .Sending build context to Docker daemon 2.048kBStep 1/10 : FROM centos ---&gt; 0d120b6ccaa8Step 2/10 : MAINTAINER kuangshen&lt;179049243@qq.com&gt; ---&gt; Running in 3dd9d64bfbffRemoving intermediate container 3dd9d64bfbff ---&gt; e56a7b5f722bStep 3/10 : ENV MYPATH /usr/local ---&gt; Running in 7707c6ee0441Removing intermediate container 7707c6ee0441 ---&gt; b67dab755631Step 4/10 : WORKDIR $MYPATH ---&gt; Running in 1797c746ab6bRemoving intermediate container 1797c746ab6b ---&gt; 3ed5689097b4Step 5/10 : RUN yum -y install vim ---&gt; Running in 5791c022d132CentOS-8 - AppStream 969 kB/s | 6.2 MB 00:06 CentOS-8 - Base 950 kB/s | 2.3 MB 00:02 CentOS-8 - Extras 11 kB/s | 8.1 kB 00:00 Dependencies resolved.================================================================================ Package Arch Version Repository Size================================================================================Installing: vim-enhanced x86_64 2:8.0.1763-15.el8 AppStream 1.4 MInstalling dependencies: gpm-libs x86_64 1.20.7-15.el8 AppStream 39 k vim-common x86_64 2:8.0.1763-15.el8 AppStream 6.3 M vim-filesystem noarch 2:8.0.1763-15.el8 AppStream 48 k which x86_64 2.21-12.el8 BaseOS 49 kTransaction Summary================================================================================Install 5 PackagesTotal download size: 7.8 MInstalled size: 30 MDownloading Packages:(1/5): gpm-libs-1.20.7-15.el8.x86_64.rpm 327 kB/s | 39 kB 00:00 (2/5): vim-filesystem-8.0.1763-15.el8.noarch.rp 827 kB/s | 48 kB 00:00 (3/5): which-2.21-12.el8.x86_64.rpm 316 kB/s | 49 kB 00:00 (4/5): vim-enhanced-8.0.1763-15.el8.x86_64.rpm 1.2 MB/s | 1.4 MB 00:01 (5/5): vim-common-8.0.1763-15.el8.x86_64.rpm 983 kB/s | 6.3 MB 00:06 --------------------------------------------------------------------------------Total 974 kB/s | 7.8 MB 00:08 warning: /var/cache/dnf/AppStream-02e86d1c976ab532/packages/gpm-libs-1.20.7-15.el8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 8483c65d: NOKEYCentOS-8 - AppStream 1.6 MB/s | 1.6 kB 00:00 Importing GPG key 0x8483C65D: Userid : \"CentOS (CentOS Official Signing Key) &lt;security@centos.org&gt;\" Fingerprint: 99DB 70FA E1D7 CE22 7FB6 4882 05B5 55B3 8483 C65D From : /etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialKey imported successfullyRunning transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : which-2.21-12.el8.x86_64 1/5 Installing : vim-filesystem-2:8.0.1763-15.el8.noarch 2/5 Installing : vim-common-2:8.0.1763-15.el8.x86_64 3/5 Installing : gpm-libs-1.20.7-15.el8.x86_64 4/5 Running scriptlet: gpm-libs-1.20.7-15.el8.x86_64 4/5 Installing : vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-common-2:8.0.1763-15.el8.x86_64 5/5 Verifying : gpm-libs-1.20.7-15.el8.x86_64 1/5 Verifying : vim-common-2:8.0.1763-15.el8.x86_64 2/5 Verifying : vim-enhanced-2:8.0.1763-15.el8.x86_64 3/5 Verifying : vim-filesystem-2:8.0.1763-15.el8.noarch 4/5 Verifying : which-2.21-12.el8.x86_64 5/5 Installed: gpm-libs-1.20.7-15.el8.x86_64 vim-common-2:8.0.1763-15.el8.x86_64 vim-enhanced-2:8.0.1763-15.el8.x86_64 vim-filesystem-2:8.0.1763-15.el8.noarch which-2.21-12.el8.x86_64 Complete!Removing intermediate container 5791c022d132 ---&gt; 05dc9bde286eStep 6/10 : RUN yum -y install net-tools ---&gt; Running in 900fc6c0b10eLast metadata expiration check: 0:00:15 ago on Thu Dec 10 15:07:33 2020.Dependencies resolved.================================================================================ Package Architecture Version Repository Size================================================================================Installing: net-tools x86_64 2.0-0.52.20160912git.el8 BaseOS 322 kTransaction Summary================================================================================Install 1 PackageTotal download size: 322 kInstalled size: 942 kDownloading Packages:net-tools-2.0-0.52.20160912git.el8.x86_64.rpm 969 kB/s | 322 kB 00:00 --------------------------------------------------------------------------------Total 181 kB/s | 322 kB 00:01 Running transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Running scriptlet: net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Verifying : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Installed: net-tools-2.0-0.52.20160912git.el8.x86_64 Complete!Removing intermediate container 900fc6c0b10e ---&gt; 3f8796ecbdeaStep 7/10 : EXPOSE 80 ---&gt; Running in 37e6f3792548Removing intermediate container 37e6f3792548 ---&gt; 2d37cf00b36bStep 8/10 : CMD echo $MYPATH ---&gt; Running in 7e4ff55e4b24Removing intermediate container 7e4ff55e4b24 ---&gt; d74bce644e74Step 9/10 : CMD echo \"----end----\" ---&gt; Running in fc3301142b73Removing intermediate container fc3301142b73 ---&gt; 2c7e307a481cStep 10/10 : CMD /bin/bash ---&gt; Running in 4e5c6edd147cRemoving intermediate container 4e5c6edd147c ---&gt; 071c7fb7d0f9Successfully built 071c7fb7d0f9Successfully tagged mycentos:0.1#3.测试镜像[root@VM-0-15-centos dockerfile]# docker run -it mycentos:0.1[root@5b68abc14a5a local]# pwd/usr/local[root@5b68abc14a5a local]# vimsh: wq: command not foundshell returned 127Press ENTER or type command to continuesh: wq: command not foundshell returned 127Press ENTER or type command to continue 镜像构建历史查看 [root@VM-0-15-centos dockerfile]# docker history 071c7fb7d0f9IMAGE CREATED CREATED BY SIZE COMMENT071c7fb7d0f9 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"/bin… 0B 2c7e307a481c 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B d74bce644e74 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B 2d37cf00b36b 6 minutes ago /bin/sh -c #(nop) EXPOSE 80 0B 3f8796ecbdea 6 minutes ago /bin/sh -c yum -y install net-tools 23.1MB 05dc9bde286e 6 minutes ago /bin/sh -c yum -y install vim 57.7MB 3ed5689097b4 7 minutes ago /bin/sh -c #(nop) WORKDIR /usr/local 0B b67dab755631 7 minutes ago /bin/sh -c #(nop) ENV MYPATH=/usr/local 0B e56a7b5f722b 7 minutes ago /bin/sh -c #(nop) MAINTAINER kuangshen&lt;1790… 0B 0d120b6ccaa8 4 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) ADD file:538afc0c5c964ce0d… 215MB 实战：Dockerfile制作tomcat镜像1、准备镜像文件tomcat压缩包，jdk的压缩包！2、编写dockerfile文件。 Docker0网络详解#删除所有镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: rxk/centos:latestDeleted: sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24dDeleted: sha256:d64085501c38257f9828c1dc9717f26e20df9bcb8d1b958b7f49ea5bb0425ae0Deleted: sha256:02a6fde9ba357c11d75c6d828c8fee80e1e5e77c4aedb3a83c7e302fc1745d40Untagged: tomcat02:1.0Deleted: sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74ebDeleted: sha256:20e9e09986f8d937efa6f2ad283aa9a1b837cc5cc5d4d76e6105f83e38e8f3deUntagged: nginx:latestUntagged: nginx@sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Deleted: sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221cUntagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Untagged: tomcat:9.0Untagged: tomcat:latestUntagged: tomcat@sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaDeleted: sha256:e0bd8b34b4ea904874e55eae50e8987815030d140f9773a4b61759f4f85bf38dUntagged: portainer/portainer:latestUntagged: portainer/portainer@sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Deleted: sha256:62771b0b9b0973a3e8e95595534a1240d8cfd968d30ec82dc0393ce0a256c5f3Untagged: elasticsearch:7.6.2Untagged: elasticsearch@sha256:1b09dbd93085a1e7bca34830e77d2981521a7210e11f11eda997add1c12711faDeleted: sha256:f29a1ee41030e3963026369105f3bee76d75fdecbeca07932ac054126be7bff9Error response from daemon: conflict: unable to delete 071c7fb7d0f9 (cannot be forced) - image is being used by running container 5b68abc14a5aError response from daemon: conflict: unable to delete 3f8796ecbdea (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete d74bce644e74 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2d37cf00b36b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2c7e307a481c (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 05dc9bde286e (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 3ed5689097b4 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete e56a7b5f722b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete b67dab755631 (cannot be forced) - image has dependent child imagesError: No such image: 02a6fde9ba35Error: No such image: d64085501c38Error: No such image: e0bd8b34b4eaError response from daemon: conflict: unable to delete 0d120b6ccaa8 (cannot be forced) - image has dependent child images 三个网络 #启动一个tomcat[root@VM-0-15-centos ~]# docker run -d -P --name tomcat02 tomcat942539fdda0c018ffa3750661373e624c4f2226f5d9532323d5c42c1daea98db[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES942539fdda0c tomcat \"catalina.sh run\" 3 seconds ago Up 3 seconds 0.0.0.0:32768-&gt;8080/tcp tomcat02#查看容器的内部网络地址[root@VM-0-15-centos ~]# docker exec -it tomcat02 ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever112: eth0@if113: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever linux可以ping通docker容器内部 原理 1.我们每安装一个docker容器，docker就会给docker容器分配一个ip，我们只要安装了docker，就会有一个网卡，docker0，使用桥接模式，使用技术是veth-pair技术！ 再次测试ip addr: 2.再启动一个容器测试，又会多一对网卡，这样一对对的网卡，其实是veth-pair技术，就是一对的虚拟设备，他们都是成对出现，一段连着协议，一段彼此相连，veth-pair充当一个桥梁，链接各种虚拟网络设备 3.测试下tomcat02和tomcat03能否ping通，确实可以ping通 容器和容器之间是可以互相ping通的 [root@VM-0-15-centos ~]# docker exec -it tomcat03 ping 172.18.0.2PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.076 ms64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.054 ms64 bytes from 172.18.0.2: icmp_seq=3 ttl=64 time=0.053 ms^C--- 172.18.0.2 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.053/0.061/0.076/0.010 ms 结论：tomcat02和tomcat03共用一个路由器，docker0 所有容器不指定网络的情况下，都是由docker0路由的，docker会给我们容器分配一个默认的可用IP 当把docker容器删除之后，ip addr之后 容器ip就没了 [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbe137857cf4e tomcat \"catalina.sh run\" 29 minutes ago Up 29 minutes 0.0.0.0:32769-&gt;8080/tcp tomcat03942539fdda0c tomcat \"catalina.sh run\" 37 minutes ago Up 37 minutes 0.0.0.0:32768-&gt;8080/tcp tomcat02[root@VM-0-15-centos ~]# docker rm -f be137857cf4ebe137857cf4e[root@VM-0-15-centos ~]# docker rm -f 942539fdda0c942539fdda0c[root@VM-0-15-centos ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:83:af:57 brd ff:ff:ff:ff:ff:ff inet 172.17.0.15/20 brd 172.17.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe83:af57/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:18:37:f9:06 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:18ff:fe37:f906/64 scope link valid_lft forever preferred_lft forever Docker –link 容器互联使用服务名去ping通tomcat docker run -d -P tomcat03 --link tomcat02 tomcatdocker exec -it tomcat03 ping tomcat02#此时3能ping 通2，但是2 ping不通3 Docker自定义网络查看docker所有网络 [root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host local00b73cedd526 none null local 网络模式 bridge:桥接 bridge(默认) none:不配置网络 host:和宿主机共享网络 container:容器内网络连通(使用较少) 创建自己的自定义网络 –driver bridge 默认桥接 –subnet 192.168.0.0/16 子网地址 –gateway 192.168.0.1 网关地址 [root@VM-0-15-centos ~]# docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet[root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host localf5100a69756f mynet bridge local00b73cedd526 none null local[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123;&#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#启动两个容器[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-01 --network mynet tomcat8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-02 --network mynet tomcat01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a\": &#123; \"Name\": \"tomcat-net-02\", \"EndpointID\": \"25be3c90c1e1a4b74f5fc9e10a59c50029ca7ff3bf692d7de636738edb3457b3\", \"MacAddress\": \"02:42:c0:a8:00:03\", \"IPv4Address\": \"192.168.0.3/16\", \"IPv6Address\": \"\" &#125;, \"8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33\": &#123; \"Name\": \"tomcat-net-01\", \"EndpointID\": \"a83c09df1a4595735453dafdde352123dd024707400716fbab256c7e3c02ea4a\", \"MacAddress\": \"02:42:c0:a8:00:02\", \"IPv4Address\": \"192.168.0.2/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#再次测试，不再使用--link 也是可以连接通的，不管使用ip还是服务名[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping 192.168.0.3PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.085 ms64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.055 ms64 bytes from 192.168.0.3: icmp_seq=3 ttl=64 time=0.053 ms^C--- 192.168.0.3 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.053/0.064/0.085/0.016 ms[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping tomcat-net-02PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.050 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.052 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.053 ms^C--- tomcat-net-02 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 0.050/0.051/0.053/0.008 ms 我们自定义的网络docker 都已经帮我们维护好了对应的关系，推荐这样使用网络！ 好处：不同的集群使用不同的网络，保证集群是安全和健康 网络连通把一个容器连接到网络上 #将tomcat01连接到mynet网络下，一个容器两个ip docker network connect mynet tomcat01 Redis集群实战通过脚本创建六个redis配置 for port in $(seq 1 6); \\do \\mkdir -p /mydata/redis/node-$&#123;port&#125;/conftouch /mydata/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;/mydata/redis/node-$&#123;port&#125;/conf/redis.confport 6379bind 0.0.0.0cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.38.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379daemonize noappendonly yesEOFdone 启动redis docker run -p 6371:6379 -p 16371:16379 --name redis-1 -v /mydata/redis/node-1/data:/data -v /mydata/redis/node-1/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.11 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6372:6379 -p 16372:16379 --name redis-2 -v /mydata/redis/node-2/data:/data -v /mydata/redis/node-2/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.12 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6373:6379 -p 16373:16379 --name redis-3 -v /mydata/redis/node-3/data:/data -v /mydata/redis/node-3/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.13 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6374:6379 -p 16374:16379 --name redis-4 -v /mydata/redis/node-4/data:/data -v /mydata/redis/node-4/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.14 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6375:6379 -p 16375:16379 --name redis-5 -v /mydata/redis/node-5/data:/data -v /mydata/redis/node-5/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.15 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6376:6379 -p 16376:16379 --name redis-6 -v /mydata/redis/node-6/data:/data -v /mydata/redis/node-6/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.16 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.conf#正常启动之后[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe1535aac14ce redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 22 seconds ago Up 22 seconds 0.0.0.0:6376-&gt;6379/tcp, 0.0.0.0:16376-&gt;16379/tcp redis-6fcf18ac7033d redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 31 seconds ago Up 30 seconds 0.0.0.0:6375-&gt;6379/tcp, 0.0.0.0:16375-&gt;16379/tcp redis-5b17f66d41588 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 37 seconds ago Up 37 seconds 0.0.0.0:6374-&gt;6379/tcp, 0.0.0.0:16374-&gt;16379/tcp redis-4afb2f4e6484e redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 44 seconds ago Up 44 seconds 0.0.0.0:6373-&gt;6379/tcp, 0.0.0.0:16373-&gt;16379/tcp redis-3ef59db5b0cc0 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 54 seconds ago Up 53 seconds 0.0.0.0:6372-&gt;6379/tcp, 0.0.0.0:16372-&gt;16379/tcp redis-20eca360704d7 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" About a minute ago Up About a minute 0.0.0.0:6371-&gt;6379/tcp, 0.0.0.0:16371-&gt;16379/tcp redis-1#进入redis-1[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # #创建redis集群[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.38.0.15:6379 to 172.38.0.11:6379Adding replica 172.38.0.16:6379 to 172.38.0.12:6379Adding replica 172.38.0.14:6379 to 172.38.0.13:6379M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) masterM: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) masterM: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) masterS: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 replicates f70e64b741bad779e2b545b1429b47b2732a9a13S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84dCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 172.38.0.11:6379)M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 slots: (0 slots) slave replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 slots: (0 slots) slave replicates f70e64b741bad779e2b545b1429b47b2732a9a13M: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 slots: (0 slots) slave replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered./data # redis-cli -c127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:149cluster_stats_messages_pong_sent:140cluster_stats_messages_sent:289cluster_stats_messages_ping_received:135cluster_stats_messages_pong_received:149cluster_stats_messages_meet_received:5cluster_stats_messages_received:289127.0.0.1:6379&gt; cluster nodesde19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379@16379 master - 0 1608624786213 2 conn61-1092213b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379@16379 slave 43b45750adfaea90314c724ddf6b40bf 0 1608624785000 4 connected03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379@16379 slave f70e64b741bad779e2b545b1432a9a13 0 1608624785000 5 connected43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379@16379 master - 0 1608624785210 3 conn923-16383b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379@16379 slave de19aadcb9d25a77ca0e5a23f6ece84d 0 1608624785712 6 connectedf70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379@16379 myself,master - 0 1608624785000cted 0-5460127.0.0.1:6379&gt; set a b-&gt; Redirected to slot [15495] located at 172.38.0.13:6379OK172.38.0.13:6379&gt; get a^C#即使把redis-3 stop了，仍然可以通过其他slaver找到key a所对应的值b，主从配置成功/data # redis-cli -c127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.38.0.14:6379\"b\"172.38.0.14:6379&gt; 搭建redis集群完成","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"}],"author":"CinKate"},{"title":"《A Comparative Study of Word Embeddings for Reading Comprehension》论文阅读","slug":"WordEmbeddingsforReadingComprehension","date":"2019-05-11T19:43:34.000Z","updated":"2020-05-17T16:12:00.854Z","comments":true,"path":"2019/05/12/WordEmbeddingsforReadingComprehension/","link":"","permalink":"http://renxingkai.github.io/2019/05/12/WordEmbeddingsforReadingComprehension/","excerpt":"","text":"众所周知，预训练好的词向量有不同的维度，比如预训练好的GloVe词向量有从50-300维等的词向量表示，但这些不同维度的表示有什么区别，以及在什么时候该用什么维度的词向量（虽然各论文中大家大多用了300维的词向量），这些问题我也确实不太清除。这篇论文解答了我的这些困惑，写的还是很精彩的。 论文原链接 Let’s have a look: 这篇论文主要解决了两个问题点： - 在阅读理解任务中应该使用什么样的预训练词向量 - 测试阶段对于OOV词应怎样处理 所用的数据集和模型数据集： Who-Did-What(WDW) 完型填空类的数据集，从新闻故事中构建 Children’s Book Test(CBT) 从children’s book构建，此论文中仅用了CBT-NE，即数据集中答案为命名实体的数据 模型：Stanford AR、GA Reader 词向量对比： GloVe (50-300) word2vec (300) 实验和结果 词向量的对比 第一个结果：使用在合适的语料库上训练的词向量可以比随机初始化提高3-6％。然而，用于预训练的语料库和方法是重要的选择：例如，在CBT上训练的word2vec词向量比随机词向量执行效果更差。 另请注意，在每种情况下，GloVe词向量都优于在同一语料库中训练的word2vec嵌入。 但是由于词向量对训练参数很敏感，并不能说GloVe一定比word2vec好，但确实从各个论文中也可以看出，一般会优先选择GloVe。 第二个结果：对比GloVe不同维度对实验结果的影响(50-300)：随着词向量维度的增加，实验结果性能是下降的。但是即使使用了300维的GloVe词向量，实验结果仍然比word2vec词向量效果好。 第三个结果：在使用原始语料进行训练时，要先将停用词去掉，与停用词的共现提供关于特定单词的语义的很少有意义的信息，因此具有高百分比的语料库可能不会产生高质量的向量。训练词向量时，超参数调节很重要。 处理OOV词 第一个结果：一般对于OOV词的处理都是赋予一个固定大小不变的词向量（UNK）。这种方法忽略了这样一个事实，即分配为UNK的许多单词可能已经训练过VG中可用的词向量。实验中在测试时，任何新token将被分配其GloVe向量（如果存在）或UNK的向量。 第二个结果，不是为所有OOV词分配一个共同的UNK向量，而是为它们分配未经训练但唯一的随机向量可能更好。此方法在训练时访问测试集词表是没必要的。 总结作者已经证明，用于初始化单词向量的预训练词向量的选择对于阅读理解的神经网络模型的性能具有显著影响。在测试时处理OOV词的方法也是如此。 根据作者的实验，我们建议使用现成的GloVe词向量，并在测试时将预先训练的GloVe向量（如果可用）或随机但唯一的向量分配给OOV词。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"FastQA学习","slug":"fastqa","date":"2019-04-21T10:52:36.000Z","updated":"2020-05-17T16:11:59.241Z","comments":true,"path":"2019/04/21/fastqa/","link":"","permalink":"http://renxingkai.github.io/2019/04/21/fastqa/","excerpt":"","text":"现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 emedding-, encoding-, interaction-, answer-layer ），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层，比如BIDAF、R-Net等等，大量的工作都在问题和文章的交互query-aware表示上创新，类似人类做阅读理解问题的思路“重复多读文章”，“带着问题读文章”等等，普通的“阅读理解思路”也都被实现了，这篇论文作者发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans: 与 question 对应的 answer type 匹配，比如说问 when 就回答 time； 与重要的 question words 位置上临近； 添加问题单词是否出现在文章中这一“重要”特征；并没有使用复杂的question与context的交互，就取得了在SQuAD榜上与SOTA接近的结果，这篇论文之后，后来的研究者们在做MRC时也会将基础特征加入到embedding中进行共同训练，开源链接。 以下是阅读源码的一些总结： 1.Highway Network的使用Highway Network主要解决的问题是，网络深度加深，梯度信息回流受阻造成网络训练困难的问题。 当网络加深，训练的误差反而上升了，而加入了Highway Network之后，这个问题得到了缓解。一般来说，深度网络训练困难是由于梯度回流受阻的问题，可能浅层网络没有办法得到调整。Highway Network 受LSTM启发，增加了一个门函数，让网络的输出由两部分组成，分别是网络的直接输入以及输入变形后的部分。 网络中把此层放在embedding层后面 import tensorflow as tffrom keras import backend as Kfrom keras.engine.topology import Layerfrom keras.layers import Lambda, Wrapperclass Highway(Layer): def __init__(self, hidden_size, **kwargs): self.hidden_size = hidden_size super().__init__(**kwargs) def build(self, input_shape): self.projection = self.add_weight(name=&apos;projection&apos;, shape=(1, input_shape[-1], self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.W_h = self.add_weight(name=&apos;W_h&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_h = self.add_weight(name=&apos;b_h&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) self.W_t = self.add_weight(name=&apos;W_t&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_t = self.add_weight(name=&apos;b_t&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) def call(self, x): x = K.conv1d(x, self.projection) H = tf.nn.tanh(K.bias_add(K.conv1d(x, self.W_h), self.b_h)) T = tf.nn.sigmoid(K.bias_add(K.conv1d(x, self.W_t), self.b_t)) return T * x + (1 - T) * H def compute_output_shape(self, input_shape): batch, seq_len, d = input_shape return (batch, seq_len, self.hidden_size) 2.tf.sequence_mask的学习这个操作和one hot也很像，但是指定的不是index而是从前到后有多少个True，返回的是True和False。 sq_mask = tf.sequence_mask([1, 3, 2], 5)print(sess.run(sq_mask)) 输出： [[True, False, False, False, False],[True, True, True, False, False],[True, True, False, False, False]] 3.tf.expand_dims()学习TensorFlow中，想要维度增加一维，可以使用 tf.expand_dims(input, dim, name=None) 函数。当然，我们常用tf.reshape(input,shape=[])也可以达到相同效果，但是有些时候在构建图的过程中，placeholder没有被feed具体的值，这时就会包下面的错误：TypeError: Expected binary or unicode string, got 1 在这种情况下，我们就可以考虑使用expand_dims来将维度加1。比如我自己代码中遇到的情况，在对图像维度降到二维做特定操作后，要还原成四维[batch, height, width, channels]，前后各增加一维。如果用reshape，则因为上述原因报错 给出官方的例子： # &apos;t&apos; is a tensor of shape [2]shape(expand_dims(t, 0)) ==&gt; [1, 2]shape(expand_dims(t, 1)) ==&gt; [2, 1]shape(expand_dims(t, -1)) ==&gt; [2, 1]# &apos;t2&apos; is a tensor of shape [2, 3, 5]shape(expand_dims(t2, 0)) ==&gt; [1, 2, 3, 5]shape(expand_dims(t2, 2)) ==&gt; [2, 3, 1, 5]shape(expand_dims(t2, 3)) ==&gt; [2, 3, 5, 1] Args: input: A Tensor. dim: A Tensor. Must be one of the following types: int32, int64. 0-D (scalar). Specifies the dimension index at which to expand the shape of input. name: A name for the operation (optional).Returns: A Tensor. Has the same type as input. Contains the same data as input, but its shape has an additional dimension of size 1 added. 4.tf.tile()学习推荐博客tf.tile( input, #输入 multiples, #某一维度上复制的次数 name=None ) import tensorflow as tfa = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)a1 = tf.tile(a, [2, 3])a2 = tf.tile(a, [1, 2])with tf.Session() as sess: print(sess.run(a)) print(sess.run(a1)) print(sess.run(a2)) 输出： [[1. 2.] [3. 4.] [5. 6.]] [[1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.] [1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.]][[1. 2. 1. 2.] [3. 4. 3. 4.] [5. 6. 5. 6.]] 5.tf.equal()学习 equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，而是逐个元素进行判断，如果相等就是 True，不相等，就是 False。 由于是逐个元素判断，所以 x，y 的维度要一致。 例子： import tensorflow as tfa = [[1,2,3],[4,5,6]]b = [[1,0,3],[1,5,1]]with tf.Session() as sess: print(sess.run(tf.equal(a,b))) 输出： [[ True False True] [False True False]] 6.tf.reduce_any()学习在boolean张量的维度上计算元素的 “逻辑或” x = tf.constant([[True, True], [False, False]])with tf.Session() as sess: print(tf.reduce_any(x)) # True print(tf.reduce_any(x, 0)) # [True, True] print(tf.reduce_any(x, 1)) # [True, False] 7.tf.squeeze()学习该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果axis可以用来指定要删掉的为1的维度，此处要注意指定的维度必须确保其是1，否则会报错squeeze( input, axis=None, name=None, squeeze_dims=None) 例子：# &apos;t&apos; 是一个维度是[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t)) # [2, 3]， 默认删除所有为1的维度# &apos;t&apos; 是一个维度[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t, [2, 4])) # [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1 8.RepeatVector层RepeatVector层将输入重复n次keras.layers.core.RepeatVector(n) 参数 n：整数，重复的次数 输入shape形如（nb_samples, features）的2D张量 输出shape形如（nb_samples, n, features）的3D张量 例子 model = Sequential()model.add(Dense(32, input_dim=32))# now: model.output_shape == (None, 32)# note: `None` is the batch dimensionmodel.add(RepeatVector(3))# now: model.output_shape == (None, 3, 32) 9.tf.gather()学习类似于数组的索引，可以把向量中某些索引值提取出来，得到新的向量，适用于要提取的索引为不连续的情况。这个函数似乎只适合在一维的情况下使用。 import tensorflow as tf a = tf.Variable([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])index_a = tf.Variable([0,2]) b = tf.Variable([1,2,3,4,5,6,7,8,9,10])index_b = tf.Variable([2,4,6,8]) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather(a, index_a))) print(sess.run(tf.gather(b, index_b))) # [[ 1 2 3 4 5]# [11 12 13 14 15]] # [3 5 7 9] tf.gather_nd同上，但允许在多维上进行索引，例子只展示了一种很简单的用法，","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"}],"author":"CinKate"},{"title":"基于NLTK的TF-IDF关键词抽取","slug":"tfidfkeyextraction","date":"2019-04-10T14:46:21.000Z","updated":"2020-05-17T16:12:00.477Z","comments":true,"path":"2019/04/10/tfidfkeyextraction/","link":"","permalink":"http://renxingkai.github.io/2019/04/10/tfidfkeyextraction/","excerpt":"","text":"基于nltk总结了用TF-IDF提取关键词的方法，同时总结了文本标准化（预处理），SVD分解、基于TF-IDF、词频等的关键词抽取 SVD奇异值分解from scipy.sparse.linalg import svdsimport reimport nltkimport unicodedatadef low_rank_svd(matrix,singular_count=2): u,s,vt=svds(matrix,k=singular_count) return u,s,vt 删除换行,进行分句def parse_document(document): document=re.sub(&apos;\\n&apos;,&apos; &apos;,document) if isinstance(document,str): document=document elif isinstance(document,unicode): return unicodedata.normalize(&apos;NFKD&apos;,document).encode(&apos;ascii&apos;,&apos;ignore&apos;) else: raise ValueError(&apos;Document is not string or unicode!&apos;) document=document.strip() sentences=nltk.sent_tokenize(document) sentences=[sentence.strip() for sentence in sentences] return sentences 转移HTML标签from html.parser import HTMLParser html_parser=HTMLParser()def unescape_html(parser,text): return parser.unescape_html(text) 缩写词表CONTRACTION_MAP = &#123;&quot;ain&apos;t&quot;: &quot;is not&quot;,&quot;aren&apos;t&quot;: &quot;are not&quot;,&quot;can&apos;t&quot;: &quot;cannot&quot;,&quot;can&apos;t&apos;ve&quot;: &quot;cannot have&quot;,&quot;&apos;cause&quot;: &quot;because&quot;,&quot;could&apos;ve&quot;: &quot;could have&quot;,&quot;couldn&apos;t&quot;: &quot;could not&quot;,&quot;couldn&apos;t&apos;ve&quot;: &quot;could not have&quot;,&quot;didn&apos;t&quot;: &quot;did not&quot;,&quot;doesn&apos;t&quot;: &quot;does not&quot;,&quot;don&apos;t&quot;: &quot;do not&quot;,&quot;hadn&apos;t&quot;: &quot;had not&quot;,&quot;hadn&apos;t&apos;ve&quot;: &quot;had not have&quot;,&quot;hasn&apos;t&quot;: &quot;has not&quot;,&quot;haven&apos;t&quot;: &quot;have not&quot;,&quot;he&apos;d&quot;: &quot;he would&quot;,&quot;he&apos;d&apos;ve&quot;: &quot;he would have&quot;,&quot;he&apos;ll&quot;: &quot;he will&quot;,&quot;he&apos;ll&apos;ve&quot;: &quot;he he will have&quot;,&quot;he&apos;s&quot;: &quot;he is&quot;,&quot;how&apos;d&quot;: &quot;how did&quot;,&quot;how&apos;d&apos;y&quot;: &quot;how do you&quot;,&quot;how&apos;ll&quot;: &quot;how will&quot;,&quot;how&apos;s&quot;: &quot;how is&quot;,&quot;I&apos;d&quot;: &quot;I would&quot;,&quot;I&apos;d&apos;ve&quot;: &quot;I would have&quot;,&quot;I&apos;ll&quot;: &quot;I will&quot;,&quot;I&apos;ll&apos;ve&quot;: &quot;I will have&quot;,&quot;I&apos;m&quot;: &quot;I am&quot;,&quot;I&apos;ve&quot;: &quot;I have&quot;,&quot;i&apos;d&quot;: &quot;i would&quot;,&quot;i&apos;d&apos;ve&quot;: &quot;i would have&quot;,&quot;i&apos;ll&quot;: &quot;i will&quot;,&quot;i&apos;ll&apos;ve&quot;: &quot;i will have&quot;,&quot;i&apos;m&quot;: &quot;i am&quot;,&quot;i&apos;ve&quot;: &quot;i have&quot;,&quot;isn&apos;t&quot;: &quot;is not&quot;,&quot;it&apos;d&quot;: &quot;it would&quot;,&quot;it&apos;d&apos;ve&quot;: &quot;it would have&quot;,&quot;it&apos;ll&quot;: &quot;it will&quot;,&quot;it&apos;ll&apos;ve&quot;: &quot;it will have&quot;,&quot;it&apos;s&quot;: &quot;it is&quot;,&quot;let&apos;s&quot;: &quot;let us&quot;,&quot;ma&apos;am&quot;: &quot;madam&quot;,&quot;mayn&apos;t&quot;: &quot;may not&quot;,&quot;might&apos;ve&quot;: &quot;might have&quot;,&quot;mightn&apos;t&quot;: &quot;might not&quot;,&quot;mightn&apos;t&apos;ve&quot;: &quot;might not have&quot;,&quot;must&apos;ve&quot;: &quot;must have&quot;,&quot;mustn&apos;t&quot;: &quot;must not&quot;,&quot;mustn&apos;t&apos;ve&quot;: &quot;must not have&quot;,&quot;needn&apos;t&quot;: &quot;need not&quot;,&quot;needn&apos;t&apos;ve&quot;: &quot;need not have&quot;,&quot;o&apos;clock&quot;: &quot;of the clock&quot;,&quot;oughtn&apos;t&quot;: &quot;ought not&quot;,&quot;oughtn&apos;t&apos;ve&quot;: &quot;ought not have&quot;,&quot;shan&apos;t&quot;: &quot;shall not&quot;,&quot;sha&apos;n&apos;t&quot;: &quot;shall not&quot;,&quot;shan&apos;t&apos;ve&quot;: &quot;shall not have&quot;,&quot;she&apos;d&quot;: &quot;she would&quot;,&quot;she&apos;d&apos;ve&quot;: &quot;she would have&quot;,&quot;she&apos;ll&quot;: &quot;she will&quot;,&quot;she&apos;ll&apos;ve&quot;: &quot;she will have&quot;,&quot;she&apos;s&quot;: &quot;she is&quot;,&quot;should&apos;ve&quot;: &quot;should have&quot;,&quot;shouldn&apos;t&quot;: &quot;should not&quot;,&quot;shouldn&apos;t&apos;ve&quot;: &quot;should not have&quot;,&quot;so&apos;ve&quot;: &quot;so have&quot;,&quot;so&apos;s&quot;: &quot;so as&quot;,&quot;that&apos;d&quot;: &quot;that would&quot;,&quot;that&apos;d&apos;ve&quot;: &quot;that would have&quot;,&quot;that&apos;s&quot;: &quot;that is&quot;,&quot;there&apos;d&quot;: &quot;there would&quot;,&quot;there&apos;d&apos;ve&quot;: &quot;there would have&quot;,&quot;there&apos;s&quot;: &quot;there is&quot;,&quot;they&apos;d&quot;: &quot;they would&quot;,&quot;they&apos;d&apos;ve&quot;: &quot;they would have&quot;,&quot;they&apos;ll&quot;: &quot;they will&quot;,&quot;they&apos;ll&apos;ve&quot;: &quot;they will have&quot;,&quot;they&apos;re&quot;: &quot;they are&quot;,&quot;they&apos;ve&quot;: &quot;they have&quot;,&quot;to&apos;ve&quot;: &quot;to have&quot;,&quot;wasn&apos;t&quot;: &quot;was not&quot;,&quot;we&apos;d&quot;: &quot;we would&quot;,&quot;we&apos;d&apos;ve&quot;: &quot;we would have&quot;,&quot;we&apos;ll&quot;: &quot;we will&quot;,&quot;we&apos;ll&apos;ve&quot;: &quot;we will have&quot;,&quot;we&apos;re&quot;: &quot;we are&quot;,&quot;we&apos;ve&quot;: &quot;we have&quot;,&quot;weren&apos;t&quot;: &quot;were not&quot;,&quot;what&apos;ll&quot;: &quot;what will&quot;,&quot;what&apos;ll&apos;ve&quot;: &quot;what will have&quot;,&quot;what&apos;re&quot;: &quot;what are&quot;,&quot;what&apos;s&quot;: &quot;what is&quot;,&quot;what&apos;ve&quot;: &quot;what have&quot;,&quot;when&apos;s&quot;: &quot;when is&quot;,&quot;when&apos;ve&quot;: &quot;when have&quot;,&quot;where&apos;d&quot;: &quot;where did&quot;,&quot;where&apos;s&quot;: &quot;where is&quot;,&quot;where&apos;ve&quot;: &quot;where have&quot;,&quot;who&apos;ll&quot;: &quot;who will&quot;,&quot;who&apos;ll&apos;ve&quot;: &quot;who will have&quot;,&quot;who&apos;s&quot;: &quot;who is&quot;,&quot;who&apos;ve&quot;: &quot;who have&quot;,&quot;why&apos;s&quot;: &quot;why is&quot;,&quot;why&apos;ve&quot;: &quot;why have&quot;,&quot;will&apos;ve&quot;: &quot;will have&quot;,&quot;won&apos;t&quot;: &quot;will not&quot;,&quot;won&apos;t&apos;ve&quot;: &quot;will not have&quot;,&quot;would&apos;ve&quot;: &quot;would have&quot;,&quot;wouldn&apos;t&quot;: &quot;would not&quot;,&quot;wouldn&apos;t&apos;ve&quot;: &quot;would not have&quot;,&quot;y&apos;all&quot;: &quot;you all&quot;,&quot;y&apos;all&apos;d&quot;: &quot;you all would&quot;,&quot;y&apos;all&apos;d&apos;ve&quot;: &quot;you all would have&quot;,&quot;y&apos;all&apos;re&quot;: &quot;you all are&quot;,&quot;y&apos;all&apos;ve&quot;: &quot;you all have&quot;,&quot;you&apos;d&quot;: &quot;you would&quot;,&quot;you&apos;d&apos;ve&quot;: &quot;you would have&quot;,&quot;you&apos;ll&quot;: &quot;you will&quot;,&quot;you&apos;ll&apos;ve&quot;: &quot;you will have&quot;,&quot;you&apos;re&quot;: &quot;you are&quot;,&quot;you&apos;ve&quot;: &quot;you have&quot;&#125; 文本标准化import stringfrom nltk.stem import WordNetLemmatizerstopword_list = nltk.corpus.stopwords.words(&apos;english&apos;)wnl = WordNetLemmatizer()html_parser = HTMLParser() 文本分词def tokenize_text(text): tokens = nltk.word_tokenize(text) tokens = [token.strip() for token in tokens] return tokens 扩展缩写def expand_contractions(text, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match if contraction_mapping.get(match) else contraction_mapping.get(match.lower())) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_text = contractions_pattern.sub(expand_match, text) expanded_text = re.sub(&quot;&apos;&quot;, &quot;&quot;, expanded_text) return expanded_text 标记文本词性from pattern.en import tagfrom nltk.corpus import wordnet as wn# 标记文本词性def pos_tag_text(text): def penn_to_wn_tags(pos_tag): if pos_tag.startswith(&apos;J&apos;): return wn.ADJ elif pos_tag.startswith(&apos;V&apos;): return wn.VERB elif pos_tag.startswith(&apos;N&apos;): return wn.NOUN elif pos_tag.startswith(&apos;R&apos;): return wn.ADV else: return None tagged_text = tag(text) tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text] return tagged_lower_text 基于词性标签提取主干词def lemmatize_text(text): pos_tagged_text = pos_tag_text(text) lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text] lemmatized_text = &apos; &apos;.join(lemmatized_tokens) return lemmatized_text 删除特殊字符def remove_special_characters(text): tokens = tokenize_text(text) pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = filter(None, [pattern.sub(&apos; &apos;, token) for token in tokens]) filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 删除停用词def remove_stopwords(text): tokens = tokenize_text(text) filtered_tokens = [token for token in tokens if token not in stopword_list] filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 转移HTML标签def unescape_html(parser, text): return parser.unescape(text) 标准化文本(合并执行上面流程)def normalize_corpus(corpus, lemmatize=True, tokenize=False): normalized_corpus = [] for text in corpus: text = html_parser.unescape(text) text = expand_contractions(text, CONTRACTION_MAP) if lemmatize: text = lemmatize_text(text) else: text = text.lower() text = remove_special_characters(text) text = remove_stopwords(text) if tokenize: text = tokenize_text(text) normalized_corpus.append(text) else: normalized_corpus.append(text) return normalized_corpus 文本特征提取 基于词项次数的二值特征 基于词袋模型的频率特征 TF-IDF权重特征 构建特征矩阵binary、frequency、tfidffrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizerdef build_feature_matrix(documents,feature_type=&apos;frequency&apos;): feature_type=feature_type.lower().strip() if feature_type==&apos;binary&apos;: vectorizer=CountVectorizer(binary=True,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;frequency&apos;: vectorizer=CountVectorizer(binary=False,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;tfidf&apos;: vectorizer=TfidfVectorizer(min_df=1,ngram_range=(1,1)) else: raise Exception(&quot;Wrong feature type entered. Possible values: &apos;binary&apos;, &apos;frequency&apos;, &apos;tfidf&apos;&quot;) feature_matrix=vectorizer.fit_transform(documents).astype(float) return vectorizer,feature_matrix 关键短语提取词项搭配from nltk.corpus import gutenbergimport nltkfrom operator import itemgetteralice = gutenberg.sents(fileids=&apos;carroll-alice.txt&apos;)alice = [&apos; &apos;.join(ts) for ts in alice]norm_alice = normalize_corpus(alice, lemmatize=False) 将语料压缩成1个大的文本串def flatten_corpus(corpus): return &apos; &apos;.join([document.strip() for document in corpus]) 计算n元分词（比较巧妙）def compute_ngrams(sequence,n):# print([sequence[index:] for index in range(n)])# print(list(zip(*[sequence[index:] for index in range(n)]))) #解压时仅按最小元素数组数量进行解压 return zip(*[sequence[index:] for index in range(n)]) 获取n元分词def get_top_ngram(corpus,ngram_val=1,limit=5): corpus=flatten_corpus(corpus) tokens=nltk.word_tokenize(corpus) ngrams=compute_ngrams(tokens,ngram_val) #获取单词频率 ngrams_freq_dist=nltk.FreqDist(ngrams) #排序频率 sorted_ngrams_fd=sorted(ngrams_freq_dist.items(),key=itemgetter(1),reverse=True) sorted_ngrams=sorted_ngrams_fd[0:limit] sorted_ngrams=[(&apos; &apos;.join(text),freq) for text,freq in sorted_ngrams] return sorted_ngrams 输出频率前10的二元分词get_top_ngram(corpus=norm_alice,ngram_val=2,limit=10) 输出结果[(&apos;said alice&apos;, 123), (&apos;mock turtle&apos;, 56), (&apos;march hare&apos;, 31), (&apos;said king&apos;, 29), (&apos;thought alice&apos;, 26), (&apos;white rabbit&apos;, 22), (&apos;said hatter&apos;, 22), (&apos;said mock&apos;, 20), (&apos;said caterpillar&apos;, 18), (&apos;said gryphon&apos;, 18)] 频率前10的三元分词get_top_ngram(corpus=norm_alice,ngram_val=3,limit=10) 输出结果 [(&apos;said mock turtle&apos;, 20), (&apos;said march hare&apos;, 9), (&apos;poor little thing&apos;, 6), (&apos;little golden key&apos;, 5), (&apos;certainly said alice&apos;, 5), (&apos;white kid gloves&apos;, 5), (&apos;march hare said&apos;, 5), (&apos;mock turtle said&apos;, 5), (&apos;know said alice&apos;, 4), (&apos;might well say&apos;, 4)] 频率前10的一元分词get_top_ngram(corpus=norm_alice,ngram_val=1,limit=10) 输出结果[(&apos;said&apos;, 462), (&apos;alice&apos;, 398), (&apos;little&apos;, 128), (&apos;one&apos;, 104), (&apos;know&apos;, 88), (&apos;like&apos;, 85), (&apos;would&apos;, 83), (&apos;went&apos;, 83), (&apos;could&apos;, 77), (&apos;queen&apos;, 75)] 使用nltk的搭配查找器二元词项from nltk.collocations import BigramCollocationFinderfrom nltk.collocations import BigramAssocMeasuresfinder=BigramCollocationFinder.from_documents([item.split() for item in norm_alice])bigram_measures=BigramAssocMeasures()#使用原始频率进行查找finder.nbest(bigram_measures.raw_freq,10) 输出结果 [(&apos;said&apos;, &apos;alice&apos;), (&apos;mock&apos;, &apos;turtle&apos;), (&apos;march&apos;, &apos;hare&apos;), (&apos;said&apos;, &apos;king&apos;), (&apos;thought&apos;, &apos;alice&apos;), (&apos;said&apos;, &apos;hatter&apos;), (&apos;white&apos;, &apos;rabbit&apos;), (&apos;said&apos;, &apos;mock&apos;), (&apos;said&apos;, &apos;caterpillar&apos;), (&apos;said&apos;, &apos;gryphon&apos;)] 二元使用点互信息PMI进行查找搭配finder.nbest(bigram_measures.pmi,10) 三元词组from nltk.collocations import TrigramAssocMeasuresfrom nltk.collocations import TrigramCollocationFinderfinder=TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])trigram_measures=TrigramAssocMeasures()#三元组频率finder.nbest(trigram_measures.raw_freq,10) 输出结果[(&apos;said&apos;, &apos;mock&apos;, &apos;turtle&apos;), (&apos;said&apos;, &apos;march&apos;, &apos;hare&apos;), (&apos;poor&apos;, &apos;little&apos;, &apos;thing&apos;), (&apos;little&apos;, &apos;golden&apos;, &apos;key&apos;), (&apos;march&apos;, &apos;hare&apos;, &apos;said&apos;), (&apos;mock&apos;, &apos;turtle&apos;, &apos;said&apos;), (&apos;white&apos;, &apos;kid&apos;, &apos;gloves&apos;), (&apos;beau&apos;, &apos;ootiful&apos;, &apos;soo&apos;), (&apos;certainly&apos;, &apos;said&apos;, &apos;alice&apos;), (&apos;might&apos;, &apos;well&apos;, &apos;say&apos;)] 三元使用点互信息PMI进行查找搭配finder.nbest(trigram_measures.pmi,10) 输出结果 [(&apos;accustomed&apos;, &apos;usurpation&apos;, &apos;conquest&apos;), (&apos;adjourn&apos;, &apos;immediate&apos;, &apos;adoption&apos;), (&apos;adoption&apos;, &apos;energetic&apos;, &apos;remedies&apos;), (&apos;ancient&apos;, &apos;modern&apos;, &apos;seaography&apos;), (&apos;apple&apos;, &apos;roast&apos;, &apos;turkey&apos;), (&apos;arithmetic&apos;, &apos;ambition&apos;, &apos;distraction&apos;), (&apos;brother&apos;, &apos;latin&apos;, &apos;grammar&apos;), (&apos;canvas&apos;, &apos;bag&apos;, &apos;tied&apos;), (&apos;cherry&apos;, &apos;tart&apos;, &apos;custard&apos;), (&apos;circle&apos;, &apos;exact&apos;, &apos;shape&apos;)] 基于权重标签的短语提取 使用浅层分析提取所有名词短语词块 计算每个词块的TF-IDF权重并返回最大加权短语 toy_text = &quot;&quot;&quot;Elephants are large mammals of the family Elephantidae and the order Proboscidea. Two species are traditionally recognised, the African elephant and the Asian elephant. Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male African elephants are the largest extant terrestrial animals. All elephants have a long trunk used for many purposes, particularly breathing, lifting water and grasping objects. Their incisors grow into tusks, which can serve as weapons and as tools for moving objects and digging. Elephants&apos; large ear flaps help to control their body temperature. Their pillar-like legs can carry their great weight. African elephants have larger ears and concave backs while Asian elephants have smaller ears and convex or level backs. &quot;&quot;&quot; import numpy as npimport itertoolsfrom gensim import corpora, models 基本上，我们有一个已定义的语法模式来分块或提取名词短语。我们在同一模式中定义一个分块器，对于文档中的每个句子，首先用它的POS标签来标注它(因此，不应该对文本进行规范化)，然后构建一个具有名词短语的浅层分析树作为词块和其他全部基于POS标签的单词作为缝隙，缝隙是不属于任何词块的部分。完成此操作后，我们使用tree2conl1tags函数来生成(w, t，c)三元组，它们是的单词、POS标签和IOB格式的词块标签。删除所有词块带有’O ‘标签的标签，因为它们基本上是不属于任何词块的单词或词项。最后，从这些有效的词块中，组合分块的词项，并从每个词块分组中生成短语。 提取文档中的名词短语 v adj adv#提取文档中的名词短语 v adj advdef get_chunks(sentences, grammar = r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;): all_chunks = [] chunker = nltk.chunk.regexp.RegexpParser(grammar) for sentence in sentences: tagged_sents = nltk.pos_tag_sents( [nltk.word_tokenize(sentence)]) chunks = [chunker.parse(tagged_sent) for tagged_sent in tagged_sents] wtc_sents = [nltk.chunk.tree2conlltags(chunk) for chunk in chunks] flattened_chunks = list( itertools.chain.from_iterable( wtc_sent for wtc_sent in wtc_sents) )# print(flattened_chunks)# print(flattened_chunks) valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk in itertools.groupby(flattened_chunks,lambda chunk: chunk != &apos;O&apos;)]# print(&apos;---&apos;*20)# print(valid_chunks_tagged) valid_chunks = [&apos; &apos;.join(word.lower() for word, tag, chunk in wtc_group if word.lower() not in stopword_list) for status, wtc_group in valid_chunks_tagged if status] all_chunks.append(valid_chunks) return all_chunks sentences = parse_document(toy_text) valid_chunks = get_chunks(sentences) 获取TF-IDF关键短语权重def get_tfidf_weighted_keyphrases(sentences, grammar=r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;, top_n=10): valid_chunks = get_chunks(sentences, grammar=grammar) dictionary = corpora.Dictionary(valid_chunks) corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks] tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] weighted_phrases = &#123;dictionary.get(id): round(value,3) for doc in corpus_tfidf for id, value in doc&#125; weighted_phrases = sorted(weighted_phrases.items(), key=itemgetter(1), reverse=True) return weighted_phrases[:top_n] 前两个关键短语get_tfidf_weighted_keyphrases(sentences, top_n=2) 输出结果 [(&apos;elephants large mammals family elephantidae order proboscidea .&apos;, 1.0), (&apos;two species traditionally recognised , african elephant asian elephant .&apos;, 1.0)] 其他语料实验get_tfidf_weighted_keyphrases(alice, top_n=10) 输出结果 [(&quot;[ alice &apos; adventures wonderland lewis carroll 1865 ]&quot;, 1.0), (&apos;chapter .&apos;, 1.0), (&apos;rabbit - hole&apos;, 1.0), (&quot;alice beginning get tired sitting sister bank , nothing : twice peeped book sister reading , pictures conversations , &apos; use book , &apos; thought alice &apos; without pictures conversation ? &apos;&quot;, 1.0), (&apos;considering mind ( well could , hot day made feel sleepy stupid ) , whether pleasure making daisy - chain would worth trouble getting picking daisies , suddenly white rabbit pink eyes ran close .&apos;, 1.0), (&quot;nothing remarkable ; alice think much way hear rabbit say , &apos; oh dear !&quot;, 1.0), (&apos;oh dear !&apos;, 1.0), (&quot;shall late ! &apos;&quot;, 1.0), (&apos;( thought afterwards , occurred ought wondered , time seemed quite natural ) ; rabbit actually took watch waistcoat - pocket , looked , hurried , alice started feet , flashed across mind never seen rabbit either waistcoat - pocket , watch take , burning curiosity , ran across field , fortunately time see pop large rabbit - hole hedge .&apos;, 1.0), (&apos;another moment went alice , never considering world get .&apos;, 1.0)]","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"}],"author":"CinKate"},{"title":"Word2Vec相关(用TFIDF加权词向量)","slug":"word-tfidf","date":"2019-04-05T10:34:06.000Z","updated":"2020-05-17T16:12:00.563Z","comments":true,"path":"2019/04/05/word-tfidf/","link":"","permalink":"http://renxingkai.github.io/2019/04/05/word-tfidf/","excerpt":"","text":"今天是快乐的清明节，而博主还在实验室敲代码，23333这次记录下Word2Vec相关的姿势~ Word2Vec模型直接用开源的gensism库进行词向量训练： import gensimimport nltkimport numpy as np#自制语料CORPUS = [&apos;the sky is blue&apos;,&apos;sky is blue and sky is beautiful&apos;,&apos;the beautiful sky is so blue&apos;,&apos;i love blue cheese&apos;]new_doc = [&apos;loving this blue sky today&apos;] 对语料进行分词 #tokenize corpusTOKENIZED_CORPUS=[nltk.word_tokenize(sentence) for sentence in CORPUS]tokenized_new_doc=[nltk.word_tokenize(sentence) for sentence in new_doc]print(TOKENIZED_CORPUS)print(tokenized_new_doc) 输出 [[&apos;the&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;], [&apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;, &apos;and&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;beautiful&apos;], [&apos;the&apos;, &apos;beautiful&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;so&apos;, &apos;blue&apos;], [&apos;i&apos;, &apos;love&apos;, &apos;blue&apos;, &apos;cheese&apos;]] [[&apos;loving&apos;, &apos;this&apos;, &apos;blue&apos;, &apos;sky&apos;, &apos;today&apos;]] 构建词向量 model=gensim.models.Word2Vec(TOKENIZED_CORPUS,size=10,window=10,min_count=2,sample=1e-3) 平均词向量来表示文档 #num_features表示的文本单词大小def average_word_vectors(words,model,vocabulary,num_features): feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) nwords=0 for word in words: if word in vocabulary: nwords=nwords+1 feature_vector=np.add(feature_vector,model[word]) if nwords: feature_vector=np.divide(feature_vector,nwords) return feature_vectordef averaged_word_vectorizer(corpus,model,num_features): #get the all vocabulary vocabulary=set(model.wv.index2word) features=[average_word_vectors(tokenized_sentence,model,vocabulary,num_features) for tokenized_sentence in corpus] return np.array(features) avg_word_vec_features=averaged_word_vectorizer(TOKENIZED_CORPUS,model=model,num_features=10)print(avg_word_vec_features) 输出array([[-0.00710545, -0.01549264, 0.02188712, -0.00322829, 0.00586532, -0.00687592, 0.00339291, -0.01177494, 0.00265543, -0.00539964], [-0.0157312 , -0.01630003, 0.00551589, 0.00166568, 0.02385859, 0.0085727 , 0.02538068, -0.02266891, 0.02231819, -0.02521743], [-0.0070758 , -0.00578274, 0.01280785, -0.00960104, 0.00821758, -0.00023592, 0.01009926, -0.00624976, 0.00913788, -0.01323305], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.00845861, -0.0247597 ]]) nd_avg_word_vec_features=averaged_word_vectorizer(corpus=tokenized_new_doc,model=model,num_features=10)print(nd_avg_word_vec_features) 输出array([[-0.00968785, -0.02889012, 0.02670473, -0.01596956, 0.00815679, -0.00325876, 0.02226594, -0.01347479, 0.01384218, -0.01042995]]) # TF-IDF加权平均词向量如果直接求平均效果不好的话，或者过于简单的话，可以对词求TFIDF，然后乘以相应的权重 def tfidf_wtd_avg_word_vectors(words,tfidf_vector,tfidf_vocabulary,model,num_features): word_tfidfs=[tfidf_vector[0,tfidf_vocabulary.get(word)] if tfidf_vocabulary.get(word) else 0 for word in words] word_tfidf_map=&#123;word:tfidf_val for word,tfidf_val in zip(words,word_tfidfs)&#125; feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) vocabulary=set(model.wv.index2word) wts=0 for word in words: if word in vocabulary: word_vector=model[word] weighted_word_vector=word_tfidf_map[word]*word_vector wts=wts+word_tfidf_map[word] feature_vector=np.add(feature_vector,weighted_word_vector) if wts: feature_vector=np.divide(feature_vector,wts) return feature_vectordef tfidf_weighted_averaged_word_vectorizer(corpus,tfidf_vectors,tfidf_vocabulary,model,num_features): docs_tfidfs=[(doc,doc_tfidf) for doc,doc_tfidf in zip(corpus,tfidf_vectors)] features=[tfidf_wtd_avg_word_vectors(tokenized_sentence,tfidf,tfidf_vocabulary,model,num_features) for tokenized_sentence,tfidf in docs_tfidfs] return np.array(features) TFIDF预处理from sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerimport pandas as pddef tfidf_transformer(bow_matrix): transformer = TfidfTransformer(norm=&apos;l2&apos;, smooth_idf=True, use_idf=True) tfidf_matrix = transformer.fit_transform(bow_matrix) return transformer, tfidf_matrixdef tfidf_extractor(corpus, ngram_range=(1,1)): vectorizer = TfidfVectorizer(min_df=1, norm=&apos;l2&apos;, smooth_idf=True, use_idf=True, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef bow_extractor(corpus, ngram_range=(1,1)): #min_df为1说明文档中词频最小为1也会被考虑 #ngram_range可以设置(1,3)将建立包括所有unigram、bigram、trigram的向量空间 vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef display_features(features, feature_names): df = pd.DataFrame(data=features, columns=feature_names) print(df) bow_vectorizer, bow_features = bow_extractor(CORPUS)feature_names = bow_vectorizer.get_feature_names()tfidf_trans, tfidf_features = tfidf_transformer(bow_features)tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)display_features(np.round(tdidf_features.todense(), 2), feature_names)nd_tfidf = tfidf_vectorizer.transform(new_doc)display_features(np.round(nd_tfidf.todense(), 2), feature_names) TFIDF加权词向量corpus_tfidf=tfidf_featuresvocab=tfidf_vectorizer.vocabulary_ wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,tfidf_vectors=corpus_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(wt_tfidf_word_vec_features) 输出array([[-0.00728862, -0.01345045, 0.02334223, -0.00258989, 0.00500905, -0.00913428, 0.00057808, -0.01095917, -0.00025702, -0.00165257], [-0.02009719, -0.01936696, 0.0056747 , 0.00887485, 0.02952368, 0.00819392, 0.02715274, -0.0298718 , 0.02297843, -0.0237992 ], [-0.00721121, -0.00258696, 0.01239834, -0.01018197, 0.00795635, -0.00085167, 0.00906817, -0.00469667, 0.00799437, -0.01167674], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.0084586 , -0.0247597 ]]) nd_wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,tfidf_vectors=nd_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(nd_wt_tfidf_word_vec_features) 输出 array([[-0.01223734, -0.02956665, 0.02708268, -0.01397412, 0.01101045, -0.00361711, 0.02421493, -0.01619775, 0.01438254, -0.00899163]])","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"《基于BiDAF多文档重排序的阅读理解模型》论文阅读","slug":"rbidaf","date":"2019-04-02T18:58:21.000Z","updated":"2020-05-17T16:12:00.222Z","comments":true,"path":"2019/04/03/rbidaf/","link":"","permalink":"http://renxingkai.github.io/2019/04/03/rbidaf/","excerpt":"","text":"0 引言目前的机器学习方法主要有两类：抽取式和生成式，抽取式通过给定问题以及相关的文章进行训练,让机器具备阅读的能力，并对提出的新问题,在相关文章中抽取出相应的答案。另一种是生成式,从理论_上来说不受知识的局限,对于问题自动生成答案,但是生成式有时产生的答案答非所问，句式不通,不能很好地体现出人类的思维逻辑以及自然表述的特点。 在本文中，提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。该模型是一-种抽取式的机器阅读理解模型,在BiDAF模型四层网络框架的基础，上添加了ParaRanking层，针对ParaRank-ing,本文提出了多特征融合的ParaRanking 算法，此外本文还在答案预测层,提出了基于先验知识的多答案交叉验证算法,进而对答案进行综合预测。 1 机器阅读理解和相关工作自斯坦福机器阅读理解数据集SQuAD问世以来,经过谷歌、微软、百度、科大讯飞、腾讯、斯坦福大学等在内的众多研究机构的不懈努力,形成了“词向量化-语义编码-语义交互-答案预测”这样一-套四层机器阅读理解模型体系。该体系的主要思想是:首先将自然文本表示为可计算的向量,其次融合问题向量与支撑文档向量来学习到语义交互信息,最后根据交互信息预测答案的位置或逐一输出最大概率的字词来生成答案。 词向量化层(Word-Embedder) 的作用是使用词向量技术将分词后的自然文本转化为稠密、连续且可计算的低维向量。 文本编码层(Encoder)的作用是进行语义加工，向量化层输出的结果是一串独立的词向量序列,而编码层根据这些词向量捕捉词与词的前后语义关系,并把这种语义关系融入词向量中,生成一串互相关联的文本编码序列。 语义交互层（Interaction-Layer)是整个模型体系中最重要的一环。在进入这一层之前，问题与给定支撑文档在大多数情况下是分别独立进行向量转化与语义编码的。当然，在有些模型中，问题词向量序列也会被提前融合到文档向量中。当前大部分研究工作集中在语义交互层的设计上，在这一层，将最终得到混合两者语义的交互向量。此外，交互向量也时常与未交互前的问题编码向量直接拼接，以强调问题语义的重要性。 答案预测层（Answer-Layer)负责根据语义交互向量产出 最终的答案。目前，答案预测模型主要是生成模型与边界模型（边界模型常用的有Pointer Network指出答案所在的开始、结束位置）。答案预测层将答案均视作一串词序列，生成模型逐个预测该序列中每个词应使用给定文档中哪个词进行填充，每次预测均基于之前预测完成的结果。边界模型则相当于一个简化的生成模型，其预先假定问题都可以使用给定文档中的一个连续短语或句子进行回答，因此只需预测答案的起点与终点词的位置即可。目前，边界模型的预测效率与结果均好于生成模型。 以下作者对比了近年来在SQuAD榜上的部分阅读理解模型： Match-LSTM(2016提出， 发表于ICLR 17)Match-LSTM是首个应用于SQuAD数据的端到端机器阅读理解模型，并成功超越原有使用人工特征进行答案抽取的基线模型。该模型的特点是：（1）在文本编码层使用单向LSTM进行语义建模；（2）在语义交互层对支撑文档中的每个词计算该词在问题编码向量上的注意力分配向量，将这一注意力分配向量与问题编码向量点乘获得文档词–问题交互向量，并再拼接上文档词编码向量，最后用一个新的单向LSTM网络对拼接后的向量进行二次语义编码；（3)用反向LSTM重复（1)、（2)操作，并将正反向二次语义编码向量拼接。 BIDAF(2016提出， 发表于ICLR 17)BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。 R-Net(发表于ACL 17)R-Net是对Match-LSTM匹配模型的改进。这一模型最大的特点是采用了双语义交互层设计。在一级语义交互层，R-Net仿照Match-LSTM实现将问题信息融入到每个文档词中去；而在二级语义交互层，R-Net则使用相同办法将已经获得的文档词–问题语义编码向量再度与问题编码向量二次融合，进一步加强语义匹配。 QANet(发表于ICLR 18)QANet则是一种在BIDAF模型基础上为追求效率而设计的模型。该模型非常创新地在文本编码层使用CNN与Multi-Head Self-Attention机制实现语义编码，由于CNN可以捕捉局部特征、Self-Attention能够捕捉全局特征，因此完全可以用它们替代传统的LSTM网络。此外，由于CNN的建模效率显著高于LSTM网络，该模型以在更大规模的数据集上进行深度学习——泛化能力得到了进一步提升。这一模型可以在SQuAD数据集上达到训练速度提高3〜13倍！推理速度提高4~9倍，且获得与先前基于LSTM网络媲美的精度。 V-net(百度公司发表于ACL 18)V-net是一种新的多文档校验深度神经网络建模方法，该模型通过注意力使不同候选文档抽取的答案能够互相印证，从而预测出更好的答案。 2 数据探索和数据处理百度数据集与其他数据集很大的区别在于，每篇文章中包含了很多个段落，而SQuAD数据集的支撑文档直接是一个最相关段落，微软数据集MS MARCO则是若干篇只有一个段落的文章。因此，在百度机器阅读理解任务中，需要在主流四层体系的基础上，增加一个段落定位层。 在DuReader原文中提到，使用recall指标增加的段落定位层，并使用recall指标进行段落选择，可以使模型的效果至少10% 3 RBiDAF模型设计与实现本文提出的基于BiDAF模型的RBiDAF模型，主要是在BiDAF模型的基础上添加了ParaRanking，在该层提出了ParaRanking算法，从而对候选段落进行排序（ParaRanking)操作，进而筛选出含答案概率更高的候选段落。 此外在答案预测层，提出了基于先验知识的多答案交叉验证（MACVerify)算法，从而对答案进行综合预测。 3.1 ParaRanking算法DuReader数据集中，每一个问题对应多个段落,尤其是在Search数据集中，问题和段落的比接近1:57,所以应该尽量检索出含有答案的段落,从而减小候选段落集的数据规模。在这里本文提出了多特征融合的ParaRanking算法,图8是ParaRanking算法的大体架构,主要包括段落过滤、段落重组、语义匹配、最大覆盖度、特征加权以及多文档投票。 3.1.1 段落过滤 本文利用特征工程根据问题类型对不相关段落进行过滤,例如,实体类型的问题中,问题中的关键词是“联系方式”、“热线”，那么本文利用正则表达式将不含电话号码的段落进行过滤，最终本文设计了23条规则对段落进行初步过滤。 3.1.2段落重组 DuReader数据集中的段落长度极度不平衡,有些段落的长度很短,这种情况会造成段落的上下文信息缺失，不利于模型的Match操作。而且本文通过观察训练集中答案的分布，发现有些答案是跨段落的，尤其是描述类的问题，所以如果仅仅以某-一个原始段落作为预测的输人，那么将无法解决答案跨段落的问题,因此本文将原始的段落进行重组，重组后长度控制在长度splice_ L之内。 3.1.3语义匹配 问题(question)与段落(paragraph)间的匹配不仅要考虑问题和段落之间的显式关系,还要考虑两者之间的隐式关系，即两者之间的语义关系。例如，question:北京2017年的商业住房的均价是多少?paragraph:据我所知是四万元一平。上例question和paragraph之间的最大覆盖度虽然为0,但是两者之间具有极大的语义相关性,并且“四万元一平”极有可能是答案。所以为了克服字词匹配上的弊端，本文选择利用深度神经网络计算question和para-graph之间的语义相关性。 由于ARC-II保留了词序信息,更具一般性，所以本文采用ARC-II文本匹配模型对question以及paragraph之间的语义相关度进行计算,在第一层中,首先把卷积窗口设定为k1,然后对句子Squestion和句子Sprangraph中所有组合的二维矩阵进行卷积,每一个二维矩阵输出一个值(文中把这个称作一维卷积,因为实际上是把组合中所有词语的vector排成一行进行的卷积计算),构成Layer-2,然后进行2X2的MaxPooling。后续的卷积层均是传统的二维卷积操作，与第一层卷积层后的简单MaxPooling方式不同,后续的卷积层的Pooling是一种动态Pooling方法。输出固定维度的向量,接着输人MLP层,最终得到文本相似度分数ps。 3.1.4 最大覆盖度 本文沿用了基线模型的最大覆盖度算法,DuReader的基线模型采用问题和段落的最大词级别的覆盖度算法对段落进行排序,然后对每一个篇章挑选top-1作为模型的输入,本文将问题与段落的最大覆盖度作为ParaRanking的一个重要特征值定义为pc,其中不同于基线模型中最大覆盖度算法的是,这里分别选择了词和字两个粒度进行最大覆盖度计算,两者相加作为最终pc的值。 3.1.5 特征加权 首先通过分析DuReader的训练集可知,在描述类问题的答案中存在大量列表类型的答案，所以本文针对描述类问题识别出段落中的列表信息,并根据这一特征对段落的ParaRanking值进行加权，定义权值为B。经过语义匹配、最大覆盖度计算以及特征加权可以得到问题和段落i的最终匹配得分，如式下式所示。 3.1.6 多文档投票 本文两次用到多文档投票，一次在ParaRanking操作中，一次在答案预测中，前后两次所用到的方法有些不同。使用多文档投票是基于某一问题的正确答案在多个段落中会多次出现这一假设。首先 定义候选段落集合为Dp，对于段落i属于Dp，那么每一个段落的投票得分如下式所示。 所以最终得分段落，的最终得分为: 其中，f函数是指数平滑函数，最终经过ParaRanking 算法 ，每一个段落,i(属于Dp)会生成一个分score，随后根据score选择输人模型的段落集合Df，并且Df数量远远小于Dp。 RBiDAF模型架构 5 总结与展望本文提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。首先对DuReader数据集进行分析并对数据进行清洗,从而提取出有利于模型训练的特征;然后本文对RBiDAF机器阅读理解模型进行相关设计和实现,该模型的创新点在于在BiDAF模型四层网络框架的基础上添加了ParaRanking层,在该层,本文提出了基于多特征融合的ParaRanking算法。此外本文还在答案预测层,提出了基于先验知识的MACVerify算法,利用该算法对答案进行综合预测。最后经过实验和分析,RBiDAF模型能够产生有效的答案。在未来的工作中，首先将尝试实验多种词嵌入方法,很多学者证实选择合适的词嵌人方法对该任务会产生很大的影响;其次尝试采用机器翻译模型与对抗式生成模型(GAN)增强训练语料;最后在文本交互层融合双向注意力(Bi-Attention)与多轮匹配机制(Multi-Matching),从而可以在多文档场景下取得更好的效果。 论文地址","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"英文文本预处理代码","slug":"textpreprocess","date":"2019-03-29T21:50:08.000Z","updated":"2020-05-17T16:12:00.814Z","comments":true,"path":"2019/03/30/textpreprocess/","link":"","permalink":"http://renxingkai.github.io/2019/03/30/textpreprocess/","excerpt":"","text":"贴一段在做Kaggle QIQC时别人开源的kernel英语文本预处理代码，在做英文nlp任务时还是很有用的~ import osimport reimport gcimport stringimport unicodedataimport operatorimport numpy as npimport pandas as pdfrom tqdm import tqdmtqdm.pandas()&quot;&quot;&quot;utils&quot;&quot;&quot;def load_data(datapath): print(&quot;loading data ......&quot;) df_train = pd.read_csv(os.path.join(datapath, &quot;train.csv&quot;)) df_test = pd.read_csv(os.path.join(datapath, &quot;test.csv&quot;)) print(&quot;train data with shape : &quot;, df_train.shape) print(&quot;test data with shape : &quot;, df_test.shape) return df_train, df_test&quot;&quot;&quot;nlp&quot;&quot;&quot;def clean_misspell(text): &quot;&quot;&quot; misspell list (quora vs. glove) &quot;&quot;&quot; misspell_to_sub = &#123; &apos;Terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;BIMARU&apos;: &apos;Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh&apos;, &apos;Hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;Hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;Babchenko&apos;: &apos;Arkady Arkadyevich Babchenko faked death&apos;, &apos;Boshniaks&apos;: &apos;Bosniaks&apos;, &apos;Dravidanadu&apos;: &apos;Dravida Nadu&apos;, &apos;mysoginists&apos;: &apos;misogynists&apos;, &apos;MGTOWS&apos;: &apos;Men Going Their Own Way&apos;, &apos;mongloid&apos;: &apos;Mongoloid&apos;, &apos;unsincere&apos;: &apos;insincere&apos;, &apos;meninism&apos;: &apos;male feminism&apos;, &apos;jewplicate&apos;: &apos;jewish replicate&apos;, &apos;unoin&apos;: &apos;Union&apos;, &apos;daesh&apos;: &apos;Islamic State of Iraq and the Levant&apos;, &apos;Kalergi&apos;: &apos;Coudenhove-Kalergi&apos;, &apos;Bhakts&apos;: &apos;Bhakt&apos;, &apos;bhakts&apos;: &apos;Bhakt&apos;, &apos;Tambrahms&apos;: &apos;Tamil Brahmin&apos;, &apos;Pahul&apos;: &apos;Amrit Sanskar&apos;, &apos;SJW&apos;: &apos;social justice warrior&apos;, &apos;SJWs&apos;: &apos;social justice warrior&apos;, &apos; incel&apos;: &apos; involuntary celibates&apos;, &apos; incels&apos;: &apos; involuntary celibates&apos;, &apos;emiratis&apos;: &apos;Emiratis&apos;, &apos;weatern&apos;: &apos;western&apos;, &apos;westernise&apos;: &apos;westernize&apos;, &apos;Pizzagate&apos;: &apos;Pizzagate conspiracy theory&apos;, &apos;naïve&apos;: &apos;naive&apos;, &apos;Skripal&apos;: &apos;Sergei Skripal&apos;, &apos;Remainers&apos;: &apos;British remainer&apos;, &apos;remainers&apos;: &apos;British remainer&apos;, &apos;bremainer&apos;: &apos;British remainer&apos;, &apos;antibrahmin&apos;: &apos;anti Brahminism&apos;, &apos;HYPSM&apos;: &apos; Harvard, Yale, Princeton, Stanford, MIT&apos;, &apos;HYPS&apos;: &apos; Harvard, Yale, Princeton, Stanford&apos;, &apos;kompromat&apos;: &apos;compromising material&apos;, &apos;Tharki&apos;: &apos;pervert&apos;, &apos;tharki&apos;: &apos;pervert&apos;, &apos;mastuburate&apos;: &apos;masturbate&apos;, &apos;Zoë&apos;: &apos;Zoe&apos;, &apos;indans&apos;: &apos;Indian&apos;, &apos; xender&apos;: &apos; gender&apos;, &apos;Naxali &apos;: &apos;Naxalite &apos;, &apos;Naxalities&apos;: &apos;Naxalites&apos;, &apos;Bathla&apos;: &apos;Namit Bathla&apos;, &apos;Mewani&apos;: &apos;Indian politician Jignesh Mevani&apos;, &apos;clichéd&apos;: &apos;cliche&apos;, &apos;cliché&apos;: &apos;cliche&apos;, &apos;clichés&apos;: &apos;cliche&apos;, &apos;Wjy&apos;: &apos;Why&apos;, &apos;Fadnavis&apos;: &apos;Indian politician Devendra Fadnavis&apos;, &apos;Awadesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Awdhesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Khalistanis&apos;: &apos;Sikh separatist movement&apos;, &apos;madheshi&apos;: &apos;Madheshi&apos;, &apos;BNBR&apos;: &apos;Be Nice, Be Respectful&apos;, &apos;Bolsonaro&apos;: &apos;Jair Bolsonaro&apos;, &apos;XXXTentacion&apos;: &apos;Tentacion&apos;, &apos;Padmavat&apos;: &apos;Indian Movie Padmaavat&apos;, &apos;Žižek&apos;: &apos;Slovenian philosopher Slavoj Žižek&apos;, &apos;Adityanath&apos;: &apos;Indian monk Yogi Adityanath&apos;, &apos;Brexit&apos;: &apos;British Exit&apos;, &apos;Brexiter&apos;: &apos;British Exit supporter&apos;, &apos;Brexiters&apos;: &apos;British Exit supporters&apos;, &apos;Brexiteer&apos;: &apos;British Exit supporter&apos;, &apos;Brexiteers&apos;: &apos;British Exit supporters&apos;, &apos;Brexiting&apos;: &apos;British Exit&apos;, &apos;Brexitosis&apos;: &apos;British Exit disorder&apos;, &apos;brexit&apos;: &apos;British Exit&apos;, &apos;brexiters&apos;: &apos;British Exit supporters&apos;, &apos;jallikattu&apos;: &apos;Jallikattu&apos;, &apos;fortnite&apos;: &apos;Fortnite &apos;, &apos;Swachh&apos;: &apos;Swachh Bharat mission campaign &apos;, &apos;Quorans&apos;: &apos;Quoran&apos;, &apos;Qoura &apos;: &apos;Quora &apos;, &apos;quoras&apos;: &apos;Quora&apos;, &apos;Quroa&apos;: &apos;Quora&apos;, &apos;QUORA&apos;: &apos;Quora&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, # extra in sample &apos;Doklam&apos;: &apos;Tibet&apos;, &apos;Drumpf &apos;: &apos;Donald Trump fool &apos;, &apos;Drumpfs&apos;: &apos;Donald Trump fools&apos;, &apos;Strzok&apos;: &apos;Hillary Clinton scandal&apos;, &apos;rohingya&apos;: &apos;Rohingya &apos;, &apos;wumao &apos;: &apos;cheap Chinese stuff&apos;, &apos;wumaos&apos;: &apos;cheap Chinese stuff&apos;, &apos;Sanghis&apos;: &apos;Sanghi&apos;, &apos;Tamilans&apos;: &apos;Tamils&apos;, &apos;biharis&apos;: &apos;Biharis&apos;, &apos;Rejuvalex&apos;: &apos;hair growth formula&apos;, &apos;Feku&apos;: &apos;The Man of India &apos;, &apos;deplorables&apos;: &apos;deplorable&apos;, &apos;muhajirs&apos;: &apos;Muslim immigrant&apos;, &apos;Gujratis&apos;: &apos;Gujarati&apos;, &apos;Chutiya&apos;: &apos;Tibet people &apos;, &apos;Chutiyas&apos;: &apos;Tibet people &apos;, &apos;thighing&apos;: &apos;masturbate&apos;, &apos;卐&apos;: &apos;Nazi Germany&apos;, &apos;Pribumi&apos;: &apos;Native Indonesian&apos;, &apos;Gurmehar&apos;: &apos;Gurmehar Kaur Indian student activist&apos;, &apos;Novichok&apos;: &apos;Soviet Union agents&apos;, &apos;Khazari&apos;: &apos;Khazars&apos;, &apos;Demonetization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;demonitisation&apos;: &apos;demonetization&apos;, &apos;demonitization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;cryptocurrencies&apos;: &apos;cryptocurrency&apos;, &apos;Hindians&apos;: &apos;North Indian who hate British&apos;, &apos;vaxxer&apos;: &apos;vocal nationalist &apos;, &apos;remoaner&apos;: &apos;remainer &apos;, &apos;bremoaner&apos;: &apos;British remainer &apos;, &apos;Jewism&apos;: &apos;Judaism&apos;, &apos;Eroupian&apos;: &apos;European&apos;, &apos;WMAF&apos;: &apos;White male married Asian female&apos;, &apos;moeslim&apos;: &apos;Muslim&apos;, &apos;cishet&apos;: &apos;cisgender and heterosexual person&apos;, &apos;Eurocentric&apos;: &apos;Eurocentrism &apos;, &apos;Jewdar&apos;: &apos;Jew dar&apos;, &apos;Asifa&apos;: &apos;abduction, rape, murder case &apos;, &apos;marathis&apos;: &apos;Marathi&apos;, &apos;Trumpanzees&apos;: &apos;Trump chimpanzee fool&apos;, &apos;Crimean&apos;: &apos;Crimea people &apos;, &apos;atrracted&apos;: &apos;attract&apos;, &apos;LGBT&apos;: &apos;lesbian, gay, bisexual, transgender&apos;, &apos;Boshniak&apos;: &apos;Bosniaks &apos;, &apos;Myeshia&apos;: &apos;widow of Green Beret killed in Niger&apos;, &apos;demcoratic&apos;: &apos;Democratic&apos;, &apos;raaping&apos;: &apos;rape&apos;, &apos;Dönmeh&apos;: &apos;Islam&apos;, &apos;feminazism&apos;: &apos;feminism nazi&apos;, &apos;langague&apos;: &apos;language&apos;, &apos;Hongkongese&apos;: &apos;HongKong people&apos;, &apos;hongkongese&apos;: &apos;HongKong people&apos;, &apos;Kashmirians&apos;: &apos;Kashmirian&apos;, &apos;Chodu&apos;: &apos;fucker&apos;, &apos;penish&apos;: &apos;penis&apos;, &apos;micropenis&apos;: &apos;tiny penis&apos;, &apos;Madridiots&apos;: &apos;Real Madrid idiot supporters&apos;, &apos;Ambedkarite&apos;: &apos;Dalit Buddhist movement &apos;, &apos;ReleaseTheMemo&apos;: &apos;cry for the right and Trump supporters&apos;, &apos;harrase&apos;: &apos;harass&apos;, &apos;Barracoon&apos;: &apos;Black slave&apos;, &apos;Castrater&apos;: &apos;castration&apos;, &apos;castrater&apos;: &apos;castration&apos;, &apos;Rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;Turkified&apos;: &apos;Turkification&apos;, &apos;turkified&apos;: &apos;Turkification&apos;, &apos;Dumbassistan&apos;: &apos;dumb ass Pakistan&apos;, &apos;facetards&apos;: &apos;Facebook retards&apos;, &apos;rapefugees&apos;: &apos;rapist refugee&apos;, &apos;superficious&apos;: &apos;superficial&apos;, # extra from kagglers &apos;colour&apos;: &apos;color&apos;, &apos;centre&apos;: &apos;center&apos;, &apos;favourite&apos;: &apos;favorite&apos;, &apos;travelling&apos;: &apos;traveling&apos;, &apos;counselling&apos;: &apos;counseling&apos;, &apos;theatre&apos;: &apos;theater&apos;, &apos;cancelled&apos;: &apos;canceled&apos;, &apos;labour&apos;: &apos;labor&apos;, &apos;organisation&apos;: &apos;organization&apos;, &apos;wwii&apos;: &apos;world war 2&apos;, &apos;citicise&apos;: &apos;criticize&apos;, &apos;youtu &apos;: &apos;youtube &apos;, &apos;sallary&apos;: &apos;salary&apos;, &apos;Whta&apos;: &apos;What&apos;, &apos;narcisist&apos;: &apos;narcissist&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, &apos;howdo&apos;: &apos;how do&apos;, &apos;whatare&apos;: &apos;what are&apos;, &apos;howcan&apos;: &apos;how can&apos;, &apos;howmuch&apos;: &apos;how much&apos;, &apos;howmany&apos;: &apos;how many&apos;, &apos;whydo&apos;: &apos;why do&apos;, &apos;doI&apos;: &apos;do I&apos;, &apos;theBest&apos;: &apos;the best&apos;, &apos;howdoes&apos;: &apos;how does&apos;, &apos;mastrubation&apos;: &apos;masturbation&apos;, &apos;mastrubate&apos;: &apos;masturbate&apos;, &apos;mastrubating&apos;: &apos;masturbating&apos;, &apos;pennis&apos;: &apos;penis&apos;, &apos;Etherium&apos;: &apos;Ethereum&apos;, &apos;bigdata&apos;: &apos;big data&apos;, &apos;2k17&apos;: &apos;2017&apos;, &apos;2k18&apos;: &apos;2018&apos;, &apos;qouta&apos;: &apos;quota&apos;, &apos;exboyfriend&apos;: &apos;ex boyfriend&apos;, &apos;airhostess&apos;: &apos;air hostess&apos;, &apos;whst&apos;: &apos;what&apos;, &apos;watsapp&apos;: &apos;whatsapp&apos;, # extra &apos;bodyshame&apos;: &apos;body shaming&apos;, &apos;bodyshoppers&apos;: &apos;body shopping&apos;, &apos;bodycams&apos;: &apos;body cams&apos;, &apos;Cananybody&apos;: &apos;Can any body&apos;, &apos;deadbody&apos;: &apos;dead body&apos;, &apos;deaddict&apos;: &apos;de addict&apos;, &apos;Northindian&apos;: &apos;North Indian &apos;, &apos;northindian&apos;: &apos;north Indian &apos;, &apos;northkorea&apos;: &apos;North Korea&apos;, &apos;Whykorean&apos;: &apos;Why Korean&apos;, &apos;koreaboo&apos;: &apos;Korea boo &apos;, &apos;Brexshit&apos;: &apos;British Exit bullshit&apos;, &apos;shithole&apos;: &apos; shithole &apos;, &apos;shitpost&apos;: &apos;shit post&apos;, &apos;shitslam&apos;: &apos;shit Islam&apos;, &apos;shitlords&apos;: &apos;shit lords&apos;, &apos;Fck&apos;: &apos;Fuck&apos;, &apos;fck&apos;: &apos;fuck&apos;, &apos;Clickbait&apos;: &apos;click bait &apos;, &apos;clickbait&apos;: &apos;click bait &apos;, &apos;mailbait&apos;: &apos;mail bait&apos;, &apos;healhtcare&apos;: &apos;healthcare&apos;, &apos;trollbots&apos;: &apos;troll bots&apos;, &apos;trollled&apos;: &apos;trolled&apos;, &apos;trollimg&apos;: &apos;trolling&apos;, &apos;cybertrolling&apos;: &apos;cyber trolling&apos;, &apos;sickular&apos;: &apos;India sick secular &apos;, &apos;suckimg&apos;: &apos;sucking&apos;, &apos;Idiotism&apos;: &apos;idiotism&apos;, &apos;Niggerism&apos;: &apos;Nigger&apos;, &apos;Niggeriah&apos;: &apos;Nigger&apos; &#125; misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_to_sub.keys())) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = misspell_to_sub.get(match.group(0)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return misspell_re.sub(_replace, text)def spacing_misspell(text): &quot;&quot;&quot; &apos;deadbody&apos; -&gt; &apos;dead body&apos; &quot;&quot;&quot; misspell_list = [ &apos;(F|f)uck&apos;, &apos;Trump&apos;, &apos;\\W(A|a)nti&apos;, &apos;(W|w)hy&apos;, &apos;(W|w)hat&apos;, &apos;How&apos;, &apos;care\\W&apos;, &apos;\\Wover&apos;, &apos;gender&apos;, &apos;people&apos;, ] misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_list)) return misspell_re.sub(r&quot; \\1 &quot;, text)def clean_latex(text): &quot;&quot;&quot; convert r&quot;[math]\\vec&#123;x&#125; + \\vec&#123;y&#125;&quot; to English &quot;&quot;&quot; # edge case text = re.sub(r&apos;\\[math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\[\\/math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\\\&apos;, &apos; LaTex &apos;, text) pattern_to_sub = &#123; r&apos;\\\\mathrm&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\mathbb&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\boxed&apos;: &apos; LaTex equation &apos;, r&apos;\\\\begin&apos;: &apos; LaTex equation &apos;, r&apos;\\\\end&apos;: &apos; LaTex equation &apos;, r&apos;\\\\left&apos;: &apos; LaTex equation &apos;, r&apos;\\\\right&apos;: &apos; LaTex equation &apos;, r&apos;\\\\(over|under)brace&apos;: &apos; LaTex equation &apos;, r&apos;\\\\text&apos;: &apos; LaTex equation &apos;, r&apos;\\\\vec&apos;: &apos; vector &apos;, r&apos;\\\\var&apos;: &apos; variable &apos;, r&apos;\\\\theta&apos;: &apos; theta &apos;, r&apos;\\\\mu&apos;: &apos; average &apos;, r&apos;\\\\min&apos;: &apos; minimum &apos;, r&apos;\\\\max&apos;: &apos; maximum &apos;, r&apos;\\\\sum&apos;: &apos; + &apos;, r&apos;\\\\times&apos;: &apos; * &apos;, r&apos;\\\\cdot&apos;: &apos; * &apos;, r&apos;\\\\hat&apos;: &apos; ^ &apos;, r&apos;\\\\frac&apos;: &apos; / &apos;, r&apos;\\\\div&apos;: &apos; / &apos;, r&apos;\\\\sin&apos;: &apos; Sine &apos;, r&apos;\\\\cos&apos;: &apos; Cosine &apos;, r&apos;\\\\tan&apos;: &apos; Tangent &apos;, r&apos;\\\\infty&apos;: &apos; infinity &apos;, r&apos;\\\\int&apos;: &apos; integer &apos;, r&apos;\\\\in&apos;: &apos; in &apos;, &#125; # post process for look up pattern_dict = &#123;k.strip(&apos;\\\\&apos;): v for k, v in pattern_to_sub.items()&#125; # init re patterns = pattern_to_sub.keys() pattern_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(patterns)) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = pattern_dict.get(match.group(0).strip(&apos;\\\\&apos;)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return pattern_re.sub(_replace, text)def normalize_unicode(text): &quot;&quot;&quot; unicode string normalization &quot;&quot;&quot; return unicodedata.normalize(&apos;NFKD&apos;, text)def remove_newline(text): &quot;&quot;&quot; remove \\n and \\t &quot;&quot;&quot; text = re.sub(&apos;\\n&apos;, &apos; &apos;, text) text = re.sub(&apos;\\t&apos;, &apos; &apos;, text) text = re.sub(&apos;\\b&apos;, &apos; &apos;, text) text = re.sub(&apos;\\r&apos;, &apos; &apos;, text) return textdef decontracted(text): &quot;&quot;&quot; de-contract the contraction &quot;&quot;&quot; # specific text = re.sub(r&quot;(W|w)on(\\&apos;|\\’)t&quot;, &quot;will not&quot;, text) text = re.sub(r&quot;(C|c)an(\\&apos;|\\’)t&quot;, &quot;can not&quot;, text) text = re.sub(r&quot;(Y|y)(\\&apos;|\\’)all&quot;, &quot;you all&quot;, text) text = re.sub(r&quot;(Y|y)a(\\&apos;|\\’)ll&quot;, &quot;you all&quot;, text) # general text = re.sub(r&quot;(I|i)(\\&apos;|\\’)m&quot;, &quot;i am&quot;, text) text = re.sub(r&quot;(A|a)in(\\&apos;|\\’)t&quot;, &quot;is not&quot;, text) text = re.sub(r&quot;n(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)re&quot;, &quot; are&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)s&quot;, &quot; is&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)d&quot;, &quot; would&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ll&quot;, &quot; will&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ve&quot;, &quot; have&quot;, text) return textdef spacing_punctuation(text): &quot;&quot;&quot; add space before and after punctuation and symbols &quot;&quot;&quot; regular_punct = list(string.punctuation) extra_punct = [ &apos;,&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;:&apos;, &apos;)&apos;, &apos;(&apos;, &apos;-&apos;, &apos;!&apos;, &apos;?&apos;, &apos;|&apos;, &apos;;&apos;, &quot;&apos;&quot;, &apos;$&apos;, &apos;&amp;&apos;, &apos;/&apos;, &apos;[&apos;, &apos;]&apos;, &apos;&gt;&apos;, &apos;%&apos;, &apos;=&apos;, &apos;#&apos;, &apos;*&apos;, &apos;+&apos;, &apos;\\\\&apos;, &apos;•&apos;, &apos;~&apos;, &apos;@&apos;, &apos;£&apos;, &apos;·&apos;, &apos;_&apos;, &apos;&#123;&apos;, &apos;&#125;&apos;, &apos;©&apos;, &apos;^&apos;, &apos;®&apos;, &apos;`&apos;, &apos;&lt;&apos;, &apos;→&apos;, &apos;°&apos;, &apos;€&apos;, &apos;™&apos;, &apos;›&apos;, &apos;♥&apos;, &apos;←&apos;, &apos;×&apos;, &apos;§&apos;, &apos;″&apos;, &apos;′&apos;, &apos;Â&apos;, &apos;█&apos;, &apos;½&apos;, &apos;à&apos;, &apos;…&apos;, &apos;“&apos;, &apos;★&apos;, &apos;”&apos;, &apos;–&apos;, &apos;●&apos;, &apos;â&apos;, &apos;►&apos;, &apos;−&apos;, &apos;¢&apos;, &apos;²&apos;, &apos;¬&apos;, &apos;░&apos;, &apos;¶&apos;, &apos;↑&apos;, &apos;±&apos;, &apos;¿&apos;, &apos;▾&apos;, &apos;═&apos;, &apos;¦&apos;, &apos;║&apos;, &apos;―&apos;, &apos;¥&apos;, &apos;▓&apos;, &apos;—&apos;, &apos;‹&apos;, &apos;─&apos;, &apos;▒&apos;, &apos;：&apos;, &apos;¼&apos;, &apos;⊕&apos;, &apos;▼&apos;, &apos;▪&apos;, &apos;†&apos;, &apos;■&apos;, &apos;’&apos;, &apos;▀&apos;, &apos;¨&apos;, &apos;▄&apos;, &apos;♫&apos;, &apos;☆&apos;, &apos;é&apos;, &apos;¯&apos;, &apos;♦&apos;, &apos;¤&apos;, &apos;▲&apos;, &apos;è&apos;, &apos;¸&apos;, &apos;¾&apos;, &apos;Ã&apos;, &apos;⋅&apos;, &apos;‘&apos;, &apos;∞&apos;, &apos;∙&apos;, &apos;）&apos;, &apos;↓&apos;, &apos;、&apos;, &apos;│&apos;, &apos;（&apos;, &apos;»&apos;, &apos;，&apos;, &apos;♪&apos;, &apos;╩&apos;, &apos;╚&apos;, &apos;³&apos;, &apos;・&apos;, &apos;╦&apos;, &apos;╣&apos;, &apos;╔&apos;, &apos;╗&apos;, &apos;▬&apos;, &apos;❤&apos;, &apos;ï&apos;, &apos;Ø&apos;, &apos;¹&apos;, &apos;≤&apos;, &apos;‡&apos;, &apos;√&apos;, &apos;«&apos;, &apos;»&apos;, &apos;´&apos;, &apos;º&apos;, &apos;¾&apos;, &apos;¡&apos;, &apos;§&apos;, &apos;£&apos;, &apos;₤&apos;] all_punct = &apos;&apos;.join(sorted(list(set(regular_punct + extra_punct)))) re_tok = re.compile(f&apos;([&#123;all_punct&#125;])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_digit(text): &quot;&quot;&quot; add space before and after digits &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_number(text): &quot;&quot;&quot; add space before and after numbers &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9]&#123;1,&#125;)&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def remove_number(text): &quot;&quot;&quot; numbers are not toxic &quot;&quot;&quot; return re.sub(&apos;\\d+&apos;, &apos; &apos;, text)def remove_space(text): &quot;&quot;&quot; remove extra spaces and ending space if any &quot;&quot;&quot; text = re.sub(&apos;\\s+&apos;, &apos; &apos;, text) text = re.sub(&apos;\\s+$&apos;, &apos;&apos;, text) return text&quot;&quot;&quot;tokenizer&quot;&quot;&quot;def preprocess(text, remove_num=True): &quot;&quot;&quot; preprocess text into clean text for tokenization NOTE: 1. glove supports uppper case words 2. glove supports digit 3. glove supports punctuation 5. glove supports domains e.g. www.apple.com 6. glove supports misspelled words e.g. FUCKKK &quot;&quot;&quot; # # 1. normalize # text = normalize_unicode(text) # # 2. remove new line # text = remove_newline(text) # 3. de-contract text = decontracted(text) # 4. clean misspell text = clean_misspell(text) # 5. space misspell text = spacing_misspell(text) # 6. clean_latex text = clean_latex(text) # 7. space text = spacing_punctuation(text) # 8. handle number if remove_num: text = remove_number(text) else: text = spacing_digit(text) # 9. remove space text = remove_space(text) return text 调用preprocess(text) 就好，返回处理完后的文本","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"}],"author":"CinKate"},{"title":"nltk---词性标注","slug":"nltk-postag","date":"2019-03-29T09:06:25.000Z","updated":"2020-05-17T16:11:59.994Z","comments":true,"path":"2019/03/29/nltk-postag/","link":"","permalink":"http://renxingkai.github.io/2019/03/29/nltk-postag/","excerpt":"","text":"1.POS标签器推荐使用nltk推荐的pos_tag()函数，基于Penn Treebank，以下代码展示了使用nltk获取句子POS标签的方法： sentence = &apos;The brown fox is quick and he is jumping over the lazy dog&apos;# recommended tagger based on PTBimport nltktokens = nltk.word_tokenize(sentence)tagged_sent = nltk.pos_tag(tokens, tagset=&apos;universal&apos;)print (tagged_sent) 输出： [(&apos;The&apos;, &apos;DET&apos;), (&apos;brown&apos;, &apos;ADJ&apos;), (&apos;fox&apos;, &apos;NOUN&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;quick&apos;, &apos;ADJ&apos;), (&apos;and&apos;, &apos;CONJ&apos;), (&apos;he&apos;, &apos;PRON&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;jumping&apos;, &apos;VERB&apos;), (&apos;over&apos;, &apos;ADP&apos;), (&apos;the&apos;, &apos;DET&apos;), (&apos;lazy&apos;, &apos;ADJ&apos;), (&apos;dog&apos;, &apos;NOUN&apos;)] 2.建立自己的POS标签器准备数据：# preparing the datafrom nltk.corpus import treebankdata = treebank.tagged_sents()train_data = data[:3500]test_data = data[3500:]print (train_data[0]) 输出：[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;61&apos;, &apos;CD&apos;), (&apos;years&apos;, &apos;NNS&apos;), (&apos;old&apos;, &apos;JJ&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;will&apos;, &apos;MD&apos;), (&apos;join&apos;, &apos;VB&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;board&apos;, &apos;NN&apos;), (&apos;as&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;DT&apos;), (&apos;nonexecutive&apos;, &apos;JJ&apos;), (&apos;director&apos;, &apos;NN&apos;), (&apos;Nov.&apos;, &apos;NNP&apos;), (&apos;29&apos;, &apos;CD&apos;), (&apos;.&apos;, &apos;.&apos;)] 2.1DefaultTagger默认标签器首先我们试下从SequentialBackoffTagger基类继承的DefaultTagger，并为每个单词分配相同的用户输入POS标签。 # default taggerfrom nltk.tag import DefaultTaggerdt = DefaultTagger(&apos;NN&apos;)print(dt.evaluate(test_data))print(dt.tag(tokens)) 输出： 0.1454158195372253[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;jumping&apos;, &apos;NN&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 默认获得了14%的准确率，由于给标签器输入的都是相同的标签（‘NN’），因此输出标签获得的都是名词。 2.2RegexpTagger正则表达式标签器# regex taggerfrom nltk.tag import RegexpTagger# define regex tag patternspatterns = [ (r&apos;.*ing$&apos;, &apos;VBG&apos;), # gerunds (r&apos;.*ed$&apos;, &apos;VBD&apos;), # simple past (r&apos;.*es$&apos;, &apos;VBZ&apos;), # 3rd singular present (r&apos;.*ould$&apos;, &apos;MD&apos;), # modals (r&apos;.*\\&apos;s$&apos;, &apos;NN$&apos;), # possessive nouns (r&apos;.*s$&apos;, &apos;NNS&apos;), # plural nouns (r&apos;^-?[0-9]+(.[0-9]+)?$&apos;, &apos;CD&apos;), # cardinal numbers (r&apos;.*&apos;, &apos;NN&apos;) # nouns (default) ... ]rt = RegexpTagger(patterns)print(rt.evaluate(test_data))print(rt.tag(tokens)) 输出： 0.24039113176493368[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率提高到了24%，还是有效果的~ 2.3一、二、三元标签器## N gram taggersfrom nltk.tag import UnigramTaggerfrom nltk.tag import BigramTaggerfrom nltk.tag import TrigramTaggerut = UnigramTagger(train_data)bt = BigramTagger(train_data)tt = TrigramTagger(train_data)print(ut.evaluate(test_data))print(ut.tag(tokens))print(bt.evaluate(test_data))print(bt.tag(tokens))print (tt.evaluate(test_data))print(tt.tag(tokens)) 输出： 0.8607803272340013[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.13466937748087907[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.08064672281924679[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)] 发现一元的准确率最高，达到了86%,二、三元准确率低的原因可能是在训练数据中观察到的二元词组和三元词组不一定会在测试数据中以相同的方式出现。 2.4包含标签列表的组合标签器及使用backoff标签器本质上，我们将创建一个标签器链，对于每一个标签器，吐过他不能标记输入的标识，则标签器的下一步将会回退到backoff标签器： def combined_tagger(train_data, taggers, backoff=None): for tagger in taggers: backoff = tagger(train_data, backoff=backoff) return backoff#backoff to regtaggerct = combined_tagger(train_data=train_data, taggers=[UnigramTagger, BigramTagger, TrigramTagger], backoff=rt)print(ct.evaluate(test_data)) print(ct.tag(tokens)) 输出： 0.9094781682641108[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率到了90% 2.5ClassifierBasedPOSTagger标签器(有监督分类算法)使用ClassifierBasedPOSTagger类中的classifier_builder参数中的有监督机器学习算法来训练标签器。 from nltk.classify import NaiveBayesClassifier, MaxentClassifierfrom nltk.tag.sequential import ClassifierBasedPOSTaggernbt = ClassifierBasedPOSTagger(train=train_data, classifier_builder=NaiveBayesClassifier.train)print(nbt.evaluate(test_data))print(nbt.tag(tokens)) 输出： 0.9306806079969019[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;JJ&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;JJ&apos;), (&apos;dog&apos;, &apos;VBG&apos;)] 有监督准确率达到了0.93 2.6Just For Fun!(MaxentClassifier)# try this out for fun!met = ClassifierBasedPOSTagger(train=train_data, classifier_builder=MaxentClassifier.train)print(met.evaluate(test_data)) print(met.tag(tokens)) 输出： ==&gt; Training (100 iterations) Iteration Log Likelihood Accuracy --------------------------------------- 1 -3.82864 0.007 2 -0.76176 0.957 Final nan 0.9840.9269048310581857[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] MaxentClassifier准确率达到了0.92","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"}],"author":"CinKate"},{"title":"nltk---分词与文本预处理","slug":"nltk-tokenize","date":"2019-03-28T15:08:16.000Z","updated":"2020-05-17T16:12:00.073Z","comments":true,"path":"2019/03/28/nltk-tokenize/","link":"","permalink":"http://renxingkai.github.io/2019/03/28/nltk-tokenize/","excerpt":"","text":"参考《text-analytics-with-python》中的第三章中的处理和理解文本对nltk等常用nlp包进行总结，以供之后复习与使用~ 1.tokenize(切分词(句子))首先，标识(token)是具有一定的句法语义且独立的最小文本成分， 1.1句子切分句子切分基本技术包括在句子之间寻找特定的分割符，例如句号(‘.’)，换行符(‘\\n’)或者分号(‘;’)等。在nltk中，主要关注以下句子切分器: nltk.sent_tokenize(默认句子切分器) nltk.tokenize.PunktSentenceTokenizer() nltk.tokenize.RegexpTokenizer()以下直接上代码： import nltkfrom nltk.corpus import gutenbergfrom pprint import pprint#载入语料alice=gutenberg.raw(fileids=&apos;carroll-alice.txt&apos;)sample_text = &apos;We will discuss briefly about the basic syntax,\\ structure and design philosophies. \\ There is a defined hierarchical syntax for Python code which you should remember \\ when writing code! Python is a really powerful programming language!&apos;# Total characters in Alice in Wonderlandprint(len(alice))# First 100 characters in the corpusprint(alice[0:100]) 输出： 144395[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]CHAPTER I. Down the Rabbit-HoleAlice was 1.1.1默认分词器–nltk.sent_tokenize#默认分词器default_st = nltk.sent_tokenizealice_sentences = default_st(text=alice)sample_sentences = default_st(text=sample_text)print(&apos;Total sentences in sample_text:&apos;, len(sample_sentences))print(&apos;Sample text sentences :-&apos;)pprint(sample_sentences)print(&apos;\\nTotal sentences in alice:&apos;, len(alice_sentences))print(&apos;First 5 sentences in alice:-&apos;)pprint(alice_sentences[0:5]) 输出： Total sentences in sample_text: 3Sample text sentences :-[&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;]Total sentences in alice: 1625First 5 sentences in alice:-[&quot;[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.&quot;, &apos;Down the Rabbit-Hole\\n&apos; &apos;\\n&apos; &apos;Alice was beginning to get very tired of sitting by her sister on the\\n&apos; &apos;bank, and of having nothing to do: once or twice she had peeped into the\\n&apos; &apos;book her sister was reading, but it had no pictures or conversations in\\n&apos; &quot;it, &apos;and what is the use of a book,&apos; thought Alice &apos;without pictures or\\n&quot; &quot;conversation?&apos;&quot;, &apos;So she was considering in her own mind (as well as she could, for the\\n&apos; &apos;hot day made her feel very sleepy and stupid), whether the pleasure\\n&apos; &apos;of making a daisy-chain would be worth the trouble of getting up and\\n&apos; &apos;picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n&apos; &apos;close by her.&apos;, &apos;There was nothing so VERY remarkable in that; nor did Alice think it so\\n&apos; &quot;VERY much out of the way to hear the Rabbit say to itself, &apos;Oh dear!&quot;, &apos;Oh dear!&apos;] 德语切分 ## 其他语言句子切分器from nltk.corpus import europarl_raw#德语german_text = europarl_raw.german.raw(fileids=&apos;ep-00-01-17.de&apos;)# 语料中的词数print(len(german_text))# 前100字符print(german_text[0:100]) 输出： 157171 Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit 使用默认分词器切分德语# 默认句子切分器german_sentences_def = default_st(text=german_text, language=&apos;german&apos;)# loading german text tokenizer into a PunktSentenceTokenizer instance german_tokenizer = nltk.data.load(resource_url=&apos;tokenizers/punkt/german.pickle&apos;)german_sentences = german_tokenizer.tokenize(german_text)# verify the type of german_tokenizer# should be PunktSentenceTokenizerprint(type(german_tokenizer))# check if results of both tokenizers match# should be Trueprint(german_sentences_def == german_sentences)# print first 5 sentences of the corpusfor sent in german_sentences[0:5]: print(sent) 输出: &lt;class &apos;nltk.tokenize.punkt.PunktSentenceTokenizer&apos;&gt;True Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der gefürchtete &quot; Millenium-Bug &quot; nicht eingetreten .Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken . 1.1.2使用PunktSentenceTokenizer## using PunktSentenceTokenizer for sentence tokenizationpunkt_st = nltk.tokenize.PunktSentenceTokenizer()sample_sentences = punkt_st.tokenize(sample_text)pprint(sample_sentences) 输出: [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.1.3使用RegexpTokenizer#使用正则表达式做句子切分## using RegexpTokenizer for sentence tokenizationSENTENCE_TOKENS_PATTERN = r&apos;(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;![A-Z]\\.)(?&lt;=\\.|\\?|\\!)\\s&apos;regex_st = nltk.tokenize.RegexpTokenizer( pattern=SENTENCE_TOKENS_PATTERN, gaps=True)sample_sentences = regex_st.tokenize(sample_text)pprint(sample_sentences) 输出： [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos; There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.2词语切分1.2.1默认分词器nltk.word_tokenize## 分词sentence = &quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;# default word tokenizerdefault_wt = nltk.word_tokenizewords = default_wt(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.2Treebank分词器# treebank word tokenizertreebank_wt = nltk.TreebankWordTokenizer()words = treebank_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.3正则分词器RegexpTokenizer# 正则切分TOKEN_PATTERN = r&apos;\\w+&apos; regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 设置正则模式: GAP_PATTERN = r&apos;\\s+&apos; regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 分词索引word_indices = list(regex_wt.span_tokenize(sentence))print(word_indices)print([sentence[start:end] for start, end in word_indices]) 输出：[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)][&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.4WordPunctTokenizer分词器# derived regex tokenizers(派生类执行分词)wordpunkt_wt = nltk.WordPunctTokenizer()words = wordpunkt_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.5WhitespaceTokenizer分词器#WhitespaceTokenizer基于诸如缩进符、换行符及空格的空白字符将句子分割成单词whitespace_wt = nltk.WhitespaceTokenizer()words = whitespace_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 2.文本规范化文本规范化定义为这样的一一个过程，它包含一系列步骤， 依次是转换、清洗以及将文本数据标准化成可供NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常称为文本清洗或转换。 本节将讨论在文本规范化过程中使用的各种技术。在探索各种技术之前，请使用以下代码段来加载基本的依存关系以及将使用的语料库: import nltkimport reimport stringfrom pprint import pprintcorpus = [&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for $199&quot;, &quot;@@You&apos;ll (learn) a **lot** in the book. Python is an amazing language!@@&quot;] 2.2文本清洗可以使用nltk中的clean_html()函数，或者BeautifulSoup库来解析HTML数据，还可以使用自定义的逻辑，包括正则表达式、xpath和lxml库来解析XML数据。 2.3文本切分def tokenize_text(text): sentences = nltk.sent_tokenize(text) word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] return word_tokens token_list = [tokenize_text(text) for text in corpus]print(token_list) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;$&apos;, &apos;199&apos;]], [[&apos;@&apos;, &apos;@&apos;, &apos;You&apos;, &quot;&apos;ll&quot;, &apos;(&apos;, &apos;learn&apos;, &apos;)&apos;, &apos;a&apos;, &apos;**lot**&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;], [&apos;@&apos;, &apos;@&apos;]]] 2.4删除特殊字符在分词后删除特殊字符def remove_characters_after_tokenization(tokens): pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = [pattern.sub(&apos;&apos;, token) for token in tokens] return filtered_tokens filtered_list_1 = [[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens] for sentence_tokens in token_list]pprint(filtered_list_1) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &apos;nt&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &apos;nt&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &apos;s&apos;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;&apos;, &apos;199&apos;]], [[&apos;&apos;, &apos;&apos;, &apos;You&apos;, &apos;ll&apos;, &apos;&apos;, &apos;learn&apos;, &apos;&apos;, &apos;a&apos;, &apos;lot&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;&apos;], [&apos;&apos;, &apos;&apos;]]] 在分词前删除特殊字符def remove_characters_before_tokenization(sentence, keep_apostrophes=False): sentence = sentence.strip() if keep_apostrophes: PATTERN = r&apos;[?|$|&amp;|*|%|@|(|)|~]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) else: PATTERN = r&apos;[^a-zA-Z0-9 ]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) return filtered_sentence filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus] print(filtered_list_2)cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]print(cleaned_corpus) 输出： [&apos;The brown fox wasnt that quick and he couldnt win the race&apos;, &apos;Hey thats a great deal I just bought a phone for 199&apos;, &apos;Youll learn a lot in the book Python is an amazing language&apos;][&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for 199&quot;, &quot;You&apos;ll learn a lot in the book. Python is an amazing language!&quot;] 2.5扩展缩写词将is’nt 还原为is not等等… from contractions import contractions_dictdef expand_contractions(sentence, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match)\\ if contraction_mapping.get(match)\\ else contraction_mapping.get(match.lower()) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_sentence = contractions_pattern.sub(expand_match, sentence) return expanded_sentence expanded_corpus = [expand_contractions(sentence, contractions_dict) for sentence in cleaned_corpus] print(expanded_corpus) 输出： [&apos;The brown fox was not that quick and he could not win the race&apos;, &apos;Hey that is a great deal! I just bought a phone for 199&apos;, &apos;You will learn a lot in the book. Python is an amazing language!&apos;] 2.6大小写转换# case conversion print(corpus[0].lower())print(corpus[0].upper()) 输出：the brown fox wasn&apos;t that quick and he couldn&apos;t win the raceTHE BROWN FOX WASN&apos;T THAT QUICK AND HE COULDN&apos;T WIN THE RACE 2.7删除停用词# removing stopwordsdef remove_stopwords(tokens): stopword_list = nltk.corpus.stopwords.words(&apos;english&apos;) filtered_tokens = [token for token in tokens if token not in stopword_list] return filtered_tokens expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus] filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]print(filtered_list_3) 输出: [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;quick&apos;, &apos;could&apos;, &apos;win&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;bought&apos;, &apos;phone&apos;, &apos;199&apos;]], [[&apos;You&apos;, &apos;learn&apos;, &apos;lot&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;]]] 2.8词语校正删除重复的字符# removing repeated characterssample_sentence = &apos;My schooool is realllllyyy amaaazingggg&apos;sample_sentence_tokens = tokenize_text(sample_sentence)[0]from nltk.corpus import wordnetdef remove_repeated_characters(tokens): repeat_pattern = re.compile(r&apos;(\\w*)(\\w)\\2(\\w*)&apos;) match_substitution = r&apos;\\1\\2\\3&apos; def replace(old_word): if wordnet.synsets(old_word): return old_word new_word = repeat_pattern.sub(match_substitution, old_word) return replace(new_word) if new_word != old_word else new_word correct_tokens = [replace(word) for word in tokens] return correct_tokensprint(remove_repeated_characters(sample_sentence_tokens)) 输出:[&apos;My&apos;, &apos;school&apos;, &apos;is&apos;, &apos;really&apos;, &apos;amazing&apos;] 2.9词干提取2.9.1Port词干提取器# porter stemmerfrom nltk.stem import PorterStemmerps = PorterStemmer()print(ps.stem(&apos;jumping&apos;), ps.stem(&apos;jumps&apos;), ps.stem(&apos;jumped&apos;))print(ps.stem(&apos;lying&apos;))print(ps.stem(&apos;strange&apos;)) 输出： jump jump jumpliestrang 2.9.2LancasterStemmer词干提取器# lancaster stemmerfrom nltk.stem import LancasterStemmerls = LancasterStemmer()print(ls.stem(&apos;jumping&apos;), ls.stem(&apos;jumps&apos;), ls.stem(&apos;jumped&apos;))print (ls.stem(&apos;lying&apos;))print (ls.stem(&apos;strange&apos;)) 输出： jump jump jumplyingstrange 2.9.3RegexpStemmer正则词干提取器# regex stemmerfrom nltk.stem import RegexpStemmerrs = RegexpStemmer(&apos;ing$|s$|ed$&apos;, min=4)print( rs.stem(&apos;jumping&apos;), rs.stem(&apos;jumps&apos;), rs.stem(&apos;jumped&apos;))print (rs.stem(&apos;lying&apos;))print (rs.stem(&apos;strange&apos;)) 输出： jump jump jumplystrange 2.9.4SnowballStemmer词干提取器# snowball stemmerfrom nltk.stem import SnowballStemmerss = SnowballStemmer(&quot;german&quot;)print (&apos;Supported Languages:&apos;, SnowballStemmer.languages)# autobahnen -&gt; cars# autobahn -&gt; carss.stem(&apos;autobahnen&apos;)# springen -&gt; jumping# spring -&gt; jumpss.stem(&apos;springen&apos;) 输出： Supported Languages: (&apos;arabic&apos;, &apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;, &apos;hungarian&apos;, &apos;italian&apos;, &apos;norwegian&apos;, &apos;porter&apos;, &apos;portuguese&apos;, &apos;romanian&apos;, &apos;russian&apos;, &apos;spanish&apos;, &apos;swedish&apos;)Out[14]:&apos;spring&apos; 2.10词形还原# lemmatizationfrom nltk.stem import WordNetLemmatizerwnl = WordNetLemmatizer()# lemmatize nounsprint( wnl.lemmatize(&apos;cars&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;men&apos;, &apos;n&apos;))# lemmatize verbsprint (wnl.lemmatize(&apos;running&apos;, &apos;v&apos;))print (wnl.lemmatize(&apos;ate&apos;, &apos;v&apos;))# lemmatize adjectivesprint (wnl.lemmatize(&apos;saddest&apos;, &apos;a&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;a&apos;))# ineffective lemmatizationprint (wnl.lemmatize(&apos;ate&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;v&apos;)) 输出： carmenruneatsadfancyatefancier","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"}],"author":"CinKate"},{"title":"《活着》读后感","slug":"huozheduhougan","date":"2019-03-25T15:01:09.000Z","updated":"2020-05-17T16:11:59.457Z","comments":true,"path":"2019/03/25/huozheduhougan/","link":"","permalink":"http://renxingkai.github.io/2019/03/25/huozheduhougan/","excerpt":"","text":"之前在本科的时候只看过《活着》这部电影（葛优、巩俐主演），电影里已经很惨了，当时虽然看了把两遍电影，但是最大的感受是：虽然这部电影名为“活着”，可却不停地有人离开，也对福贵的悲痛人生感到痛惜，看着自己的亲人，一个个，一个个离开自己，世上也孤零零仅剩自己一个人，那种悲伤之情真的难以承受。 最近，在微信读书上看原著，可能由于年龄的增加，经历的事多了，以及作者的绝妙文笔，感觉读起来的画面感，不亚于看一部精彩的电影，甚至了超过了表演形式。 原著中，福贵的生平更惨，电影中他的孙子还能陪他一起在世上，原著中却真的只有他一个人走到了最后，亲手埋下了自己的儿子、女儿、妻子、女婿、孙子……从最初的悲恸不已，到后来的“心情平淡”，或许生活的残酷已经将这个男人摧残的遍体鳞伤，但他仍然坚强地活着，攒了两年钱却买了一头已然年暮的老牛，因为他知道活着的不易，他珍惜活着，他珍惜每一天。 可惜生活能留给他的，也仅剩了活着。“少年去游荡，中年想掘藏，老年做和尚。”经历了最惨痛的事情，才能看淡生活吧。 真的希望，活着的人能好好活着，每天清晨迎接初日，傍晚送走晚霞，日复一日，如此安好~ 二零一九年三月二十五日 于岳麓山下","categories":[{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"}],"tags":[{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"}],"author":"CinKate"},{"title":"Keras踩坑总结","slug":"kerestricks","date":"2019-03-23T17:04:49.000Z","updated":"2020-05-17T16:11:59.830Z","comments":true,"path":"2019/03/24/kerestricks/","link":"","permalink":"http://renxingkai.github.io/2019/03/24/kerestricks/","excerpt":"","text":"转载自：链接 Keras 是一个用 Python 编写的高级神经网络 API，它能够以 TensorFlow, CNTK, 或者 Theano 作为后端运行。Keras 的开发重点是支持快速的实验。能够以最小的时间把你的想法转换为实验结果，是做好研究的关键。本人是keras的忠实粉丝，可能是因为它实在是太简单易用了，不用多少代码就可以将自己的想法完全实现，但是在使用的过程中还是遇到了不少坑，本文做了一个归纳，供大家参考。 Keras 兼容的 Python 版本: Python 2.7-3.6。 详细教程请参阅Keras官方中文文档 1、Keras输出的loss，val这些值如何保存到文本中去：Keras中的fit函数会返回一个History对象，它的History.history属性会把之前的那些值全保存在里面，如果有验证集的话，也包含了验证集的这些指标变化情况，具体写法： hist=model.fit(train_set_x,train_set_y,batch_size=256,shuffle=True,nb_epoch=nb_epoch,validation_split=0.1)with open(&apos;log_sgd_big_32.txt&apos;,&apos;w&apos;) as f: f.write(str(hist.history)) 我觉得保存之前的loss，val这些值还是比较重要的，在之后的调参过程中有时候还是需要之前loss的结果作为参考的，特别是你自己添加了一些自己的loss的情况下，但是这样的写法会使整个文本的取名比较乱，所以其实可以考虑使用Aetros的插件，Aetros网址，这是一个基于Keras的一个管理工具，可以可视化你的网络结构，中间卷积结果的可视化，以及保存你以往跑的所有结果，还是很方便的，就是有些不稳定，有时候会崩。。。 history对象包含两个重要属性：epoch：训练的轮数history：它是一个字典，包含val_loss,val_acc,loss,acc四个key。 2、关于训练集，验证集和测试集：其实一开始我也没搞清楚这个问题，拿着测试集当验证集用，其实验证集是从训练集中抽取出来用于调参的，而测试集是和训练集无交集的，用于测试所选参数用于该模型的效果的，这个还是不要弄错了。。。在Keras中，验证集的划分只要在fit函数里设置validation_split的值就好了，这个对应了取训练集中百分之几的数据出来当做验证集。但由于shuffle是在validation _split之后执行的，所以如果一开始训练集没有shuffle的话，有可能使验证集全是负样本。测试集的使用只要在evaluate函数里设置就好了。 print model.evaluate（test_set_x，test_set_y ,batch_size=256） 这里注意evaluate和fit函数的默认batch_size都是32，自己记得修改。 总结： 验证集是在fit的时候通过validation_split参数自己从训练集中划分出来的； 测试集需要专门的使用evaluate去进行评价。 3、关于优化方法使用的问题之学习率调整开始总会纠结哪个优化方法好用，但是最好的办法就是试，无数次尝试后不难发现，Sgd的这种学习率非自适应的优化方法，调整学习率和初始化的方法会使它的结果有很大不同，但是由于收敛确实不快，总感觉不是很方便，我觉得之前一直使用Sgd的原因一方面是因为优化方法不多，其次是用Sgd都能有这么好的结果，说明你网络该有多好啊。其他的Adam，Adade，RMSprop结果都差不多，Nadam因为是adam的动量添加的版本，在收敛效果上会更出色。所以如果对结果不满意的话，就把这些方法换着来一遍吧。 （1）方法一：通过LearningRateScheduler实现学习率调整有很多初学者人会好奇怎么使sgd的学习率动态的变化，其实Keras里有个反馈函数叫LearningRateScheduler，具体使用如下： #使学习率指数下降def step_decay(epoch): initial_lrate = 0.01 drop = 0.5 epochs_drop = 10.0 lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) return lratelrate = LearningRateScheduler(step_decay)sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)model.fit(train_set_x, train_set_y, validation_split=0.1, nb_epoch=200, batch_size=256, callbacks=[lrate]) （2）方式二：最直接的调整学习率方式当然也可以直接在sgd声明函数中修改参数来直接修改学习率，学习率变化如下图： sgd = SGD(lr=learning_rate, decay=learning_rate/nb_epoch, momentum=0.9, nesterov=True) 具体可以参考这篇文章Using Learning Rate Schedules for Deep Learning Models in Python with Keras 除此之外，还有一种学利率调整方式，即 （3）方法三：通过ReduceLROnPlateau调整学习率keras.callbacks.ReduceLROnPlateau(monitor=&apos;val_loss&apos;, factor=0.1, patience=10, verbose=0, mode=&apos;auto&apos;, epsilon=0.0001, cooldown=0, min_lr=0) 当评价指标不在提升时，减少学习率。当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在patience个epoch中看不到模型性能提升，则减少学习率 参数 monitor：被监测的量factor：每次减少学习率的因子，学习率将以lr = lr*factor的形式被减少patience：当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值触发学习率减少。在max模式下，当检测值不再上升则触发学习率减少。epsilon：阈值，用来确定是否进入检测值的“平原区”cooldown：学习率减少后，会经过cooldown个epoch才重新进行正常操作min_lr：学习率的下限 代码示例如下： from keras.callbacks import ReduceLROnPlateaureduce_lr = ReduceLROnPlateau(monitor=&apos;val_loss&apos;, patience=10, mode=&apos;auto&apos;)model.fit(train_x, train_y, batch_size=32, epochs=5, validation_split=0.1, callbacks=[reduce_lr]) 4、如何用 Keras 处理超过内存的数据集？你可以使用 model.train_on_batch(x，y) 和 model.test_on_batch(x，y) 进行批量训练与测试。请参阅 模型文档。 或者，你可以编写一个生成批处理训练数据的生成器，然后使用 model.fit_generator(data_generator，steps_per_epoch，epochs) 方法。 5、Batchnormalization层的放置问题：BN层是真的吊，简直神器，除了会使网络搭建的时间和每个epoch的时间延长一点之外，但是关于这个问题我看到了无数的说法，对于卷积和池化层的放法，又说放中间的，也有说池化层后面的，对于dropout层，有说放在它后面的，也有说放在它前面的，对于这个问题我的说法还是试！虽然麻烦。。。但是DL本来不就是一个偏工程性的学科吗。。。还有一点是需要注意的，就是BN层的参数问题，我一开始也没有注意到，仔细看BN层的参数： keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, axis=-1, momentum=0.9, weights=None, beta_init=&apos;zero&apos;, gamma_init=&apos;one&apos;) 参数mode：整数，指定规范化的模式，取0或10：按特征规范化，输入的各个特征图将独立被规范化。规范化的轴由参数axis指定。注意，如果输入是形如（samples，channels，rows，cols）的4D图像张量，则应设置规范化的轴为1，即沿着通道轴规范化。输入格式是‘tf’同理。1：按样本规范化，该模式默认输入为2D 我们大都使用的都是mode=0也就是按特征规范化，对于放置在卷积和池化之间或之后的4D张量，需要设置axis=1，而Dense层之后的BN层则直接使用默认值就好了。 6、在验证集的误差不再下降时，如何中断训练？你可以使用 EarlyStopping 回调： from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor=&apos;val_loss&apos;, patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) 总结：关于callbacks参数的妙用 （1）查询每隔epoch之后的loss和acc （2）通过LearningRateScheduler实现衰减学习率或自定义衰减学习率 （3）通过EarlyStopping实现中断训练 （4）我们还可以自己定义回调函数，所为回调函数其实就是在训练完每一个epoch之后我们希望实现的操作。 7.如何「冻结」网络层？「冻结」一个层意味着将其排除在训练之外，即其权重将永远不会更新。这在微调模型或使用固定的词向量进行文本输入中很有用。有两种方式实现： 方式一：在构造层的时候传递一个bool类型trainable参数，如下： frozen_layer = Dense(32, trainable=False) 您可以将 trainable 参数（布尔值）传递给一个层的构造器，以将该层设置为不可训练的： 方式二：通过层对象的trainable属性去设置，如下： x = Input(shape=(32,))layer = Dense(32) #构造一个层layer.trainable = False #设置层的trainable属性y = layer(x) 注意：可以在实例化之后将网络层的 trainable 属性设置为 True 或 False。为了使之生效，在修改 trainable 属性之后，需要在模型上调用 compile()。及重新编译模型。 8.如何从 Sequential 模型中移除一个层？你可以通过调用模型的 .pop() 来删除 Sequential 模型中最后添加的层： model = Sequential()model.add(Dense(32, activation=&apos;relu&apos;, input_dim=784))model.add(Dense(32, activation=&apos;relu&apos;))print(len(model.layers)) # &quot;2&quot;model.pop()print(len(model.layers)) # &quot;1&quot;","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"}],"author":"CinKate"},{"title":"BIDAF代码阅读","slug":"bidaf","date":"2019-03-21T11:31:10.000Z","updated":"2020-05-17T16:11:59.217Z","comments":true,"path":"2019/03/21/bidaf/","link":"","permalink":"http://renxingkai.github.io/2019/03/21/bidaf/","excerpt":"","text":"1.assert的用法 ： 主要用于检查条件，不符合就终止程序a=-1#报错assert a&gt;0,&quot;a超出范围&quot;#正常运行assert a&lt;0 2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicodeimport codecs, sys# 用codecs提供的open方法来指定打开的文件的语言编码，它会在读 取的时候自动转换为内部unicode bfile = codecs.open( &quot; dddd.txt &quot; , &apos; r &apos; , &quot; big5 &quot; )# bfile = open(&quot;dddd.txt&quot;, &apos;r&apos;) ss = bfile.read()bfile.close()# 输出，这个时候看到的就是转换后的结果。如果使用语言内建的open函数 来打开文件，这里看到的必定是乱码 以下是prepro.py文件的代码阅读与分析：import spacyimport jsonfrom tqdm import tqdmfrom collections import Counterimport randomimport codecsimport numpy as npimport osimport tensorflow as tf#加载模型nlp=spacy.blank(&apos;en&apos;)#对句子进行分词def word_tokenize(sent): doc=nlp(sent) return [token.text for token in doc]#常用的word2idx#此处输出spans形式：[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)]#意为取出该词当前所在的位置，并且结束长度+当前长度#两者之差即为该单词长度def convert_idx(text,tokens): current=0 spans=[] for token in tokens: current=text.find(token,current) if current&lt;0: print(&apos;Token &#123;&#125; cannot be found!&apos;.format(token)) raise Exception() #[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)] #取出该词当前所在的位置，并且结束长度+当前长度 #两者之差即为该单词长度 spans.append((current,current+len(token))) current+=len(token) return spans#预处理文件def process_file(filename,data_type=None,word_counter=None,char_counter=None): print(&quot;Generating &#123;&#125; examples...&quot;.format(data_type)) examples = [] eval_examples = &#123;&#125; total=0 with open(filename,&apos;r&apos;) as fh: source=json.load(fh) print(len(source[&apos;data&apos;])) #遍历每篇文章dev有48篇文章 for article in tqdm(source[&quot;data&quot;]): #遍历每篇文章的段落 for para in article[&apos;paragraphs&apos;]: #替换段落中的&apos;&apos;和`` context = para[&quot;context&quot;].replace(&quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #并对段落进行分词,分词中还是带了标点和特殊符号，需要后面进行处理 context_tokens=word_tokenize(context) #[&apos;The&apos;, &apos;connection&apos;, &apos;between&apos;, &apos;macroscopic&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;and&apos;, &apos;microscopic&apos;, &apos;conservative&apos;, &apos;forces&apos;, &apos;is&apos;, &apos;described&apos;, &apos;by&apos;, &apos;detailed&apos;, &apos;treatment&apos;, &apos;with&apos;, &apos;statistical&apos;, &apos;mechanics&apos;, &apos;.&apos;, &apos;In&apos;, &apos;macroscopic&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;act&apos;, &apos;to&apos;, &apos;change&apos;, &apos;the&apos;, &apos;internal&apos;, &apos;energies&apos;, &apos;of&apos;, &apos;the&apos;, &apos;system&apos;, &apos;,&apos;, &apos;and&apos;, &apos;are&apos;, &apos;often&apos;, &apos;associated&apos;, &apos;with&apos;, &apos;the&apos;, &apos;transfer&apos;, &apos;of&apos;, &apos;heat&apos;, &apos;.&apos;, &apos;According&apos;, &apos;to&apos;, &apos;the&apos;, &apos;Second&apos;, &apos;law&apos;, &apos;of&apos;, &apos;thermodynamics&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;necessarily&apos;, &apos;result&apos;, &apos;in&apos;, &apos;energy&apos;, &apos;transformations&apos;, &apos;within&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;from&apos;, &apos;ordered&apos;, &apos;to&apos;, &apos;more&apos;, &apos;random&apos;, &apos;conditions&apos;, &apos;as&apos;, &apos;entropy&apos;, &apos;increases&apos;, &apos;.&apos;] #获取每个单词的字符表示 context_chars = [list(token) for token in context_tokens] #word2idx 每个词开始的位置和结束的位置 spans = convert_idx(context, context_tokens) for token in context_tokens: #这儿加的是每个qas的长度？？ word_counter[token] += len(para[&quot;qas&quot;]) for char in token: #每个单词的字符这儿也加的是每个qas的长度 #Counter(&#123;&apos;e&apos;: 28293, &apos;a&apos;: 19610, &apos;n&apos;: 17317, &apos;t&apos;: 17071, &apos;r&apos;: 15443, &apos;o&apos;: 15358, &apos;i&apos;: # 14669, &apos;s&apos;: 14081, &apos;h&apos;: 11839, &apos;l&apos;: 9031, &apos;d&apos;: 8982, &apos;c&apos;: 6540, &apos;u&apos;: 5885, # &apos;w&apos;: 5806, &apos;f&apos;: 4516, &apos;g&apos;: 4463, &apos;p&apos;: 4372, &apos;m&apos;: 4165, &apos;,&apos;: 3116, &apos;y&apos;: 2842, &apos;b&apos;: 2321, &apos;v&apos;: 2152, # &apos;.&apos;: 2057, &apos;B&apos;: 2052, &apos;S&apos;: 1832, &apos;1&apos;: 1776, &apos;k&apos;: 1553, &apos;0&apos;: 1168, &apos;C&apos;: 1107, &apos;F&apos;: 963, &apos;T&apos;: 876, # &apos;2&apos;: 856, &apos;P&apos;: 836, &apos;I&apos;: 819, &apos;5&apos;: 798, &apos;N&apos;: 766, &apos;L&apos;: 741, &apos;X&apos;: 714, &apos;M&apos;: 672, &apos;4&apos;: 662, &apos;3&apos;: 636, # &apos;A&apos;: 619, &apos;9&apos;: 584, &quot;&apos;&quot;: 552, &apos;-&apos;: 523, &apos;7&apos;: 488, &apos;D&apos;: 470, &apos;–&apos;: 415, &apos;(&apos;: 412, &apos;)&apos;: 412, &apos;8&apos;: 380, # &apos;6&apos;: 371, &apos;V&apos;: 352, &apos;O&apos;: 272, &apos;J&apos;: 268, &apos;j&apos;: 249, &apos;q&apos;: 235, &apos;&quot;&apos;: 222, &apos;G&apos;: 221, &apos;x&apos;: 220, &apos;E&apos;: 177, # &apos;R&apos;: 173, &apos;W&apos;: 168, &apos;K&apos;: 159, &apos;H&apos;: 117, &apos;U&apos;: 108, &apos;z&apos;: 107, &apos;½&apos;: 81, &apos;:&apos;: 81, &apos;;&apos;: 63, &apos;$&apos;: 49, &apos;#&apos;: 30, # &apos;é&apos;: 26, &apos;/&apos;: 21, &apos;Q&apos;: 15&#125;) char_counter[char] += len(para[&quot;qas&quot;]) #遍历qas for qa in para[&quot;qas&quot;]: total += 1 #替换问题&apos;&apos; `` ques = qa[&quot;question&quot;].replace( &quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #对问题进行分词 ques_tokens = word_tokenize(ques) #取出问题中的字符 ques_chars = [list(token) for token in ques_tokens] #遍历问题每个词 for token in ques_tokens: #此处真的正确 word_counter[token] += 1 for char in token: char_counter[char] += 1 y1s, y2s = [], [] answer_texts = [] #遍历答案文本 for answer in qa[&quot;answers&quot;]: #答案文本 answer_text = answer[&quot;text&quot;] #开始位置 answer_start = answer[&apos;answer_start&apos;] answer_end = answer_start + len(answer_text) answer_texts.append(answer_text) answer_span = [] #加入答案span answer_span for idx, span in enumerate(spans): if not (answer_end &lt;= span[0] or answer_start &gt;= span[1]): answer_span.append(idx) y1, y2 = answer_span[0], answer_span[-1] y1s.append(y1) y2s.append(y2) example = &#123;&quot;context_tokens&quot;: context_tokens, &quot;context_chars&quot;: context_chars, &quot;ques_tokens&quot;: ques_tokens, &quot;ques_chars&quot;: ques_chars, &quot;y1s&quot;: y1s, &quot;y2s&quot;: y2s, &quot;id&quot;: total&#125; examples.append(example) #未分词结果 eval_examples[str(total)] = &#123; &quot;context&quot;: context, &quot;spans&quot;: spans, &quot;answers&quot;: answer_texts, &quot;uuid&quot;: qa[&quot;id&quot;]&#125; random.shuffle(examples) print(&quot;&#123;&#125; questions in total&quot;.format(len(examples))) return examples, eval_examples#获取词向量def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None): print(&quot;Generating &#123;&#125; embedding...&quot;.format(data_type)) embedding_dict=&#123;&#125; #过滤掉低频词，仅取出频率较高的词 filtered_elements=[k for k,v in counter.items() if v&gt;limit] #判断词向量文件是否为空 if emb_file is not None: assert size is not None#如果size为空直接退出程序 assert vec_size is not None#如果vec_size为空直接退出程序 #读取词向量 with codecs.open(emb_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as fh: # 依次遍历词向量每一行 for line in tqdm(fh, total=size): #分开词和向量 array = line.split() #取出开头的单词 word=&quot;&quot;.join(array[0:-vec_size]) #取出单词对应的词向量 vector=list(map(float,array[-vec_size:])) #词向量的单词在counter单词中，并且 在文本中的单词数目&gt;limit if word in counter and counter[word]&gt;limit: embedding_dict[word]=vector print(&quot;&#123;&#125; / &#123;&#125; tokens have corresponding &#123;&#125; embedding vector&quot;.format( len(embedding_dict), len(filtered_elements), data_type)) #如果词向量文件为空 else: assert vec_size is not None #遍历所有过滤的词 for token in filtered_elements: #对每个单词进行随机初始化向量 embedding_dict[token]=[np.random.normal(scale=0.01) for _ in range(vec_size)] print(&quot;&#123;&#125; tokens have corresponding embedding vector&quot;.format( len(filtered_elements))) #处理OOV词 NULL = &quot;--NULL--&quot; OOV = &quot;--OOV--&quot; #从下标2索引开始，过滤掉NULL和OOV 创建token2_idx_dict token2idx_dict=&#123;token:idx for idx,token in enumerate(embedding_dict.keys(),2)&#125; if token2idx_dict is None else token2idx_dict #NULL OOV 设置token2idx token2idx_dict[NULL] = 0 token2idx_dict[OOV] = 1 #NULL OOV设置embedding_dict embedding_dict[NULL] = [0. for _ in range(vec_size)] embedding_dict[OOV] = [0. for _ in range(vec_size)] #id2embedding 单词id对应的词向量 id2emb_dict=&#123;idx:embedding_dict[token] for token,idx in token2idx_dict.items() &#125; #获取词向量矩阵 emb_mat=[id2emb_dict[idx] for idx in range(id2emb_dict)] #仅返回 词向量矩阵，token2idx_dict return emb_mat, token2idx_dict#构建文本特征question paragraph answer and so ondef build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False): #文章长度 para_limit=config.test_para_limit if is_test else config.para_limit #问题长度 ques_limit = config.test_ques_limit if is_test else config.ques_limit #字符限制长度 char_limit = config.char_limit #过滤文章和问题长度函数 def filter_func(example, is_test=False): return len(example[&quot;context_tokens&quot;]) &gt; para_limit or len(example[&quot;ques_tokens&quot;]) &gt; ques_limit print(&quot;Processing &#123;&#125; examples...&quot;.format(data_type)) writer = tf.python_io.TFRecordWriter(out_file) total = 0 total_ = 0 meta = &#123;&#125; #处理文章 for example in tqdm(examples): total_+=1 #过滤长度大于限制值的文章 if filter_func(example, is_test): continue total += 1 #段落ids context_idxs = np.zeros([para_limit], dtype=np.int32) #段落id char对应的矩阵 context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32) ##问题ids ques_idxs = np.zeros([ques_limit], dtype=np.int32) ##问题id char对应的矩阵 ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32) #段落长度 y1 = np.zeros([para_limit], dtype=np.float32) y2 = np.zeros([para_limit], dtype=np.float32) #获取单词 def _get_word(word): for each in (word, word.lower(), word.capitalize(), word.upper()): if each in word2idx_dict: #返回每个单词对应的id return word2idx_dict[each] return 1 #获取字符 def _get_char(char): if char in char2idx_dict: # 返回每个字符对应的id return char2idx_dict[char] return 1 #为每个文章内容获取对应的ids context_tokens为已经分好词的文章 for i, token in enumerate(example[&quot;context_tokens&quot;]): context_idxs[i] = _get_word(token) # 为每个问题内容获取对应的ids ques_tokens为已经分好词的问题 for i, token in enumerate(example[&quot;ques_tokens&quot;]): ques_idxs[i] = _get_word(token) # 为每个文章内容获取对应的chars for i, token in enumerate(example[&quot;context_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break #赋值char 不够的用0填充 context_char_idxs[i, j] = _get_char(char) # 为每个问题内容获取对应的chars for i, token in enumerate(example[&quot;ques_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break # 赋值char 不够的用0填充 ques_char_idxs[i, j] = _get_char(char) #开始，结束位置 start, end = example[&quot;y1s&quot;][-1], example[&quot;y2s&quot;][-1] y1[start], y2[end] = 1.0, 1.0 #构建tensorflow 记录 record = tf.train.Example(features=tf.train.Features(feature=&#123; &quot;context_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])), &quot;ques_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])), &quot;context_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])), &quot;ques_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])), &quot;y1&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])), &quot;y2&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])), &quot;id&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[example[&quot;id&quot;]])) &#125;)) writer.write(record.SerializeToString()) print(&quot;Build &#123;&#125; / &#123;&#125; instances of features in total&quot;.format(total, total_)) meta[&quot;total&quot;] = total writer.close() return meta#保存文件def save(filename, obj, message=None): if message is not None: print(&quot;Saving &#123;&#125;...&quot;.format(message)) with open(filename, &quot;w&quot;) as fh: json.dump(obj, fh)# 预处理文件def prepro(config): #单词，字符计数器 word_counter, char_counter = Counter(), Counter() #处理训练集 train_examples, train_eval = process_file( config.train_file, &quot;train&quot;, word_counter, char_counter) #处理验证集 dev_examples, dev_eval = process_file( config.dev_file, &quot;dev&quot;, word_counter, char_counter) #处理测试集 test_examples, test_eval = process_file( config.test_file, &quot;test&quot;, word_counter, char_counter) #词向量文件 word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file #字符向量文件 char_emb_file = config.glove_char_file if config.pretrained_char else None #字符向量大小 char_emb_size = config.glove_char_size if config.pretrained_char else None #字符向量维度 char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim #word2idx字典 word2idx_dict = None #如果存在word2idx字典 则直接导入 if os.path.isfile(config.word2idx_file): with open(config.word2idx_file, &quot;r&quot;) as fh: word2idx_dict = json.load(fh) #构建词向量矩阵 word_emb_mat, word2idx_dict = get_embedding(word_counter, &quot;word&quot;, emb_file=word_emb_file, size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict) #构建字符向量矩阵 char2idx_dict = None # 如果存在char2idx字典 则直接导入 if os.path.isfile(config.char2idx_file): with open(config.char2idx_file, &quot;r&quot;) as fh: char2idx_dict = json.load(fh) # 构建字符向量矩阵 char_emb_mat, char2idx_dict = get_embedding( char_counter, &quot;char&quot;, emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim, token2idx_dict=char2idx_dict) #对训练集、验证集、测试集构建特征 build_features(config, train_examples, &quot;train&quot;, config.train_record_file, word2idx_dict, char2idx_dict) dev_meta = build_features(config, dev_examples, &quot;dev&quot;, config.dev_record_file, word2idx_dict, char2idx_dict) test_meta = build_features(config, test_examples, &quot;test&quot;, config.test_record_file, word2idx_dict, char2idx_dict, is_test=True) #对预处理的文件进行保存 save(config.word_emb_file, word_emb_mat, message=&quot;word embedding&quot;) save(config.char_emb_file, char_emb_mat, message=&quot;char embedding&quot;) save(config.train_eval_file, train_eval, message=&quot;train eval&quot;) save(config.dev_eval_file, dev_eval, message=&quot;dev eval&quot;) save(config.test_eval_file, test_eval, message=&quot;test eval&quot;) save(config.dev_meta, dev_meta, message=&quot;dev meta&quot;) save(config.word2idx_file, word2idx_dict, message=&quot;word2idx&quot;) save(config.char2idx_file, char2idx_dict, message=&quot;char2idx&quot;) save(config.test_meta, test_meta, message=&quot;test meta&quot;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"神经网络调参的一些tips","slug":"nntuningparameter","date":"2019-03-19T20:10:50.000Z","updated":"2020-05-17T16:12:00.003Z","comments":true,"path":"2019/03/20/nntuningparameter/","link":"","permalink":"http://renxingkai.github.io/2019/03/20/nntuningparameter/","excerpt":"","text":"参考建议调整超参数思想为控制变量法，并且按照学习率、批处理大小和隐藏层设计的顺序进行。以下是一些建议： 使用一个规模比较小的数据集一般都会把数据集分为训练集、测试集、验证集三份，训练集和验证集也被称为开发数据集，有的数据集不设验证集，是因为数据量小，通常可以用训练集调整超参数。 如果有验证集，验证集的数据不宜过多，因为数据越多，越需要多次迭代才能看到超参数的效果，所需要的时间就越长，在寻找一组最佳参数阶段，需要比较不同参数下损失变化的曲线和精度的值。 调整学习率控制其他超参数不变，改变学习率，比如从0.0001开始，顺序选择0.001、0.01、0.005、0.1和0.5，然后比较在不同的学习率下损失函数的曲线增长或减小的幅度，我们可以找到一个区间，也就是在这个区间内，损失函数的波形是稳定下降的，不会发生振荡，则取这个区间最小的值就可以。 调整batch_size控制其他超参数不变，改变批处理大小，可以依次选择20、50、100、200，然后比较在不同的批处理大小下能使准确率变化最陡的值，准确率变化越陡，证明参数学习收敛越快。 调整隐藏层设计控制其他超参数不变，改变隐藏层的层数或每层神经元的多少，选择能获得最高准确率的值。 超参数的调整主要还是需要自己做大量的实验，得出较好的“经验”，这样调起来会更得心应手一些~","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"}],"author":"CinKate"},{"title":"第一篇博文","slug":"firstpage","date":"2019-03-19T14:30:40.000Z","updated":"2020-05-17T16:11:59.814Z","comments":true,"path":"2019/03/19/firstpage/","link":"","permalink":"http://renxingkai.github.io/2019/03/19/firstpage/","excerpt":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~","text":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~ 分享一首自己很喜欢的词： 扬州慢·淮左名都宋代：姜夔淳熙丙申至日，予过维扬。夜雪初霁，荠麦弥望。入其城，则四顾萧条，寒水自碧，暮色渐起，戍角悲吟。予怀怆然，感慨今昔，因自度此曲。千岩老人以为有“黍离”之悲也。 淮左名都，竹西佳处，解鞍少驻初程。过春风十里。尽荠麦青青。自胡马窥江去后，废池乔木，犹厌言兵。渐黄昏，清角吹寒。都在空城。杜郎俊赏，算而今、重到须惊。纵豆蔻词工，青楼梦好，难赋深情。二十四桥仍在，波心荡、冷月无声。念桥边红药，年年知为谁生。 接下来的日子，锻炼自己的耐力，Always learn from the dalao~坐看天边云卷云舒~","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}],"author":"CinKate"}],"categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"},{"name":"比赛","slug":"比赛","permalink":"http://renxingkai.github.io/categories/比赛/"},{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"},{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"},{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"},{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"},{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"},{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"},{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"},{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"},{"name":"法研杯2021MRC","slug":"法研杯2021MRC","permalink":"http://renxingkai.github.io/tags/法研杯2021MRC/"},{"name":"MRC NQ","slug":"MRC-NQ","permalink":"http://renxingkai.github.io/tags/MRC-NQ/"},{"name":"MTL MRC","slug":"MTL-MRC","permalink":"http://renxingkai.github.io/tags/MTL-MRC/"},{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"},{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"},{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"},{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"},{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"},{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"},{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"},{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"},{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"},{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"},{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}]}