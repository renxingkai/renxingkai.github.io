{"meta":{"title":"CinKate's Blogs","subtitle":"长笛一声人倚楼~","description":"长笛一声人倚楼~","author":"CinKate","url":"http://renxingkai.github.io","root":"/"},"pages":[{"title":"archives","date":"2019-03-19T17:20:15.000Z","updated":"2020-05-17T16:11:58.945Z","comments":true,"path":"archives/index.html","permalink":"http://renxingkai.github.io/archives/index.html","excerpt":"","text":""},{"title":"","date":"2021-11-30T16:05:48.471Z","updated":"2021-11-30T16:05:48.471Z","comments":true,"path":"about/index.html","permalink":"http://renxingkai.github.io/about/index.html","excerpt":"","text":"个人信息姓名： 任星凯 性别： 男 出生年月： 1996.02 邮箱： renxingkai0101@163.com QQ： 179049243 教育经历 2018.09–2021.06 中南大学，计算机学院，硕士 2019.07–2021.01 国防科技大学，电子科学学院，联合培养 2014.09–2018.06 中北大学，物联网工程，学士 工作经历 2021.07–至今 快手 搬砖工程师 实习经历 2020.06-2020.07 贝壳找房 NLP算法工程师 2020.07-2020.09 联想研究院 AI Lab NLP算法工程师 比赛获奖 时间 比赛 结果 2021.11 2021年法研杯法律阅读理解竞赛 9th 2021.10 2021科大讯飞算法挑战赛-非标准化疾病诉求的简单分诊挑战赛 1st 2021.08 Kaggle CommonLit Readability Prize 26th/3633银牌 2021.08 SoDic基于文本挖掘的企业隐患排查质量分析模型 1st/636 2021.07 CCKS 2021：面向中文医疗科普知识的内容理解（一）医疗科普知识阅读理解 6th 2021.06 天池-全球人工智能技术创新大赛-赛道一:医学影像报告异常检测 5th/4337 2021.06 天池-全球人工智能技术创新大赛-赛道三: 小布助手对话短文本语义匹配 3rd/5345 2021.05 2021海华AI挑战赛·中文阅读理解·技术组 6th/141 2021.01 2020年CCF BDCI 房产行业聊天问答匹配竞赛 3rd/2985 2020.09 2020年法研杯法律阅读理解竞赛 1st 2020.09 2020 年 CCL 小牛杯幽默情绪识别竞赛 2nd 2020.09 2020 年 CCKS 新冠知识图谱构建与问答评测 2nd 2020.09 2020 年百度人工智能开源大赛 10th/826 2020.08 2020 ICDM Knowledge Graph Contest : Specification 10th 2020.04 中国人工智能大赛·语言与知识技术竞赛（个人赛) 7th/738 2020.02 Kaggle Google QUEST Q&amp;A Labeling 5th/1571 金牌 2020.01 Kaggle TensorFlow 2.0 Question Answering 53rd/1233 银牌 2019.11 汽车论坛消费者用车体验内容的判别与标注 竞赛 5th/837 2019.10 莱斯杯军事阅读理解竞赛 16th/625 2019.10 CCF技术需求匹配竞赛 23rd/862 2019.05 2019年法研杯法律阅读理解竞赛 4th/148 2017.05 蓝桥杯程序设计竞赛(Java) 国家二等奖 2017.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2016.11 华北五省计算机应用大赛 国家二等奖 2016.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2015.09 全国大学生英语竞赛 三等奖 奖学金 时间 奖项 2020.09 中南大学研究生学业一等奖学金 2019.09 中南大学研究生学业二等奖学金 2018.09 中南大学研究生学业一等奖学金 2017.09 本科生国家奖学金 2017.01 中北大学综合素质一等奖学金 论文 Xingkai Ren, Ronghua Shi, Fangfang Li. Distill BERT to Traditional Models in Chinese Machine Reading Comprehension. AAAI Workshop, 2020 基于多任务联合训练的法律文本机器阅读理解模型.中文信息学报 专利 一种基于多任务联合训练的机器阅读理解模型的使用方法(已授权)"},{"title":"categories","date":"2019-03-19T18:33:09.000Z","updated":"2020-05-17T16:11:59.906Z","comments":true,"path":"categories/index.html","permalink":"http://renxingkai.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-19T17:12:23.000Z","updated":"2020-05-17T16:11:59.002Z","comments":true,"path":"tags/index.html","permalink":"http://renxingkai.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"推荐系统排序模型-从LR到XXXX","slug":"deeprec","date":"2022-07-13T22:46:37.000Z","updated":"2022-07-13T14:48:09.573Z","comments":true,"path":"2022/07/14/deeprec/","link":"","permalink":"http://renxingkai.github.io/2022/07/14/deeprec/","excerpt":"","text":"平时会用到不少的排序模型，但是一直没有系统化总结，今天还是认真总结下，如有错误，求大佬们不吝指出。 1. LR-逻辑回归逻辑回归通常对输入特征如用户年龄、性别、item属性、描述等进行变换，然后输入到模型中，通常训练目标为是否点击。在推理阶段，将同样的特征输入到模型中，模型预测出点击概率，最终经过排序得到推荐item的列表。 逻辑回归核心为sigmoid函数，wx输入到sigmoid函数中，通常使用梯度下降算法来更新参数w。 逻辑回归的优点： 强数学含义支撑。LR属于广义线性模型的一种。 可解释性强。从公式层面来看，LR数学形式就是不同特征之间的加权之和，最终过一个sigmoid函数，将输出值限制在0到1之间。根据不同特征的权重，可以明显观察到哪些特征重要。 工程实现容易。 逻辑回归的缺点： 表示能力不足。 无法进行特征交叉，学习高阶特征。 2.POLY2-特征交叉的开始LR存在无法自动进行特征交叉的问题，最容易想到的是人工构造交叉组合特征。公式如下： $ POLY2(w,x)=\\sum_{j_1=i}^{n-1} \\sum_{j_2=j_1+1}^{n} w_{h(j_1,j_2)}x_{j_1}x_{j_2}$ 可以看到，该方法对所有特征均进行交叉，并对特征组合赋予权重$ w_{h(j_1,j_2)} $，一定程度解决了特征交叉问题，但是本质上还是对于不同特征加权求和的线性模型。 POLY2缺点： 交叉特征容易出现极度稀疏问题。使用one-hot编码类别特征之后，容易出现稀疏特征问题。 权重参数由n上升到n2，增加训练复杂度。 3.FM-隐向量特征交叉FM的主要优点是解决稀疏数据下的特征组合问题。原始的FM公式为： $ FM=w_0+\\sum_{i=1}^n w_ix_i+\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}x_ix_j$ 前两项其实就是一阶加权特征，计算复杂度为O(n)，第三项中的权重$w_{ij}$，这儿使用到了矩阵分解，分解为 $ W=V^TV $, vi、vj分别为xi、xj的隐向量 于是，原始公式变为了： $ FM=w_0+\\sum_{i=1}^n w_ix_i+\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} &lt;v_i,v_j&gt;x_ix_j$ 我们假设隐向量的长度为k ，那么交叉项的参数量变为 kn 个。此时时间复杂度仍为O(kn^2)，通过以下方式可以简化为O(kn)，如下图： 附上核心代码： class FeaturesLinear(torch.nn.Module): def __init__(self, field_dims, output_dim=1): super().__init__() self.fc = torch.nn.Embedding(sum(field_dims), output_dim) self.bias = torch.nn.Parameter(torch.zeros((output_dim,))) self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long) def forward(self, x): &quot;&quot;&quot; :param x: Long tensor of size ``(batch_size, num_fields)`` &quot;&quot;&quot; x = x + x.new_tensor(self.offsets).unsqueeze(0) return torch.sum(self.fc(x), dim=1) + self.biasclass FeaturesEmbedding(torch.nn.Module): def __init__(self, field_dims, embed_dim): super().__init__() #sum(field_dims) 是为了把所有的field结合在一起 self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim) # 这里为什么取[:-1]? 这样0-9992 的embedding表中 # 0-6040是对应user，6041-9992对应item self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long) torch.nn.init.xavier_uniform_(self.embedding.weight.data) def forward(self, x): &quot;&quot;&quot; :param x: Long tensor of size ``(batch_size, num_fields)`` &quot;&quot;&quot; x = x + x.new_tensor(self.offsets).unsqueeze(0) return self.embedding(x) # output size (batch_size, num_fields, embed_dim) class FactorizationMachine(torch.nn.Module): def __init__(self, reduce_sum=True): super().__init__() self.reduce_sum = reduce_sum def forward(self, x): &quot;&quot;&quot; :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)`` &quot;&quot;&quot; square_of_sum = torch.sum(x, dim=1) ** 2 sum_of_square = torch.sum(x ** 2, dim=1) ix = square_of_sum - sum_of_square if self.reduce_sum: ix = torch.sum(ix, dim=1, keepdim=True) return 0.5 * ixclass FactorizationMachineModel(torch.nn.Module): &quot;&quot;&quot; A pytorch implementation of Factorization Machine. Reference: S Rendle, Factorization Machines, 2010. &quot;&quot;&quot; def __init__(self, field_dims, embed_dim): super().__init__() self.embedding = FeaturesEmbedding(field_dims, embed_dim) self.linear = FeaturesLinear(field_dims) self.fm = FactorizationMachine(reduce_sum=True) def forward(self, x): &quot;&quot;&quot; :param x: Long tensor of size ``(batch_size, num_fields)`` &quot;&quot;&quot; x = self.linear(x) + self.fm(self.embedding(x)) return torch.sigmoid(x.squeeze(1)) 4.DeppFM顾名思义，DeepFM是Deep与FM结合的产物， 4.1 Sparse FeatureSparse Feature是指离散型变量。比如现在我有数据：xx公司每个员工的姓名、年龄、岗位、收入的表格，那么年龄和岗位就属于离散型变量，而收入则称为连续型变量。这从字面意思也能够理解。 Sparse Feature框里表示的是将每个特征经过one-hot编码后拼接在一起的稀疏长向量，黄色的点表示某对象在该特征的取值中属于该位置的值。 4.2 Dense Embeddings该层为嵌入层，用于对高维稀疏的 01 向量做嵌入，得到低维稠密的向量 e (每个01向量对应自己的嵌入层，不同向量的嵌入过程相互独立，如上图所示）。然后将每个稠密向量横向拼接，在拼接上原始的数值特征，然后作为 Deep 与 FM 的输入。 最终输入模型的值如下图，Sparse Feature经过embedding之后，与归一化后的连续特征拼接，一起输入模型 4.3 FM Layer线性部分 (黑色线段) 是给与每个特征一个权重，然后进行加权和；交叉部分 (红色线段) 是对特征进行两两相乘，然后赋予权重加权求和。然后将两部分结果累加在一起即为 FM Layer 的输出。 4.4 Hidden LayerDeep 部分的输入 为所有稠密向量的横向拼接，然后经过多层线性映射+非线性转换得到 Hidden Layer 的输出，一般会映射到1维，因为需要与 FM 的结果进行累加。 4.5 Output Units$ DeepFM=sigmoid(y_{FM}+y_{DNN})$ 输出层为 FM Layer 的结果与 Hidden Layer 结果的累加，低阶与高阶特征交互的融合，然后经过 sigmoid 非线性转换，得到预测的概率输出。 优点： 两部分联合训练，无需加入人工特征，更易部署； 结构简单，复杂度低，两部分共享输入，共享信息，可更精确的训练学习。 缺点： 将类别特征对应的稠密向量拼接作为输入，然后对元素进行两两交叉。这样导致模型无法意识到域的概念，FM 与 Deep 两部分都不会考虑到域，属于同一个域的元素应该对应同样的计算。 最后上核心代码： class DeepFM(nn.Module): def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8, hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]): &quot;&quot;&quot; cate_fea_nuniqs: 类别特征的唯一值个数列表，也就是每个类别特征的vocab_size所组成的列表 nume_fea_size: 数值特征的个数，该模型会考虑到输入全为类别型，即没有数值特征的情况 &quot;&quot;&quot; super().__init__() self.cate_fea_size = len(cate_fea_nuniqs) self.nume_fea_size = nume_fea_size &quot;&quot;&quot;FM部分&quot;&quot;&quot; # 一阶 if self.nume_fea_size != 0: self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1) # 数值特征的一阶表示 self.fm_1st_order_sparse_emb = nn.ModuleList([ nn.Embedding(voc_size, 1) for voc_size in cate_fea_nuniqs]) # 类别特征的一阶表示 # 二阶 self.fm_2nd_order_sparse_emb = nn.ModuleList([ nn.Embedding(voc_size, emb_size) for voc_size in cate_fea_nuniqs]) # 类别特征的二阶表示 &quot;&quot;&quot;DNN部分&quot;&quot;&quot; self.all_dims = [self.cate_fea_size * emb_size] + hid_dims self.dense_linear = nn.Linear(self.nume_fea_size, self.cate_fea_size * emb_size) # 数值特征的维度变换到FM输出维度一致 self.relu = nn.ReLU() # for DNN for i in range(1, len(self.all_dims)): setattr(self, &apos;linear_&apos;+str(i), nn.Linear(self.all_dims[i-1], self.all_dims[i])) setattr(self, &apos;batchNorm_&apos; + str(i), nn.BatchNorm1d(self.all_dims[i])) setattr(self, &apos;activation_&apos; + str(i), nn.ReLU()) setattr(self, &apos;dropout_&apos;+str(i), nn.Dropout(dropout[i-1])) # for output self.dnn_linear = nn.Linear(hid_dims[-1], num_classes) self.sigmoid = nn.Sigmoid() def forward(self, X_sparse, X_dense=None): &quot;&quot;&quot; X_sparse: 类别型特征输入 [bs, cate_fea_size] X_dense: 数值型特征输入（可能没有） [bs, dense_fea_size] &quot;&quot;&quot; &quot;&quot;&quot;FM 一阶部分&quot;&quot;&quot; fm_1st_sparse_res = [emb(X_sparse[:, i].unsqueeze(1)).view(-1, 1) for i, emb in enumerate(self.fm_1st_order_sparse_emb)] fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim=1) # [bs, cate_fea_size] fm_1st_sparse_res = torch.sum(fm_1st_sparse_res, 1, keepdim=True) # [bs, 1] if X_dense is not None: fm_1st_dense_res = self.fm_1st_order_dense(X_dense) fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res else: fm_1st_part = fm_1st_sparse_res # [bs, 1] &quot;&quot;&quot;FM 二阶部分&quot;&quot;&quot; fm_2nd_order_res = [emb(X_sparse[:, i].unsqueeze(1)) for i, emb in enumerate(self.fm_2nd_order_sparse_emb)] fm_2nd_concat_1d = torch.cat(fm_2nd_order_res, dim=1) # [bs, n, emb_size] n为类别型特征个数(cate_fea_size) # 先求和再平方 sum_embed = torch.sum(fm_2nd_concat_1d, 1) # [bs, emb_size] square_sum_embed = sum_embed * sum_embed # [bs, emb_size] # 先平方再求和 square_embed = fm_2nd_concat_1d * fm_2nd_concat_1d # [bs, n, emb_size] sum_square_embed = torch.sum(square_embed, 1) # [bs, emb_size] # 相减除以2 sub = square_sum_embed - sum_square_embed sub = sub * 0.5 # [bs, emb_size] fm_2nd_part = torch.sum(sub, 1, keepdim=True) # [bs, 1] &quot;&quot;&quot;DNN部分&quot;&quot;&quot; dnn_out = torch.flatten(fm_2nd_concat_1d, 1) # [bs, n * emb_size] if X_dense is not None: dense_out = self.relu(self.dense_linear(X_dense)) # [bs, n * emb_size] dnn_out = dnn_out + dense_out # [bs, n * emb_size] for i in range(1, len(self.all_dims)): dnn_out = getattr(self, &apos;linear_&apos; + str(i))(dnn_out) dnn_out = getattr(self, &apos;batchNorm_&apos; + str(i))(dnn_out) dnn_out = getattr(self, &apos;activation_&apos; + str(i))(dnn_out) dnn_out = getattr(self, &apos;dropout_&apos; + str(i))(dnn_out) dnn_out = self.dnn_linear(dnn_out) # [bs, 1] out = fm_1st_part + fm_2nd_part + dnn_out # [bs, 1] out = self.sigmoid(out) return out 参考链接[1] https://blog.csdn.net/weixin_44556141/article/details/120790057 [2] https://zhuanlan.zhihu.com/p/354994307 [3] https://blog.csdn.net/Jeremiah_/article/details/120740877 [4] https://zhuanlan.zhihu.com/p/361451464","categories":[],"tags":[{"name":"排序 - 推荐系统","slug":"排序-推荐系统","permalink":"http://renxingkai.github.io/tags/排序-推荐系统/"}]},{"title":"Learning Robust Models for e-Commerce Product Search阅读笔记","slug":"paper-lstm-ved","date":"2022-03-26T10:41:12.000Z","updated":"2022-03-26T02:42:27.460Z","comments":true,"path":"2022/03/26/paper-lstm-ved/","link":"","permalink":"http://renxingkai.github.io/2022/03/26/paper-lstm-ved/","excerpt":"","text":"论文链接Abstract商品显示与搜索查询意图不匹配的商品会降低电商中的用户体验。这些不匹配源于排名算法对嘈杂的行为信号（如搜索日志中的点击和购买）的不真实偏差。解决这类问题需要一个海量的标注数据，然而成本很高。本文中，我们开发了一个端到端模型，该模型学习如何有效地对不匹配进行分类，并生成难不匹配样本以改进分类器。我们通过在交叉熵损失中引入一个潜在变量来对模型进行端到端的训练，交叉熵损失在使用真实样本和生成样本之间交替进行。这不仅使分类器更加健壮，还提高了整体排名性能。 1 Introduction基于文本的电商搜索挑战很大，可能简单的修改query，就会明显改变搜索意图。此外，用户行为的噪声也会给排序算法带来较大的误导，可能用户搜索XX手机充电器，最后购买了手机。 在本文中，我们考虑的问题主要是query-item不匹配，以提高产品搜索的排名性能。此类任务需要大量的正负样本标注数据，即使我们能够部分地负担昂贵且耗时的标记数据，所获取的数据集也是不平衡的，并且缺乏难正样本，这使得分类器无法学习稳健的决策边界。然而，上面的Iphone X和Iphone X充电器示例表明，利用标记的数据可以人工生成有意义的正样本。事实上，我们可以通过观察相应query通常购买哪些商品来启发式地构造大量负样本。问题是，我们能用这样的负样本来生成难以分类的来增强分类器的稳健性吗？我们在图1中说明了生成的目标。 为此，我们开发了一个端到端模型，该模型能够识别不匹配的query-item对，并且能够生成给定item的不匹配query。我们在一个端到端的模型中包含了我们的分类器和生成器。分类器只需要将生成query的连续表示作为第二个输入，而不是离散的文本序列。这一关键特性使我们能够使用高效的基于梯度的优化技术，绕过基于强化学习的方法（这些方法要复杂得多），以及最近开发的启发式方法GAN等。 2 Proposed Model: QUARTS提出模型QUARTS(QUery-based Adversarial learning for Robust Textual Search)包含： 一个LSTM&amp;Attention分类器 一个VED(variational encoder-decoderquery generator) 用于输出(I,Q) ，输出Q’,Q’与I不相关但是与Q 词粒度相关 一个状态组合器 为了建立端到端的模型，我们将分类器和生成器计算的query表示结合起来，形成对注意层的适当输入。在上图橘色部分，我们增加了一个融合层，H、Hgen为相应LSTM对Q和Qgen输入的标识，z服从伯努利分布，控制Q或者Qgen是否使用。当z=0时，QUARTS计算不匹配的概率。 y=0—&gt;s=z ，这样就可以同时输入H和Hgen y=1—&gt;s=0，仅输入H 具体训练过程如下: 3 Experiments and Results实验不多说了，具体有兴趣看原文。 4 Conclusion and Future Work我们开发了一个端到端的模型，用于在电商产品搜索中检索难以分类的query。我们基于文本蕴涵的思想，使用逐字attention层帮助创建以输入query为条件的表示。我们训练了一个生成器，它生成与源项不匹配的query表示，同时又是“真实的”。这允许我们解决数据集的类别不平衡问题，同时生成有助于稳健训练分类器的样本。为了对模型进行端到端的训练，我们修改了交叉熵损失，从而避免了优化极小极大目标。在离线数据集和实时产品搜索流量上的实验表明，我们的方法比基线显著提高。","categories":[{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"}],"tags":[{"name":"搜索NLP","slug":"搜索NLP","permalink":"http://renxingkai.github.io/tags/搜索NLP/"}],"author":"CinKate"},{"title":"Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter阅读笔记","slug":"paper-lebert","date":"2022-03-19T19:22:05.000Z","updated":"2022-03-19T11:23:11.413Z","comments":true,"path":"2022/03/20/paper-lebert/","link":"","permalink":"http://renxingkai.github.io/2022/03/20/paper-lebert/","excerpt":"","text":"论文链接Abstract目前，中文序列标注任务中有很多预训练模型(BERT)与词汇信息的探索工作。然而，大多工作只是将词汇信息通过浅层的网络(随机初始化)编码，并没有将其集成到BERT的底层网络中。本文我们提出了LexiconEnhanced BERT (LEBERT) ，使用Lexicon Adapter layer将外部词汇信息集成到BERT中。我们的模型促进了外部知识在BERT底层网络中的融合，在中文NER、分词、词性标注等任务中，得到了SOTA的结果。 1 Introduction目前有两个主线用于增强字符级别的中文NER方法，第一种认为将此信息集成到基于字符的序列编码器中，以便可以明确建模字特征。第二种考虑集成大规模的预训练文本向量，eg:BERT；使用隐式词级别的语义知识。 最近主要的思路是从BERT获取全文表示和词汇特征到一个神经序列标注模型，如下图1a，然而这种方式没有完全开发BERT的表示能力，因为外部特征没有集成到底层。 我们提出LEBERT，具体而言，通过将句子与现有的词典匹配，中文句子转换为char-words序列。然后设计一个词汇Adapter去为每个字符抽取最相关匹配的词通过一个char-to-word的Bilinear注意力机制。如图1b。在训练期间同时微调BERT与词汇Adapter。 2 Related WorkLexicon-based. 目标是加强字符模型与词汇信息的融合，比如lattice-LSTM,词汇信息在中文分词和词性标注任务中是有效的。 Pre-trained Model-based.比如BERT+Softmax等 Hybrid Model集成词汇信息与预训练模型从而利用他们各自的长处。一般是通过将BERT表示和词汇信息拼接输入到一个浅层网络中。ERNIE、ZEN都尝试对预训练模型在词汇粒度信息融入。 BERT Adapter该模块主要去学习下游特定任务的参数。具体地说，它们在预先训练好的模型的层之间添加适配器，并仅为特定任务调整添加的适配器中的参数。MAD-X、K-ADAPTER 3 Method 模型主要结构如上图所示。LEBERT相对BERT有两点不同，第一，将中文句子的字符和词汇特征转为字符-词汇pair对同时输入到模型。第二，在Transformer层之间使用词汇adapter，将词汇知识集成到BERT中。 3.1 Char-Words Pair Sequence给定一个词汇表D和一个中文句子$s_c=\\{c_1,c_2,...c_n\\}$。我们通过将字符序列与D匹配，找出句子中所有的潜在单词，具体来说，我们首先基于D构建一个前缀树，然后遍历句子的所有字符子序列，并将它们与前缀树匹配，以获得所有潜在单词。如下图3所示，最终获得到$s_{cw}=\\{(c_1,ws_1),(c_2,ws_2),...(c_n,ws_n)\\}$,ci为句子中第i个字符，wsi为ci相匹配的词汇。 3.2 Lexicon Adapter句子中的每个位置有两种信息组成，字符级特征和词汇级特征。如下图4，我们提出Lexicon Adapter在BERT中直接注入词汇信息。为了对齐两种不同的表示，我们应用了一个非线性变换。由于每个字符可能对应了多个词语，并且多个词在不同任务中的重要性不同，我们使用了一个char-to-word的注意力机制。 3.3 Lexicon Enhanced BERTLexicon Adapter(LA)放置在BERT的transformer层中间，注入外部知识到BERT中。在正常的token embedding输入到Transformer层得到${h^k_1}$之后，我们将每对${(h^k_i,x^{ws}_i)}$传入到LA，最终经过多层编码后，拿出最后的表示H进行序列标注任务。 3.4 Training and Decoding使用CRF去完成序列标注任务。 4 Experiments在中文NER、分词、词性标注等任务上实验，对比模型BERT、BERT-word、ERNIE、ZEN等，具体结果直接看下表吧， 作者探索了LA使用在单层、多层、所有层Transformer的效果，结果如下，单层效果相对较好，这可能是因为浅层促进了词典特征和文本之间更分层的交互。 5 Conclusion本文提出LEBERT，在Trasformer layer中集成词汇信息，与模型级别的集成方法对比，LEBERT在BERT级别进行了词汇信息与BERT表示的深度融合。在三个中文序列标注任务中，拿到了SOTA。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"序列标注","slug":"序列标注","permalink":"http://renxingkai.github.io/tags/序列标注/"}],"author":"CinKate"},{"title":"Context Enhanced Short Text Matching using Clickthrough Data","slug":"arxiv-2022-CBM","date":"2022-03-13T00:01:46.000Z","updated":"2022-03-12T16:02:48.588Z","comments":true,"path":"2022/03/13/arxiv-2022-CBM/","link":"","permalink":"http://renxingkai.github.io/2022/03/13/arxiv-2022-CBM/","excerpt":"","text":"论文链接TL;DR: 使用搜索引擎扩充 短文本相关结果，构造正负样本，补充短文本缺失的信息，最终与BERT baseline进行融合获得最终结果，主要是使用了外部信息来进行数据补充与增强。 Abstract目前的短文本匹配模型通常依赖于匹配的一对短文本，然而短文本通常缺少关键的线索；因此，短文本需要外部知识去弥补更多的语义信息。为了解决这个问题，我们提出了一个新的短文本匹配框架去引入外部知识加强短文本语义表示。我们使用self-attention机制使用外部信息去丰富短文本表示，在英文与中文数据集上，实验表明了我们的框架比SOTA模型好。 1 Introduction短文本匹配在QA、IR、段落识别等任务中有丰富应用。目前DL模型主要分为基于表示的文本匹配模型与基于交互式的文本匹配模型。后者通常比前者效果好些，但是仍然由于文本较短会缺少重要的语义信息，这种情况在中文语料更为明显，例如下图1。 我们通过搜索引擎，将原始句子输入，去查找相关的文本。检索结果通常包含足够的上下文信息，因此匹配模型能使用充足信息完成匹配任务。 从以上观点，我们提出了上下文感知的BERT模型(CBM)，通过外部语义相关句子。CBM根据上下文增强注意机制选择所需的上下文并更新短文本表示。 2 Framework给定两个句子$S_a$、$S_b$，每个句子$S_i$有一个列表$C_i$，它从搜索引擎而来。我们的结构有3个模块，1)文本爬取模块，2)内容选择模块，3)内容加强文本匹配模块。 2.1 Context Crawler每个句子$S_i$的候选集合表$C_i$是从搜索结果爬取出来的，然而$C_i$是有噪声的。我们首先使用正则等方式进行清洗，同时清洗掉个人信息，最后得到相对干净的候选集$C_i$。 2.2 Context Selector使用BRET baseline去得到句子$S_i$的候选集合表$C_i$中每个句子的相似度分数，划分正负样本。 2.3 Context-enhanced Text Matcher使用sentence-BERT去编码句子$S_a$、$S_b$得到$h_a$、$h_b$，同时使用context-BERT编码$C_a$、$C_b$得到$h^c_a$、$h^c_b$，然后将$h_a$、$h_b$、$h^c_a$、$h^c_b$一起拼接输入到一个3层的Transformer，最终获取$h_a$、$h_b$为句子$S_a$、$S_b$的表示。 2.4 Matching Classifier最终使用$h_a$、$h_b$输入FFN做二分类预测，loss使用BCE。 2.5 Result Selector由于并不是每一对短文都需要语境增强，因为这些短文对BERT基线有很高的可信度，我们将保留结果和logits。最终概率表达式为 $y_i=\\hat{y_i}+\\bar{y_i}-1$ $\\hat{y_i}$为BERT baseline，$\\bar{y_i}$为我们的模型结果，最终结果大于0.5作为label 1，小于0.5为label 0。 3 Experiments实验结果:中英文 4 Conclusion本文我们提出的模型用于加强BERT在短文本匹配的能力，以两句话和相关语境为输入，整合外部信息来缓和单词歧义。消融实验表明，语义信息和多粒度信息对文本匹配建模都很重要。","categories":[{"name":"语义搜索","slug":"语义搜索","permalink":"http://renxingkai.github.io/categories/语义搜索/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://renxingkai.github.io/tags/BERT/"},{"name":"搜索相关性","slug":"搜索相关性","permalink":"http://renxingkai.github.io/tags/搜索相关性/"}],"author":"CinKate"},{"title":"Pretraining without Wordpieces Learning Over a Vocabulary of Millions of Word 笔记","slug":"paper-wordbert","date":"2022-03-01T22:46:08.000Z","updated":"2022-03-01T14:47:39.018Z","comments":true,"path":"2022/03/02/paper-wordbert/","link":"","permalink":"http://renxingkai.github.io/2022/03/02/paper-wordbert/","excerpt":"","text":"论文链接TL;DR:使用训练词的方式替代了原始BERT训练字的方式，在中英文下表现不错，尤其是NER、MRC等任务，且推理速度和BERT差不多(虽然增加了word embedding参数) Abstract标准的BERT分词使用子词分词法，可能将一个单词分为多个子词，如：lossless–&gt;loss、less。这将会带来如下的问题：(1)获得被分成多个子词的单词的上下文向量的最佳方法是什么？(2)如何通过完形填空的方式预测一个单词而不事先知道子词的数量？本篇工作中，我们探索使用词表的方式去预训练BERT相关模型，替代原始的子词预训练方式，称为WordBERT。我们使用不同大小的词表、初始配置、语言训练模型。结果表明：对比基于子词的BERT模型，WordBERT在机器阅读理解和完型填空等任务上有较大的提升。在许多其他自然语言理解任务中，包括词性标注、NER，WordBERT的表现始终优于BERT。模型分析表明，WordBERT比BERT的主要优势在于对低频词和稀有词的理解。此外，我们对WordBERT在汉语上也进行了训练，并在五个NLU上获得了显著的收益。最后，对推理速度的分析表明，WordBERT在自然语言理解任务中的时间开销与BERT相当。 1 Introduction我们尝试丢弃子词级别的原始BERT模型，使用全词方式的BERT训练方式，探索了不同语言、不同词表大小(500K、1M)、Glove等结构。为了避免更新巨大的词表参数，训练过程中，在每个batch内，我们只更新一小部分词向量参数。虽然WordBERT相比BERT有更多的参数，但在NLU任务中，基本和BERT耗时差不多。 3 Model 3.1 WordBERT使用500K、1M大小的词表来训练BERT 3.2 WordBERT-Glove使用Glove初始化,原始Glove词向量维度为300，使用Glove词表与BERT词表重复的22, 860个词进行训练，将词向量维度从300映射到800，损失函数用MSE。 3.3 WordBERT-ZH考虑到其他语言，使用WUDAO语料，训练WordBERT-ZH,词表大小278K，维度768，随机初始化。 4 Experiment我们尝试实验了多种NLU任务，例如完型填空、MRC、词性标注、NER；此外，我们构建了两个数据集去分析模型在不同词频下的性能。同时在CLUE上验证了WordBERT-ZH，最后我们对比了在不同任务上的模型推理速度。 6 Conclusion在这篇论文中，我们探索了使用基于抽样的训练策略，使用基于单词的预训练方式训练BERT。我们采用不同的配置，即是否使用Glove等预训练词嵌入和不同的词汇量。在MRC、完型填空、词性标注、NER等任务有了较大提升。模型分析表明，我们的模型生成的单词更加多样化和有意义，而BERT倾向于生成一般的单词。在中文领域，WordBERT 于NLU中也有不错的性能。","categories":[{"name":"预训练语言模型","slug":"预训练语言模型","permalink":"http://renxingkai.github.io/categories/预训练语言模型/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://renxingkai.github.io/tags/BERT/"}],"author":"CinKate"},{"title":"shell学习笔记-02-变量","slug":"shell-learn-02","date":"2022-02-08T23:15:08.000Z","updated":"2022-02-08T15:16:55.615Z","comments":true,"path":"2022/02/09/shell-learn-02/","link":"","permalink":"http://renxingkai.github.io/2022/02/09/shell-learn-02/","excerpt":"","text":"1.ping test1变量定义与使用#!/usr/bin/baship=10.18.42.1if ping -c1 $ip &amp;&gt; /dev/null;thenecho &quot;$ip is up.&quot;else echo &quot;$ip is down.&quot;fi 2.ping test2 使用上一个命令的返回值 $?上一个命令的返回值#!/usr/bin/baship=10.18.42.1ping -c1 $ip &amp;&gt; /dev/nullif [ $? -eq 0 ];thenecho &quot;$ip is up.&quot;else echo &quot;$ip is down.&quot;fi 3.ping test3 从键盘读入变量并赋值 $?上一个命令的返回值#!/usr/bin/bashread -p &quot;please input ip:&quot; ip #从键盘输入值，只需要定义变量名称ping -c1 $ip &amp;&gt; /dev/nullif [ $? -eq 0 ];thenecho &quot;$ip is up.&quot;else echo &quot;$ip is down.&quot;fi 4.ping test4 命令行传入变量值 #$1代表第一个变量#!/usr/bin/bashping -c1 $1 &amp;&gt; /dev/nullif [ $? -eq 0 ];thenecho &quot;$1 is up.&quot;else echo &quot;$1 is down.&quot;fi ./pingtest4.sh 10.18.42.1 10.18.42.127##会传入10.18.42.1第一个参数 5.自定义变量与环境变量 自定义变量影响范围仅在当前shell 环境变量会影响当前shell/子shell，关键字export .表示在当前shell中执行，下图即可在不同shell中，拿到其他shell文件中的变量 6.位置变量+预定义变量 上图输出 7.$0的使用，输出脚本名 #!/usr/bin/bash#如果用户没有输入参数，，要进行判断并提示if [ $# -eq 0 ];then echo &quot;usage: $0 file&quot; exitfi#-f判断是不是一个文件，! -f 取反，不是一个文件if [ ! -f $1 ];then echo &quot;error file!&quot; exitfi#读取文件，进行pingfor ip in `cat $1`do ping -c1 $ip &amp;&gt; /dev/null if [ $? -eq 0 ]; then echo &quot;$ip is up&quot; else echo &quot;$ip is down&quot; fidone 8.变量的赋值 ``$()#都表示将内部的命令先替换，再执行`date +%F`$(date +%F)read 可以同时读取多个变量，使用空格分割 9.变量的运算 10.计算当前内存所占百分比 #! /usr/bin/bashmem_used=`free -m |grep '^Mem:' |awk '&#123;print $3&#125;'`mem_total=`free -m |grep '^Mem:' |awk '&#123;print $2&#125;'`mem_percent=$((mem_used*100/mem_total))echo \"当前使用百分比: $mem_percent\"输出：root@iZhp3j31ufmsnq0rht2ynkZ:~# bash men_use.sh 当前使用百分比: 2 11.变量的删除 12.各种特殊符号 () 子shell中执行(()) 数值比较，类似C语言$() 命令替换$(()) 整数运算&#123;&#125; 集合$&#123;&#125; 取变量值[] 条件测试 [[]] 条件测试，支持正则=~$[] 整数运算","categories":[{"name":"shell","slug":"shell","permalink":"http://renxingkai.github.io/categories/shell/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://renxingkai.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://renxingkai.github.io/tags/linux/"}],"author":"CinKate"},{"title":"C++基础--程序内存模型","slug":"cpp-program-mem-model","date":"2021-11-30T23:56:13.000Z","updated":"2021-11-30T15:57:39.064Z","comments":true,"path":"2021/12/01/cpp-program-mem-model/","link":"","permalink":"http://renxingkai.github.io/2021/12/01/cpp-program-mem-model/","excerpt":"","text":"1.内存分区模型C++程序在执行时，将内存大方向划分为4个区域 代码区：存放函数体的二进制代码，由操作系统进行管理 全局区：存放全局变量和静态变量以及常量 栈区：由编译器自动分配释放，存放函数的参数值、局部变量等 堆区：有程序员分配和释放，若程序员不释放，程序结束时候由操作系统回收 不同区存放数据会赋予不同的生命周期。 1.1程序运行前在程序编译后，生成了exe可执行程序，未执行该程序前分为两个区域：代码区和全局区。 代码区： 存放CPU执行的机器指令。代码区是共享的，共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。代码区是只读的，使其只读的原因是防止程序意外地修改了它的指令 全局区： 全局变量和静态变量存放在此，全局区还包含了常亮区，字符串常量和其他常量也存放在此。该区域的数据在程序结束后由操作系统释放。 #include&lt;iostream&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;//全局变量int g_a = 10;int g_b = 10;//const修饰的全局常量const int c_g_a = 10;const int c_g_b = 10;int main() &#123; //全局区 //存放全局变量、静态变量、常量 //创建普通局部变量 int a = 10; int b = 10; cout &lt;&lt; \"局部变量a的地址为:\" &lt;&lt; (int)&amp;a &lt;&lt; endl; cout &lt;&lt; \"局部变量b的地址为:\" &lt;&lt; (int)&amp;b &lt;&lt; endl; cout &lt;&lt; \"全局变量a的地址为:\" &lt;&lt; (int)&amp;g_a &lt;&lt; endl; cout &lt;&lt; \"全局变量b的地址为:\" &lt;&lt; (int)&amp;g_b &lt;&lt; endl; //静态变量 static static int s_a = 10; static int s_b = 10; cout &lt;&lt; \"静态变量s_a的地址为:\" &lt;&lt; (int)&amp;s_a &lt;&lt; endl; cout &lt;&lt; \"静态变量s_b的地址为:\" &lt;&lt; (int)&amp;s_b &lt;&lt; endl; //常量 //字符串常量 cout &lt;&lt; \"字符串常量的的地址为:\" &lt;&lt; (int)&amp;\"hello world\" &lt;&lt; endl; //const修饰全局常量 cout &lt;&lt; \"全局常量 c_g_a的地址:\" &lt;&lt; (int)&amp;c_g_a &lt;&lt; endl; cout &lt;&lt; \"全局常量 c_g_b的地址:\" &lt;&lt; (int)&amp;c_g_b &lt;&lt; endl; //const修饰的局部变量 const int c_l_a = 10; const int c_l_b = 10; cout &lt;&lt; \"局部常量 c_l_a的地址:\" &lt;&lt; (int)&amp;c_l_a &lt;&lt; endl; cout &lt;&lt; \"局部常量 c_l_b的地址:\" &lt;&lt; (int)&amp;c_l_b &lt;&lt; endl;&#125; 总结： C++中在程序运行前分为全局区和代码区 代码区特点共享、只读 全局区中存放全局变量、静态变量、常量 常量区中存放const修饰的全局常量和字符串常量 1.2程序运行后栈区： 由编译器自动分配释放，存放函数的参数值，局部变量等。注意事项：不要返回局部变量的地址，栈区开辟的数据有编译器自动释放。 示例: #include&lt;iostream&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;//栈区数据的注意事项 --不要返回局部变量的地址//栈区由编译器管理int* func(int b) &#123;//形参也会存放在栈区 b = 100; int a = 10;//局部变量，存放栈区，函数执行完自动释放 return &amp;a;//不要返回局部变量的地址&#125;int main() &#123; int* p = func(10); cout &lt;&lt; \"返回地址为:\"&lt;&lt;*p &lt;&lt; endl;//第一次 因为编译器做了保留 cout &lt;&lt; \"返回地址为:\" &lt;&lt; *p &lt;&lt; endl;//第二次 不再保留数据&#125; 堆区： 由程序员分配释放，若程序员不释放，程序运行完后操作系统回收，在C++中主要使用new在堆区开辟内存 #include&lt;iostream&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;//栈区数据的注意事项 --不要返回局部变量的地址//栈区由编译器管理int* func(int b) &#123; //指针本质也是局部变量，存放在栈区，指针保存的数据存放在堆区 int* a = new int(10);//new 到堆区 return a;&#125;int main() &#123; //堆区开辟数据 int* p = func(10); cout &lt;&lt; \"返回地址为:\" &lt;&lt; *p &lt;&lt; endl;//第一次 因为编译器做了保留 cout &lt;&lt; \"返回地址为:\" &lt;&lt; *p &lt;&lt; endl;//第二次 不再保留数据&#125; 1.3new操作符C++中利用new操作符在堆区开辟数据 堆区开辟的数据，有程序员手动开辟，手动释放，释放使用delete 语法: new 数据类型 利用new创建的数据，会返回该数据对应类型的指针 示例1 #include&lt;iostream&gt;#include&lt;string&gt;#include&lt;ctime&gt;using namespace std;//栈区数据的注意事项 --不要返回局部变量的地址//栈区由编译器管理//1.new的基本语法int* func() &#123; //在堆区创建整型数据 //new返回该数据类型的指针 int *p=new int(10); return p;&#125;void test01() &#123; int* p = func(); cout &lt;&lt; *p &lt;&lt; endl; cout &lt;&lt; *p &lt;&lt; endl; cout &lt;&lt; *p &lt;&lt; endl; delete p; //cout &lt;&lt; *p &lt;&lt; endl;//非法操作，内存已经被释放&#125;//2.在堆区使用new开辟数组void test02() &#123; //10个整型数据的数组 int *arr=new int[10]; for (int i = 0; i &lt; 10; i++) &#123; arr[i] = i + 100; &#125; for (int i = 0; i &lt; 10; i++) &#123; cout &lt;&lt; arr[i] &lt;&lt; endl; &#125; //释放数组 delete[]arr;&#125;int main() &#123; //test01(); //堆区数据有程序员管理与释放 test02();&#125;","categories":[{"name":"C++","slug":"C","permalink":"http://renxingkai.github.io/categories/C/"}],"tags":[{"name":"程序内存模型","slug":"程序内存模型","permalink":"http://renxingkai.github.io/tags/程序内存模型/"}],"author":"CinKate"},{"title":"Que2Search Fast and Accurate Query Document Understanding for Search at Facebook笔记","slug":"Que2Search-paper-md","date":"2021-11-20T19:54:07.000Z","updated":"2021-11-20T11:55:59.804Z","comments":true,"path":"2021/11/21/Que2Search-paper-md/","link":"","permalink":"http://renxingkai.github.io/2021/11/21/Que2Search-paper-md/","excerpt":"","text":"ABSTRACT本文提出了Que2Search,用于搜索场景下query和product理解系统。Que2Search使用多模态和多任务去训练query和product的表示。通过将最新的多语言自然语言理解体系结构（如XLM和XLM-R）与多模态技术相结合，我们实现了超过5%的绝对离线相关性改进和超过4%的在线收益，超过了最先进的Facebook产品理解系统。下文也会描述基于大量离线和在线A/B实验的哪些模型优化有效（哪些无效）。 1 INTRODUCTION传统的搜索引擎使用基于各种term匹配的算法，最近有人提出了基于embedding的检索方式，使用wide and deep网络去为query和doc构建embedding；这些方法缺乏代表性，由于计算性能问题没有引入BERT。 本文介绍了Que2Search，一种query2product相似性模型，它提供了一种考虑综合多模态特征的建模方法，并利用XLMR进行文本特征编码。Que2Search在弱监督数据集上训练，相比于之前的FB baseline，实现了SOTA；并已部署到了FB Marketplace，它增强了基于product embedding的检索的query检索能力，还可用作排名特征。 以下是几个Que2Search的挑战： 产品描述充满噪声。卖方提供的产品描述质量差异很大。在许多情况下，产品的属性丢失或拼写错误。 国际化支持。我们希望构建一个模型，在启用Facebook Marketplace的情况下，该模型能够在多种语言中运行良好。 有效处理多模态。我们需要将所有多模态特征（如产品图像和文本信息）有效地考虑到一个模型中。 严格的延迟约束。我们需要满足搜索引擎的严格延迟约束，这尤其具有挑战性，因为我们使用基于Transformer的语言模型，这在计算上非常昂贵 由于基于Transformer的语言模型参数较多，推理速度也会很慢，推理时间对于搜索问题尤其重要，因为我们需要为实时的自由文本query提供embedding表示。query侧，我们使用2层的XLM encoder去编码query，product侧，我们使用多语言XLM-R去编码商品文本描述等信息。Que2Search通过引入几种建模技术在生产应用中取得了进展，包括多阶段课程学习、多模态处理和多任务学习，这些技术可以联合优化对产品检索任务和产品分类任务的query。 2 RELATED WORK 基于embedding的检索方式。孪生网络、双塔模型、wide&amp;deep等 多模态建模。之前大多使用双塔模型，分别编码query与doc，本文使用孪生结构 自然语言处理。BERTs、XLM-R 3 MODELING3.1 Model architecture模型结构如下图所示： query塔：使用3个输入特征：tri-gram字符特征(多类别特征，hash滑窗等方法获取embedding之后sum_pool作为每个句子表示)，搜索用户的国家(单类别特征，同样的EmbeddingBag方法)，原始的query文本(2层XLM编码，提取最后一层[CLS]表示作为句子表示)，最后使用attention层融合三个特征得到query的最终表示。 doc塔：输入包含product的标题、描述和图像，同样使用了EmbeddingBag编码文本的tri-gram等特征，使用预训练的MLP layer完成图像编码，最终也使用attention层去获取最后的doc表示。使用simple attention方式优于concat特征的融合方式。 具体训练细节：lr=7e-4,batch size=768，使用分层学习率，梯度裁剪，early stop等 3.1.1 Two Tower with Classification.我们使用额外的分类任务扩展了双塔模型，对于每个doc，我们从搜索日志中收集了一些列相关的query，维护了一个45k的最常见query列表。将此视为一个多标签多类别分类任务 3.2 Training训练数据 除了传统需要正负样本的监督数据，负样本我们采样了query到其他doc，正样本可从搜索日志中获得，我们通过过滤以下事件序列，从Facebook Marketplace搜索日志创建query-doc数据集：（1）用户搜索查询，（2）点击产品，（3）向市场卖家发送消息，以及（4）卖家回复。如果所有4个事件都发生在短时间内（如24小时），则用户可能找到了他们要查找的内容，因此该query与doc相关。 负样本采样在每个batch中，我们计算query-doc互相之间的余弦分数，结果矩阵中行为query,列为doc，doc di与query qi相似，与其他不相似，因此转换为一个多分类任务，类别数为batch size B 3.3 Curriculum Training除了batch内的负采样，我们还使用了课程学习。第一阶段，我们使用in-batch的负采样方法让模型先收敛，第二阶段，我们希望喂入更难的样本，传统方式是使用分开的难样本数据喂入到模型中，我们仍然使用余弦矩阵(B*B),挑选出每行中最高分数dj作为难的负样本(除了对角线)，因此我们获得了(qi,di,dj)的训练数据(三元组对)，尝试了BCE和margin rank loss，margin rank loss效果最好(margin 在0.1至0.2之间)。 3.4 EvaluationBatch recall@K、ROC AUC、KNN Recall@K 3.5 Speeding up model inferenceTorch Just-In-Time (JIT) We experimented with two XLM models inthe query tower: one with 2 layers, 4 attention heads and 128 sentence embedding dimension and the other with 3 encoder layers, 4attention heads and 256 sentence embedding dimension. We foundthe former to have a P99 latency of 1.5ms while the latter had aP99 latency of 3.5ms while the performance gain was minimal. 4 SYSTEM ARCHITECTURE CONCLUSION本文提出了Que2Search，使用多任务与多模态的方式学习query和product的表示。分享了为搜索用例调优和部署基于BERT的query理解模型的经验，并在99%分为实现了1.5毫秒的推断时间。同时分享了部署情节，以及关于部署步骤的实用建议，以及如何在搜索语义检索和排名中集成Que2Search组件。","categories":[{"name":"搜索NLP","slug":"搜索NLP","permalink":"http://renxingkai.github.io/categories/搜索NLP/"}],"tags":[{"name":"搜索NLP","slug":"搜索NLP","permalink":"http://renxingkai.github.io/tags/搜索NLP/"}],"author":"CinKate"},{"title":"shell学习笔记-01","slug":"shell-learn-01","date":"2021-10-10T23:16:18.000Z","updated":"2021-10-10T15:17:27.821Z","comments":true,"path":"2021/10/11/shell-learn-01/","link":"","permalink":"http://renxingkai.github.io/2021/10/11/shell-learn-01/","excerpt":"","text":"bash中调用python1.正常情况下bash解释器只能执行.sh脚本，加入&lt;&lt;-EOF EOF之后，可以执行python脚本（EOF只是人为定义的开始结束标记，可以换为其他字符） #!/usr/bin/bashping -c1 www.baidu.com &amp;&amp; echo \"www.com\" || echo \"www.com is not connect\"python &lt;&lt;-EOFprint(\"hello world\")EOF 2.当前shell执行与子shell执行区别 #以下两种都是在子shell中执行，不会改变当前目录路径bash test.sh./test.sh#以下两种都是在当前shell中执行，不会改变当前目录路径. bash.shsource bash.sh login与nologin shellbash路径： #系统级/etc/profile/etc/bashrc#用户级(各个用户各自独立) ~为用户根目录~/.bash_profile~/.bashrc#(以上四个为登陆shell时候会执行，以下两个为离开shell时候会执行)~/.bash_logout~/.bash_history login shell su - alice以下四个命令都会被执行/etc/profile/etc/bashrc~/.bash_profile~/.bashrc nologin shell su alice只会执行以下两个命令/etc/bashrc~/.bashrc 常用操作1.查看历史命令 # history45 python predict_torch_t5.py 46 ls47 cd .jupyter/48 ls49 cd title_generate/50 ls51 python predict_torch_t5.py 52 history 2.使用 !number(行号) 执行该行命令 3.上一个命令的最后一个参数 !$ 4.上一个命令 !! 5.搜索历史命令 ctrl+r 6.查看已经设置的别名 alias，删除别名unalias 7.快捷键ctrl+r 搜索历史命令ctrl+d 退出命令行ctrl+a 命令编辑，光标移动到命令最前ctrl+e 命令编辑，光标移动到命令最后ctrl+l 清除屏幕所有的内容，并开启一个新的一行ctrl+k 剪切（删除）光标处到行尾的所有字符ctrl+u 剪切（删除）光标处到行首的所有字符ctrl+s 锁定终端，使之任何人无法输入ctrl+q 解锁ctrl+s的锁定状态 8.后台执行命令 nohup 9.重定向：每一个进程打开一个文件都会有一个与之对应的文件描述符，只要该文件没有关闭，描述符就不会释放。三个通用的进程描述符0进程打开的输入的文件,1进程打开的输出的文件,2进程打开的输出错误的文件 ##重定向符号&gt; 输出重定向到一个文件或设备 覆盖原来的文件&gt;! 输出重定向到一个文件或设备 强制覆盖原来的文件&gt;&gt; 输出重定向到一个文件或设备 追加原来的文件&lt; 输入重定向到一个程序 ##标准错误重定向符号2&gt; 将一个标准错误输出重定向到一个文件或设备 覆盖原来的文件 b-shell2&gt;&gt; 将一个标准错误输出重定向到一个文件或设备 追加到原来的文件2&gt;&amp;1 将一个标准错误输出重定向到标准输出 注释:1 可能就是代表 标准输出&gt;&amp; 将一个标准错误输出重定向到一个文件或设备 覆盖原来的文件 c-shell|&amp; 将一个标准错误 管道 输送 到另一个命令作为输入##命令重导向示例在 bash 命令执行的过程中，主要有三种输出入的状况，分别是：标准输入；代码为 0 ；或称为 stdin ；使用的方式为 &lt;标准输出：代码为 1 ；或称为 stdout；使用的方式为 1&gt;错误输出：代码为 2 ；或称为 stderr；使用的方式为 2&gt; 例子： [test @test test]# ls -al &gt; list.txt将显示的结果输出到 list.txt 文件中，若该文件以存在则予以取代！[test @test test]# ls -al &gt;&gt; list.txt将显示的结果累加到 list.txt 文件中，该文件为累加的，旧数据保留！[test @test test]# ls -al 1&gt; list.txt 2&gt; list.err将显示的数据，正确的输出到 list.txt 错误的数据输出到 list.err[test @test test]# ls -al 1&gt; list.txt 2&gt; &amp;1将显示的数据，不论正确或错误均输出到 list.txt 当中！错误与正确文件输出到同一个文件中，则必须以上面的方法来写！不能写成其它格式！[test @test test]# ls -al 1&gt; list.txt 2&gt; /dev/null将显示的数据，正确的输出到 list.txt 错误的数据则予以丢弃！ /dev/null ，可以说成是黑洞装置。为空，即不保存。 cat重定向用法 ##将111 222 333输入到file2中cat &lt;&lt;EOF &gt;file2111222333EOF##将111 222 333输入到屏幕中cat &lt;&lt;EOF 111222333EOF 10.管道。tee最基本的用法就是显示输出结果并且保存内容到文件中。/home/renxingkai# ip addr | grep 'inet' | grep eth0 inet 10.120.102.100/28 brd 10.199.102.111 scope global eth0##保存到test中(覆盖)ip addr | grep 'inet' |tee test | grep eth0## 追加到test中ip addr | grep 'inet' |tee -a test | grep eth0/home/renxingkai# df | grep '/$'overlay 1874458716 742825040 1131633676 40% //home/renxingkai# cd shell_fs//home/renxingkai/shell_fs# df | tee df.txt | grep '/$'overlay 1874458716 742825040 1131633676 40% //home/renxingkai/shell_fs# lsdf.txt ping01.sh python01.py/home/renxingkai/shell_fs# cat df.txt Filesystem 1K-blocks Used Available Use% Mounted onoverlay 1874458716 742825040 1131633676 40% /tmpfs 65536 0 65536 0% /devtmpfs 131616820 0 131616820 0% /sys/fs/cgroup10.48.154.172:6789,10.48.155.148:6789,10.48.156.148:6789:/mmu_ssd/pub 579820584960 568357687296 11462897664 99% /share/dev/sda2 97039692 13075992 78991260 15% /opt/data/kesstmpfs 131616820 0 131616820 0% /dev/shm/dev/sdb 1874458716 742825040 1131633676 40% /etc/hosts10.116.34.12:6789,10.116.45.11:6789,10.116.140.186:6789:/search_ssd/renxingkai 164283838464 98382159872 65901678592 60% /home/renxingkaitmpfs 131616820 548 131616272 1% /dev/shm/kesstmpfs 131616820 12 131616808 1% /run/secrets/kubernetes.io/serviceaccounttmpfs 131616820 12 131616808 1% /proc/driver/nvidiadevtmpfs 131606300 0 131606300 0% /dev/nvidia6tmpfs 131616820 0 131616820 0% /proc/acpitmpfs 131616820 0 131616820 0% /proc/scsitmpfs 131616820 0 131616820 0% /sys/firmware## tee管道不会截流(rapids) root@kml-dtmachine-5430-prod:/home/renxingkai/shell_fs# date &gt; date.txt(rapids) root@kml-dtmachine-5430-prod:/home/renxingkai/shell_fs# date | tee date.txtSun Oct 10 18:23:13 CST 2021 11.命令排序 如果在一行写多个shell命令，可以使用;分割 cd;eject &amp;&amp; 前面命令执行成功后面才会执行 || 前面命令失败后面也会执行 ##ping成功执行is up，否则返回码非0，执行is downping -c1 10.18.42.1 &amp;&amp; echo &quot;is up.&quot; || echo &quot;is down.&quot; 注意： command &amp; ##后台执行command &amp;&gt; /dev/null ##混合重定向(标准输出1，错误输出2)command1 &amp;&amp; command2 ## 命令排序，逻辑判断 12.shell 通配符 * 匹配任意多个字符 ls in* #in开头的所有文件rm -rf *rm -rf *.pdffind / -iname &quot;*-eth0&quot;?匹配任意一个字符[]匹配括号中任意一个字符,^取非[abc][a-z][0-9][a-zA-Z0-9][^a-z]()在子shell中执行(cd /boot;ls) (umask 077;touch file1000)&#123;&#125;集合 touch file&#123;1-9&#125; 建立file1-file9的文件mkdir -pv /home/&#123;333/&#123;aaa,bbb&#125;,444&#125;可用于拷贝文件cp -rv /home/test/&#123;1.txt,2.txt&#125;\\转义符号","categories":[{"name":"shell","slug":"shell","permalink":"http://renxingkai.github.io/categories/shell/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://renxingkai.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://renxingkai.github.io/tags/linux/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第十二章--推荐应用实战：基于广告平台的推荐","slug":"SearchAndRec-Chapter12","date":"2021-06-12T12:14:30.000Z","updated":"2021-06-12T04:17:18.456Z","comments":true,"path":"2021/06/12/SearchAndRec-Chapter12/","link":"","permalink":"http://renxingkai.github.io/2021/06/12/SearchAndRec-Chapter12/","excerpt":"","text":"广告是为了达到某种特定的目的，向公众传递信息的宣传手段。广告对每一个企业业务的增长起到了举足轻重的作用。本章将介绍基于广告平台的推荐实践经验。 1 推荐系统的架构设计广告是互联网公司流量商业变现的主要形式，是由多方参与主体共同完成的一项商业营销活动。计算广告的核心问题是在给定的一系列上下文环境中，去寻求最合适的广告投放策略，从而实现广告价值最大化。广告涉及的三方包括平台、用户以及广告主。 从技术实现角度来讲，广告更偏推荐技术，但从本质上讲是将推荐和搜索技术完美统一。总体来说，广告平台需要均衡三方利益，推荐系统则需要更多关注用户体验。举一个简单的例子，同样使用CTR预估模型，对于广告平台，其可能更多是从收益出发，排序的最终结果也是为了最大化收益；对于推荐系统，除了关注排序结果，还需要在评价体系中评估用户体验，比如推荐结果的惊喜度、新颖性等。下图是将搜索、推荐、广告三者统一的架构设计。 另一方面，推荐更多是一种技术，而广告是一种业务。个性化推荐可以用在广告中，也可以用在其他产品中，只是计算广告的一个环节。个性化推荐不能等同于机器学习，因为从推荐系统实现的角度来看，我们可以使用机器学习也可以使用其他技术和策略。但是广告系统一般会使用机器学习。下表从不同维度说明了搜索、推荐以及广告的异同之处。 所以，广告系统的架构和推荐系统的架构有类似的地方，也有些许区别。下图是一个简单的DSP广告系统架构，图中箭头表示数据流的走向，1是收到一个广告展示请求，8是发出针对此次请求的出价、广告创意等。 其中，枢纽、检索模块和排序模块是广告系统的核心，特征计算系统、计费系统和投放系统是辅助模块。 枢纽：对外提供HTTP服务，接收请求后，依次与特征计算系统、检索模块、排序模块交互，最后返回出价和广告创意等。 检索模块：解决相关性问题，检索出与用户查询相关性较高的广告。相关性较低的广告会影响用户体验和广告效果。 排序模块：解决收益最大化问题，在约束下最大化收益。特征计算系统：实时计算场景（媒体、广告位、上下文、设备等）曝光度、用户的特征，并向其他模块提供实时查询支持。 计费系统：实时处理曝光后媒体返回的数据，以及其他点击、转化等数据，并计算广告费用、剩余预算等。 投放系统：供广告主设置定向条件、创意等，是广告主直接交互的系统。 下图是微博广告架构，基本的广告系统具有类似的框架体系。 2 推荐系统的召回和冷启动广告平台关于广告的两种思考方式：当前推荐的广告的优劣以广告收益为评价标准，还是以用户体验为评价标准。注重广告收益的策略，在推荐广告时会更关注广告人均点击、推广人数等指标。这些指标能有效识别当前推荐的广告是否与用户相匹配。注重用户体验的策略其实更关注广告平台的用户留存。这样看，广告平台其实更像是一个推荐系统。注重广告收益的策略，可能会发生重复推荐同一条广告给一个用户的情况，因此我们应在最大化商业利益的前提下，合理制定触达方式。所以，广告平台应该平衡好广告收益和用户体验。 再回到推荐系统的冷启动策略。前文已经介绍了一些推荐系统冷启动的基本解决方法。在计算广告中，解决冷启动问题通常还有两种方法：一种是利用强化学习的方法。这类方法通常将冷启动问题转化为多臂赌博机问题进行解决，根据用户对广告的反馈，不断调整系统推荐广告的策略。另一种是利用推荐系统中基于内容的推荐方法。这类方法利用用户的描述信息，比如用户的年龄、性别、地理位置、工作、爱好等信息，给用户推荐相应的广告。传统的解决冷启动问题的方法通常在为用户推荐过程中需要进行大量的计算，非常耗时。如何降低推荐过程中计算的复杂度，是我们持续保持关注和待解决的问题。 以Google的广告平台为例，它是业内少数能将多方利益平衡好的公司之一。首先，我们先看看Google广告平台的整体运作逻辑。对于每一个想要加入GA（Google Adwords）平台的广告主来说，GA会要求客户以组为单位，提供相应的广告并选择合适的热词、地区范围等相关信息，以便通过最基本的标签信息获得足够的内容支持。下图展示的是在Google中搜索video games时，右侧出现的与video games相关的广告及其推荐原理解释。我们可以发现Google在冷启动召回时，为了保证搜索界面信息的准确，召回广告主要采用基于关键词的方法。这也是我们之前在广告提供商处提到的提交的相关信息。当在短时间内反复搜索video games，除了第一次之外，并没有再看到广告。由此可见对于非注册用户，GA平台的冷启动广告推荐具有时效性。 再举一个360广告系统召回的例子，召回模块先初步选出广告候选集，然后进入过滤模块，最后进入排序阶段。过滤方法主要包括基于规则、黑白名单、预算控制。在排序阶段，粗排模块对初选的广告候选集按评价函数模型进行打分，但没有精排模块那么复杂，相对比较简单。召回过程中使用的方法如下图所示。 上下文召回包括以下三种类型，第一种是基于图片的，即将图片向量化，通过计算广告商品与图片向量的相似度进行召回；第二种是基于标题的，主要是基于文本分类模型进行召回；第三种是基于位置的，广告主自身设定某个标签区域进行投放，在该区域内进行标签匹配召回。 用户行为召回有以下三种类型，第一种是基于兴趣的，即基于用户历史行为建立用户画像，形成兴趣标签，属于布尔召回；第二种是基于查询的，根据用户的历史查询行为，通过NLP相关模型进行召回；第三种是基于访问行为的，利用广告主回传的用户行为，采用Item CF、ALS、Neural MF等模型进行召回。 深度召回主要是把人群属性、群体特征、上下文特征等结合起来，采用深度学习模型进行召回。 3 ES在推荐系统中的应用ES加入了分布式、分片等特性后越来越多地应用在大型的搜索、推荐以及广告系统中。那么，为什么选择ES呢？究其原因，ES有以下4大特性。 1）可实时分析。ES可以根据业务要求，发挥分布式的优点，尽最大可能实时分析、解析出业务需要的数据。 2）可实时存储。ES在某个主节点保存数据时候，只有当副分片保存成功，才能认为是实时保存成功，并且支持批量保存数据。 3）分布式集群。根据业务需求及当前的搜索量，ES可以横向扩展，支持存储最大PB级的数据，可以提高搜索速度。 4）支持快速搜索。ES可以并发从N台机器的副分片或主分片搜索数据，通过查询机器负载进行组合数据，最终响应请求。 4 推荐系统中NLP的应用在广告的召回过程中，查询理解是一个重要的环节。第4章也归纳梳理了一些对查询理解的基本方法。下图是广告中的NLP算法应用，具体包括两个最基本的任务：用户查询意图的识别和查询重写。 1.用户查询意图识别查询分析过程中需要对查询进行分词，分析分词后的标引项以及标引项的权重。这和搜索中的查询分析完全一致。查询类目识别主要是判定类目，通常这类问题都会转化为文本分类问题来解决。目前，文本分类方法也有很多，比如基于传统统计模型、基于深度神经网络的方法等。关于具体的模型和方法前面的章节中都有讲解，针对不同的业务场景我们选择合适的分类模型即可。下面介绍在使用文本分类方面的经验技巧，这些技巧可能体现在预处理和训练阶段。 （1）文本预处理阶段 1）需要对文本进行泛化，如泛化命名实体。 2）规范文本的长度，可以取所有文本长度的均值或者中位数。 3）构建数据集的词典时，应该注意以下几点。 ·可以取高频词或者过滤次数小于某个阈值的词。 ·可考虑去掉停用词。 ·采用的预训练模型，要尽可能让词典中的词找到对应词向量。 4）中文分词 ·判断是否需要分词，判断选用哪个分词器。 ·分词后可以根据词的长度过滤一些词，降低维度。 ·采用预训练的词向量时，要保证分词后的大部分词语能够出现在预训练的词向量表中，否则词嵌入就相当于是随机初始化，预训练的词向量没有提供任何信息。 5）数据增强 常见的方法有随机删除文本、改变文本顺序、同义词替换等。 （2）模型训练 1）特征工程。传统的机器学习方法根据特征工程可以分为三大类。 ·词袋模型，矩阵维度高且稀疏。 ·向量空间模型，需要考虑文档的频率、互信息、信息熵增益、卡方等。 ·主题模型，利用PLSA/LDA等模型提取文本特征，解释性较好。 2）模型选择。建立好一个基线模型，然后依次选用不同模型做比较。 3）双向RNN模型一般比单向效果要好。 4）处理训练震荡及适当调整学习率等。 2.查询重写 查询重写的第一种方法是做查询扩展。前面的章节中也提到过一些查询扩展通用的方法。这里举一些实际的例子，比如“阿迪运动鞋”中的“阿迪”和“阿迪达斯”是同义关系。所以，我们可以把“阿迪运动鞋”这个查询直接扩展到“阿迪达斯运动鞋”。再举一个例子“小棕瓶眼霜”，这个查询中并没有提到“小棕瓶”属于哪一个品牌，所以可以通过知识图谱建立“小棕瓶”与“雅诗兰黛”品牌的关系，这里可以将“小棕瓶眼霜”直接改写为“雅诗兰黛眼霜”。 第二种方法是相关性分析。相关性分析的方法也有很多，这里再介绍一种查询和商品条目相关性分析的方法，可以抽象成排序学习的问题。如给定查询Q和比对的短文本D，判别相关性的档位，假设相关性档位从0到5逐渐变强，相关性分析问题就可以抽象成如式（12-1）所示的形式。 5 推荐系统中粗排和精排广告从召回到曝光的过程需要经历粗排、精排和竞价及反作弊等阶段。广告粗排框架是对引擎端召回的若干广告进行排序，并将排序的结果进行截断。截断后的候选集会被传给广告精排模块处理。粗排目的是尽可能在候选广告集中找到与流量相关性较高的广告，一般可以有效转化为目标。 CTR是指网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率，即该广告的实际点击次数除以广告的展现量。CTR预估是互联网主流应用（广告、推荐、搜索等）的核心环节，包括Google、Facebook等业界巨头都在对该问题进行研究，国内阿里巴巴、腾讯、百度等一线互联网公司也在做持续研究。精排是使用CTR预估模型进行排序。 为什么CTR预估是互联网计算广告的关键技术环节呢？我们可以把CTR问题抽象成如下的形式P(X=click|query，ad)，广告被展现后有两种可能的结果——点击或不点击，在n次展现中被点击的次数X服从二项分布。根据历史统计，CTR预估可以简化为： 6 推荐系统的评价和优化之前讲述了搜索系统的评价和优化，其中所提到的指标同样适用于推荐系统排序阶段。本节将在此基础上讲述推荐系统的相关实验方法：离线评估、在线评估和主观评估，同时将各个指标嵌入其中，对各个阶段的模型和数据进行评估。 离线评估是工程师拉取线上数据在实验环境训练模型，并进行离线指标评估的过程。该过程不需要各方人员全部参与，可以高效地进行多种模型的调整和优化，但可能导致线上和线下模型表现不一致，无法完全复制到线上环境，无法衡量具体业务指标的波动。在这个过程中，工程师衡量模型表现的指标一般有：1）准确度指标，包括：均方根误差、平均绝对误差、准确率、召回率、AUC以及nDCG等；2）覆盖度指标；3）多样性指标；4）实时性能指标；5）鲁棒性指标。工程师只有对上述指标评估后，才可将模型放到线上环境进行在线实验。 在线实验常用的方法是A/B实验，把旧策略下的流量分出一定的比例给新策略，通过一段时间的测试，根据制定好的业务指标对新旧策略进行比较。在线评估涉及的主要指标有：1）服务响应时间；2）抗高并发能力；3）相关业务指标，主要包括曝光、浏览、下单等业务转化率指标。除了使用A/B实验，常用的还有Interleaving实验框架。该框架相比于传统A/B实验，有较小的样本需求量、较低的实验误差率，但也比传统A/B实验更加复杂，缺乏部分指标的预测能力。所以，读者在使用时需要根据具体需求自行选择。 策略上线后，我们还可以进行主观评估。用户调研是一种常用的主观评估方法。问卷调查是一种重要的用户调查途径，但需要注意调查问卷的设计、用户引导、调查对象选取的范围和数量，从而确保调查结果的可用性等。 总之，在推荐系统的评价和优化过程中，离线评估、在线评估以及主观评估都是必不可少的，在此基础上进行系统的迭代和优化才能更加有效。 7 深度学习在推荐系统应用深度学习在推荐、搜索以及广告中都发挥着举足轻重的作用。在数据驱动的时代背景下，深度学习模型也开始在各个领域发挥作用。广告系统是深度学习的实验田。 下面以阿里为例具体讲一下阿里在排序方面的进化。 在2012年，阿里巴巴就提出了MLR(Mixed Logistic Regression)模型并实际部署到线上系统，同时期的模型还有FM模型。MLR的本质是由多个LR模型组合而成，用分片线性模式来拟合高维空间的非线性模式。当时，只有阿里采用了“大规模离散特征+分布式非线性MLR模型”。MLR模型最大的问题如下。 1）从数百维统计特征到数十亿离散特征，训练架构需要调整。 2）模型学习到兼具拟合能力和泛化能力的范式存疑。 3）这种超大规模数据上的非凸优化问题难以解决。 2015年，MLR模型遇到了发展瓶颈，当数据量增加，训练样本量也逐渐增大，而且引入高阶特征，需要更复杂的模型，代价高。 2016年，阿里巴巴引入了深度学习模型，把基于第一代端到端深度网络模型GWEN引入CTR预估实际应用中，并产生了第一代Deep CTR模型，如下图所示。 从2016年到2017年，阿里巴巴从第一代GWEN模型开始不断进行变革。同时期，工业界也从机器学习的特征工程跨越到深度学习的模型工程，前者是特征驱动，后者是数据驱动。同时，业界提出了很多模型，如PNN/DeepFM/DCN/xDeepFM等。这些模型是一脉相承的思路，即用人工构造的代数式的先验来帮助模型建立对某种认知模式的预设，如LR模型对原始离散特征的交叉组合。 如下图所示，这个时期产生了DIN(Deep Interest Network)和DIEN(Deep Interest Evolution Network)模型。这两个模型都是围绕着用户兴趣建模，切入点是从电商场景观察到的数据特征，并针对性地进行了网络结构设计。DIN模型捕捉了用户兴趣的多样性以及与预测目标的局部相关性；DIEN模型进一步强化了兴趣的扩展以及兴趣在不同维度的投影关系。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第十一章--搜索应用实战","slug":"SearchAndRec-Chapter11","date":"2021-06-11T20:36:22.000Z","updated":"2021-06-11T12:37:07.127Z","comments":true,"path":"2021/06/12/SearchAndRec-Chapter11/","link":"","permalink":"http://renxingkai.github.io/2021/06/12/SearchAndRec-Chapter11/","excerpt":"","text":"1 电商搜索系统的架构设计在电商领域，一个完整的搜索系统的设计需要多年的经验和对整个领域的认知。下面先给出一个搜索系统架构示例，如下图所示。 对于电商公司来说，网站索引商品的量大概在百万级别，因此要求搜索引擎既要搜得准，又要搜得全。搜得准能够提升客户满意度，减少客户流失；搜得全能够给商品带来更多的曝光度，提高转化率。二者最终目的都是要提高用户转化率。下图是电商搜索引擎的架构。 通常，搜索系统可以做以下几方面优化。 1）做好联想输入。好的联想输入能够帮助客户快速找到自己想要的搜索词，不需要输入完整词汇，只需要输入首字母，或者输入拼音就可以快速定位搜索词，进而快速找到自己想要的商品。这种方式可以提升用户满意度，提高转化率。 2）做好分词。分词是搜索中比较重要的一环，是构成倒排索引的基础。分词相对单字索引能够减少索引量，提高统计排序的灵活性。 3）做好纠错。用户在使用搜索时，难免会输入错误，这时候就要对输入进行纠错。我们通常会采用两种纠错方式：一种是固定纠错法，即统计客户输入的词语，找到错误率比较高的词语，以及一些有歧义的词语，并固定设置遇到此类词语就转成纠错词去查询；另一种是拼音纠错法，即对经常输入错误的词语提取拼音，对同音词按照搜索频率排序提示给客户。 4）做好对搜索无结果时的推荐。对于电商搜索来说，搜索无结果经常出现，比如客户输入错误或者没有客户想要的东西，如果仅仅给客户展示白屏，那就相当于放弃了该客户，所以针对这种情况，应有针对性地做一些推荐，比如可以采用查询扩展和用户意图识别的方法来解决。 5）做好商品排序。排序是整个搜索过程比较重要的一个过程。通常，我们可以将排序过程分为几个阶段，在每一个阶段完成一项重要的任务，并在整个排序过程关注转化率。 6）做好搜索业务的支撑平台，比如训练、标注、监控、算法管理与发布等。训练平台提供专门的训练环境，供算法工程师使用；标注平台给运营人员提供专门标注的地方，同时将这些系统打通，实现自动化管理，为后续算法上线提供系统化管理。 7）关注系统性能，提高系统的响应能力及吞吐能力。我们要对系统高并发能力有一定的预判，尽量做好相关预案。线上系统要能经受“双11”这种大型促销活动的考验。 2 ES在搜索系统中的应用首先，我们应该知道为什么选择ES？在构建搜索引擎时，我们通常优先考虑开源的搜索引擎，因为完全自研成本会很高。选择ES的依据是它是一款分布式、高可用的搜索引擎，并且面向文本存储，存储的数据格式是JSON，方便通用。ES不用限制其内部的Schema，插件丰富，而且支持一些机器学习算法。 对于电商领域来讲，第一步是搜索引擎的选择；第二步是看数据，一般是从大数据中搜索数据，通过分析数据解决转化率低的问题；第三步需要实现实时数据的同步，数据的增删改等操作都需要在搜索引擎中实现同步，原始数据也需要实时同步到搜索引擎中；第四步就是实现用户访问。 在使用ES的过程中，也有一些值得注意的地方。对于分布式系统，其会将数据分布到多台机器或者多个节点。查询请求发出后，系统会通过总调度将数据分流到对应的分片上。ES在创建索引的时候就可以指定分片数，如果只指定一个节点会出现数据倾斜的现象，也不利于充分利用机器，因此一般会依据机器数量来指定分片数。查询时需要从每个分片中找到满足对应查询条件的数据，然后在总调度中汇聚。例如，要查询100条数据，需要在分片中查询100次数据，最后在总调度中汇聚成400条数据，再从这400条数据依据优先级取前100条数据，返回到客户端。分片数越多查询效率越低，但是写的性能会提升。在设置分片数量时，我们需要依据业务需求设定。如果分片主要用于读操作就将分片数设置得少一些，如果是写操作就根据数据量匹配分片数，达到最大存储量。 在使用ES的过程中，笔者的一些经验和教训如下。 1）硬件必须要强。公司应尽量使用SSD（固态硬盘），因为SSD读取速度快，能够提升I/O吞吐量。对于高并发情况，内存条主频应尽量保持一致；内存条主频不一致可能使系统运行变慢。 2）索引不能只依靠磁盘，即使是SSD，查询一次的代价也是昂贵的。对于高并发，ES会加载索引到堆外内存，包括倒排索引、正排索引、向量信息。整个查询过程会使用堆外内存查询。如果堆外内存不存在索引，再去磁盘查询，同时写入堆外内存；如果堆外内存已满，ES会替换掉一些不常用的索引。需要注意的是，我们应该把服务器交换分区关掉，否则内存满了会使用磁盘，使系统性能大幅下降。如果有条件，我们可以把所有文件加载到JVM内存，这样查询会更快，但如果JVM内存故障，要保证对刚更新的内存索引进行备份。 3）预热。为了把索引文件充分加载到堆外内存，我们可以把系统需要使用的词典文件加载到JVM内存。 4）注意ES缓存设计。ES缓存分为三种：查询缓存、请求缓存、数据结构的缓存。在设计ES缓存的时候，要注意缓存大小的设置。如果设置得过小，缓存命中率可能比较低，起不到效果；如果设置得过大，可能会导致垃圾回收时间过长。我们可以根据索引大小以及机器配置适当调整缓存大小。 5）建议禁用Source字段。因为开启Source字段，ES会在返回结果时根据查询到的文档ID，找到对应的文档信息，将JSON反序列化成结果对象，这个过程比较消耗CPU。 6）合并段。ES在查询过程中对每一个段都要根据输入词查询一次索引，这个过程会涉及I/O或者堆外内存的交互。ES在每次提交结果时都会生成一个段文件，如果段文件非常多会导致多次循环，严重影响性能，因此可以合并成一个段去请求。但是要注意，段合并非常消耗CPU，建议不要在服务高峰期合并段。 3NLP在搜索系统中的应用NLP在搜索系统中的应用主要包括：搜索意图识别、查询理解、网页内容理解、搜索排序、相关推荐等。排序是一个比较大的专题，这里先看看NLP在搜索系统中的应用。 查询理解的任务是最经典的关于NLP在搜索场景中的应用，主要通过对海量的查询日志、点击反馈日志进行数据挖掘。查询理解任务主要包括中文分词、新词发现、词性标注、句法分析、同义词挖掘、拼写纠错、查询扩展、查询改写等。这些知识点在第4章和第5章也有一定程度的总结和梳理。毫不夸张地说，查询理解是搜索场景中的灵魂。 当然，查询理解也有一些技术上的挑战。比如，针对长尾关键词查询，如果用基于规则的方式处理，工作量大；如果用机器学习的方式处理，没有足够的样本支持。另外，从技术实现角度看，真正做到语义上的召回也是非常有挑战的。 NLP在查询理解上的应用如下图所示。 下图是基于电商的搜索逻辑设计示意图。从图中可以看出，搜索逻辑可以在查询扩展的基础上进行各种演变。这样做的目的是丰富查询条件，并且加强对查询的语义理解。 下面再举一个“猜你喜欢”逻辑设计示意图，如下图所示。如何让搜索引擎能够“猜”到用户的兴趣点是该功能逻辑设计的主要目的。当然让搜索引擎具有“猜”的功能在技术层面上也是比较有挑战的。首先要分析用户的查询内容以及用户的查询行为。用户查询内容的分析可以通过对查询条件进行分析。用户查询行为一般需要经过大数据的统计分析，挖掘用户深层次的需求。 4 商品数据排序算法研究随着我国经济与互联网技术的飞速发展，电商平台上商品的种类也急剧增加。如何让用户在最短的时间内便捷地找到自己感兴趣的商品，并且对商品有效排序已经成为每一个电商平台必须面对的问题。 在开始讨论排序算法之前，我们先回顾一下电商平台数据的运作流程，这对于合理地使用排序算法是尤为重要的。如果说排序是将现有的结果按照用户感兴趣的程度由高到低排列，那么用户当前的兴趣和召回数据的极限范围就是排序算法的重要依据。 首先，对于确定用户兴趣，我们通常会选择使用用户画像、用户行为以及热点推荐的方式来综合评价。其次，我们还要确定用户的最近行为反映出了用户的哪些特质，如性格、爱好等。最后，我们需要确定最近热门的商品，这些商品中有哪些可能和当前用户的兴趣强相关。基于这三步，最终产出优质的召回数据。 对于用户当前兴趣的确定，我们可以将不同的属性按照其内部的互异性、分割后的特质所含样本数量，以及样本时效性等维度进行分析。首先是获取用户基本属性，可以依据用户长期以来在平台上产生的数据获得。这里需要格外注意网格的大小。如果把用来确定用户基本属性的每一个特征当作一个维度，那么由性别和收入特征产生的简单网格可以是2×X，其中X是收入的划分间隔，而2是性别的划分。具体要收纳多少个特征作为基础维度，而每一个特征又要如何分割，这既要考虑每一个特征分割后的区分性或者说互异性，又要考虑网格内用户的数量。 比如，给0～1岁、1～3岁、3～5岁与5～8岁的孩子在选择教育的时候，其目标商品是有明显区别的。那么在当前电商场景下，就可以将其细分到不同的子分类上，然后根据用户的数量确定维度，例如，如果当前1～3岁的男孩数量较少，那么可以酌情删除当前分割的年龄区间或与其他近似特征分割后的区间进行合并。 关于时效性，我们可以分为两个不同的维度：用户侧的时效性以及商品侧的时效性。这里我们先以用户侧的时效性举例。一个18岁的用户在5～6月的时候感兴趣的商品可能是《三年真题五年模拟》。而2个月后，他还会对于这些商品感兴趣吗？如果我们能确定他是一位高三考生，那么2个月后，他对这种商品的兴趣度就会下降。 这里是对基础数据准备的简单样例的讲解。对于召回数据的理解也同样如此。召回数据的理解是建立在平台总体情况与当前页面情况基础之上的。在不考虑商业逻辑的前提下，我们首先要清楚当前页面在当前平台上到底会召回哪些范围的数据，它们又包含哪些特征，这些特征在接下来的排序阶段是否可用，等等。 下一步我们进入模型的选择。无论是复杂度较低的规则排序，还是排序算法模型，首先要权衡的是算法的性能。很简单的一个逻辑就是保证用户体验。除了参考性能指标之外，很多时候我们优先选择可解释性强的模型，即模型本身对于排序结果的计算方式是易于理解的，比如规则模型、逻辑回归模型、树模型及其部分衍生模型等。对于如何选择合适的模型去满足不同的需求，就需要大家对于模型本身的数学逻辑和应用场景等有着更深入的理解，这里就不再过多赘述了。 而当确定了模型后，我们就需要考虑评估模型性能和优化模型。 5 搜索排序的评价及优化搜索排序方法的评价在前面章节中已经做了详细描述。这里需要讲的是具体指标的选取和实验方法。在算法实现阶段，每一个模型都是单一目标的实现。但是，由于业务场景的需要，线上评估有可能是多目标融合结果的评估。 离线实验对搜索排序的评价主要侧重两个方面：效率评价和效果评价。效率评价一般是指搜索系统在响应时间和空间消耗方面的评估。一个系统的响应时间受诸多因素的影响：硬件、数据量、数据结构等。对于搜索排序模型，我们要根据实际应用场景对模型进行选择，按照系统能够给出的时间限度，选择具有合适的复杂度的模型、特征等。对于搜索系统而言，不同的文档存储结构所用内存区别较大，如正排索引和倒排索引所需内存空间不同，布尔检索和正文检索所需内存空间也不同。 效果评价是对模型排序后的结果进行评价，即对比排序结果和标准结果的差异。评估指标包括：准确率、召回率、F值、平均准确率以及nDCG指标等。准确率描述最终的推荐列表中有多少比例是发生过的用户–物品评分记录；召回率描述有多少比例的用户–物品评分记录包含在最终的列表结果中；F值是同时考虑准确率和召回率的综合指标，使用调和平均数的计算方式强调较小值的重要性；平均准确率考虑到相关文档的位置信息，在召回率从0到1逐渐提高的过程中，对每个相关文档位置上的准确率进行相加；在关注排序靠前的评价指标中，使用最多的是nDCG指标，其对文档的相关度进行多种等级的打分，同时综合考虑文档的位置信息。 ROC曲线是解决正负样本比例不均衡以及分类阈值选择的一种方法。下图中有三条ROC曲线，我们可以通过AUC对其进行量化。AUC为ROC曲线下半部分的面积。AUC越大，模型效果越好。 6 深度学习在搜索系统中的应用1.将深度学习应用于“扫一扫”功能 谈到深度学习最成功的应用莫过于在图像与语音方面的应用了。在搜索框上的“扫一扫”功能正是利用了图像识别技术，这颠覆了传统的以文字作为输入的方式。 2.将深度学习用于搜索引擎的召回和排序阶段 在召回阶段，我们可以利用序列标注模型进行查询理解的分析工作，也可以利用Fasttext或者其他模型进行意图识别。在排序阶段，我们可以使用DSSM模型，也可以使用其他深度学习模型。另外，我们还可以通过深度学习模型主动学习特征，利用深度学习自动扩展查询条件，等等。 7 电商搜索系统中的SEM搜索引擎营销（Search Engine Marketing，SEM）简单地说就是基于搜索平台的营销方法，主要分为自身调整和外部购买。为了提高商品曝光度，我们会考虑通过一些手段提高商品的召回率，这就是我们常说的SEM。与SEM相关的还有SEO和手机平台的ASM和ASO。 比如，我们在应用商店单纯地搜索“淘宝”，可以看到很多与“淘宝”不太相关的内容，如“唯品会”和“美团”等。阅读“美团”的内容发现，美团在内容中有描述：“相关应用：大众点评、百度糯米、饿了么、淘宝、京东……”。 在尝试了所有相关App的搜索之后，我们发现美团或多或少地存在于相关的搜索结果中，所以猜测“美团”App在内容描述上极大概率地应用了SEM。当然由于商业保密等，我们无法知道应用商店具体使用的是什么样的搜索策略。由苹果官方公布的影响搜索结果的因素可知：搜索结果只与用户行为和文本相关性有关。 再比如，下图所示为SensorTower提供的2017年5～8月游戏与非游戏App下载来源比例。我们可以发现来自搜索的比例要远远大于来自推荐的比例。2018年，随着应用商店的大规模改版，也有人称搜索比重有所下降。就现在情况来看，应用商店中App下载来源依旧主要依赖于搜索。而考虑到应用商店所提供的搜索服务，ASO主要是通过对搜索规则的解析尽可能地提高召回率，进而提高App曝光度。 下图是在京东搜索“手机”的SEM结果，我们能清晰地看到在结果的右下角有一个广告的标记。这也是我们常见的一种基于电商平台的SEM方法，即置顶。当产品方购买了相关热词之后，与当前用户搜索内容相关的结果展示中一定会展示这类购买了“置顶”广告的产品。这类SEM其实是一种流量出售的方式。通常，一件商品的购买量在存量充足的情况下，我们可以简单地理解为：购买量=流量×浏览系数×服务系数×商品本身属性。以这个公式为例，我们看出商品的购买量除了与商品本身的属性有关，还与商品的曝光量即流量和商家的服务有关。这也是现代商品社会中广告基于搜索平台的一种新型表达。 流量购买型的SEM方式也是多种多样的，不仅限于置顶一种，还有系数调整方法。而在搜索排序中，所有的SEM策略统称为商业策略。 前面的章节讲解搜索排序的时候讲到，展示给用户的最终排序结果应该是经过模型排序之后再依照商业逻辑调整之后的结果。所以，SEM策略应该独立于整个排序结果之外，但又与搜索结果相关。通常，SEM有两种策略，即添加和重排。添加即将与当前搜索内容相关但不存在于当前结果中的数据强行添加至排序结果的预设位置。值得注意的是，添加的内容是否与搜索内容相关是需要仔细斟酌的。在一些搜索平台中，平台自身可能允许SEM购买方选择关键词或热词。在这种情况下，只要热词与添加内容相关就会触发SEM规则。如果没有控制好SEM购买方选择的热词，同时在关系判断时又过于简单地依赖于热词，很有可能损失当前排序结果的精度。 相比于添加，重排的方法会显得更温和，也更有可能保证搜索排序的精度。同时，就商业逻辑而言，重排效果可能不如添加方法，主要表现在受众和价格上。因此，如何选择合适的SEM方法与触发SEM规则，就成了SEM最重要的考验。常见广告位、专题推送后台逻辑如下图所示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第十章--搜索引擎工具","slug":"SearchAndRec-Chapter10","date":"2021-06-10T22:41:45.000Z","updated":"2021-06-10T14:42:26.857Z","comments":true,"path":"2021/06/11/SearchAndRec-Chapter10/","link":"","permalink":"http://renxingkai.github.io/2021/06/11/SearchAndRec-Chapter10/","excerpt":"","text":"本章主要介绍几款主流的搜索引擎工具：Lucene、Solr、Elasticsearch，因为在搜索、推荐和广告等场景中会越来越多地使用这些搜索引擎工具。 1.Lucene简介Lucene是一种高性能、可伸缩的信息搜索引擎，最初由鼎鼎大名的Doug Cutting开发，是基于Java实现的开源项目。Lucene采用了基于倒排表的设计原理，可以非常高效地实现文本查找；在底层采用了分段的存储模式，大大提升了读写性能。Lucene作为搜索引擎，优点是具有成熟的解决方案，低成本，快速上手；支持多种格式索引。缺点是不能友好地支持分布式扩展，可靠性差等。所以在实际应用中，我们需要根据特定场景评估Lucene是否适合于当前场景。 1.1 Lucene的由来及现状 为了更好地理解Lucene，我们先看一下全文索引。Lucene搜索架构如上图所示。 数据包括结构化和非结构化数据。结构化数据是指具有固定格式或有限长度的数据，如数据库、元数据等。非结构化数据是指长度不固定或无固定格式的数据，如邮件、HTML、Word文档等。因此，根据数据分类，我们可以把搜索分为两种：对结构化数据的搜索，如对数据库的搜索，可以使用SQL语句；对非结构化数据的搜索，如使用Windows搜索、grep命令。百度、Google等搜索引擎对非结构化数据搜索采用的方法包括顺序扫描。所谓顺序扫描，即一个文档一个文档查，逐行扫描，直到扫描完成为止。对于一个500GB或者更大的源文件，按照这种方式处理，可能需要花费几个小时甚至数天的时间。简单来说，这种方式只适合于小文档搜索，直接方便。顺序扫描处理非结构化数据很慢，而处理结构化数据速度非常快，我们是否可以考虑把非结构化数据转换成结构化数据？那么具体怎么转换呢？举个例子，根据新华字典检字表的音节和部首，我们可按照拼音排序，根据每一个读音指向字的详细页面，迅速定位。按照这种方式，我们先对搜索词进行分词，然后找每个词对应的文本，接着每个词取一个交集，最后获得查询结果。这种先建立索引，再对索引进行搜索的过程就叫作全文检索。 Lucene中常用的核心术语如下。 1）Term：索引里最小的存储和查询单元，对于英文来说一般指一个单词，对于中文来说一般指一个分词后的词。 2）词典（Term Dictionary）：也叫作字典，是Term的集合。查找词典中的数据的方法有很多种，每种都有优缺点，比如哈希表比排序数组的检索速度更快，但是会浪费存储空间。 3）倒排表是Lucene的核心思想。一篇文章通常由多个词组成，倒排表记录的是某个词在哪些文章中出现过。倒排表结构如下图所示。词典和倒排表是实现快速检索的重要基石。词典和倒排表是分两部分存储的。倒排表不但存储了文档编号，还存储了词频等信息。 4）正向信息是原始的文档信息，可以用来做排序、聚合、展示等。 5）段是索引中最小的独立存储单元。一个索引文件由一个或者多个段组成。Lucene中的段有不变性，也就是说段一旦生成，只能有读操作，不能有写操作。 Lucene主要模块如下图所示。 1）分析模块一般由Token和Filter组成。Token是分词器，Filter一般是同义词、大小写转换过滤器等，主要负责词法分析及语言处理，也就是我们常说的分词。通过该模块可获得存储或者搜索的最小单元 2）索引模块主要负责索引的创建工作，包括建立倒排索引、写入磁盘操作。 3）存储模块主要负责索引的读写，主要是针对文件，抽象出和平台文件系统无关的存储信息。 4）查询解析主要负责语法分析，将查询语句转换成Lucene底层可以识别的语句。Lucene的语法分析主要基于JavaCC。JavaCC是词法分析器以及语法分析器的生成器 5）搜索模块主要负责对索引的搜索工作，后续会有一个详细的搜索过程描述。 6）相似度模块主要负责相关性打分和排序。 1.2 Lucene创建索引过程分析Lucene创建索引过程如下图所示。 1）添加文档。这个过程会处理没有写入磁盘的数据，遍历需要索引的文件，构造对应的文档和字段，生成DefaultIndexingChain。DefaultIndexingChain是一个默认的索引处理链。后续生成正排表以及倒排表都是在这个链里完成的。 2）构建正排表。在该过程中，我们会使用差值存储、压缩算法将文档写入正排表。 3）构建倒排表，流程如下。 ·获取原文档，将原文档传递给分词组件，将文档分成单独的单词，去除标点符号。 ·去除停用词。所谓停用词，是语言中最普通的一些词，没有特殊含义，一般情况不能作为搜索的关键词，因此创建索引时候会被去掉，以减少索引量。 ·添加同义词 ·将得到的词元传给语言处理组件，然后由语言处理组件对得到的词元做一些相关处理，比如大写变小写、单词缩减为词根形式。 ·语言处理组件得到的结果被称为词，将词传递给索引组件，并利用得到的词创建一个词典。词典是每个词和词所在的文档ID。 ·对词典按字母进行排序，合并相同的词。 ·DefaultIndexingChain、processDocument()方法主要用来构建正排信息，而针对每个字段的processField()则通过一系列的操作，构建出倒排信息。 4）写入磁盘。触发写入磁盘文件的是DocumentsWriterPerThread(DWPT)的flush()方法。触发时间可能是以下条件：超过MaxBufferedDocs限制；超过RAMBufferSizeMB限制；人为设置flush()或commit()；MergePolicy触发。 经过以上4步处理，Lucene就可以生成一个最小的独立索引单元——段。一个逻辑上的索引（表现为一个目录）由N个段构成。 1.3 Lucene的搜索过程解析Lucene的搜索过程解析如下。 1）对查询语句进行词法分析、语法分析、语言处理。词法分析主要用来识别单词和关键字，语法分析主要根据查询语句的语法规则形成一棵语法树。 2）搜索索引，得到符合语法树的文档，根据得到的文档和查询语句的相关性，对结果进行排序。 3）计算词的重要性。词的权重表示词对文档的重要程度。越重要的词权重越大，因而权重在计算文档相关性上发挥很大的作用。判断词之间的关系，从而得到文档相关性可用向量空间模型（Vector Space Model，VSM）。 TF-IDF是Lucene默认使用的打分公式，是一种统计方法，用于评估一个字词对一个文件集或一个语料库中其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，同时会随着在语料库中出现的频率成反比下降。Lucene自6.0起使用BM25算法代替了之前的TF-IDF算法。 BM25算法将相关性当作概率问题。相对于TF-IDF，BM25限制TF值的增长极限、平均了文档长度。如图10-5所示，BM25中的TF值有一个上限，文档里出现5~10次的词会比那些只出现一两次的词与搜索相关性更高，但是，文档中出现20次的词几乎与那些出现上千次的词与搜索的相关性几乎相同。 结合代码的详细搜索过程如下。 1）初始化Indexsearch。在该过程中，词典被加载到内存，这步操作是在Directory-Reader.Open()函数中完成的。而完成加载的类叫作BlockTreeTermsReader，每次初始化IndexSearch都会将.tim和.tip加载到内存中，这些操作是很耗时的。 2）Query生成Weight。首先Weight类会将Query重写。重写的目标是将Query组装成一个TermQuery。最典型的，prefixquery会被重写成多个TermQuery。接着计算查询权重，Boost算法通过TF-IDF打分机制，计算出Term的IDF值、QueryNorm值，返回Weight。 1. /** 2. * Creates a normalized weight for a top-level &#123;@link Query&#125;. 3. * The query is rewritten by this method and &#123;@link Query#createWeight&#125; called, 4. * afterwards the &#123;@link Weight&#125; is normalized. The returned &#123;@code Weight&#125; 5. * can then directly be used to get a &#123;@link Scorer&#125;. 6. * @lucene.internal 7. */ 8. public Weight createNormalizedWeight(Query query, boolean needsScores) throws IOException &#123; 9. query = rewrite(query); 10. Weight weight = createWeight(query, needsScores); 11. float v = weight.getValueForNormalization(); 12. float norm = getSimilarity(needsScores).queryNorm(v); 13. if (Float.isInfinite(norm) || Float.isNaN(norm)) &#123; 14. norm = 1.0f; 15. &#125; 16. weight.normalize(norm, 1.0f); 17. return weight; 18. &#125; 3）由Weight生成Scorer。首先根据Term获取TermsEnum，然后根据TermEnum获取DocsEnum，最后生成Scorer。代码如下： . for (LeafReaderContext ctx : leaves) &#123; 2. final LeafCollector leafCollector; 3. try &#123; 4. leafCollector = collector.getLeafCollector(ctx); 5. &#125; catch (CollectionTerminatedException e) &#123; 6. continue; 7. &#125; 8. BulkScorer scorer = weight.bulkScorer(ctx); 9. if (scorer != null) &#123; 10. try &#123; 11. scorer.score(leafCollector, ctx.reader().getLiveDocs()); 12. &#125; catch (CollectionTerminatedException e) &#123; 13. &#125; 14. &#125; 15. &#125; 4）给每个文档打分，并添加到结果集。该过程是最耗时的，需要对每个文档进行打分，并将结果放入生成的容器中。代码如下： 1. static void scoreAll(LeafCollector collector, DocIdSetIterator iterator, TwoPhaseIterator twoPhase, Bits acceptDocs) throws IOException &#123; 2. if (twoPhase == null) &#123; 3. for (int doc = iterator.nextDoc(); doc != DocIdSetIterator.NO_ MORE_DOCS; doc = iterator.nextDoc()) &#123; 4. if (acceptDocs == null || acceptDocs.get(doc)) &#123; 5. collector.collect(doc); 6. &#125; 7. &#125; 8. &#125; else &#123; 9. final DocIdSetIterator approximation = twoPhase.approximation(); 10. for (int doc = approximation.nextDoc(); doc != DocIdSetIterator. NO_MORE_DOCS; doc = approximation.nextDoc()) &#123; 11. if ((acceptDocs == null || acceptDocs.get(doc)) &amp;&amp; twoPhase.matches()) &#123; 12. collector.collect(doc); 13. &#125; 14. &#125; 15. &#125; 16. &#125; 2 Solr简介Solr是基于Apache LuceneTM构建的快速、开源的企业搜索平台，是一个Java Web应用，可以运行在任何主流Java Servlet引擎中。Solr服务器的主要构成如下图所示。 Solr基于已有的XML、JSON格式和HTTP标准，提供简单的类似REST的服务，使得Solr可以被不同编程语言的应用访问。其可以使用Zookeeper实现简易分片和复制，统一配置。为了提高查询速度和处理更多的文档，Solr通过索引分片来实现分布式查询。为了提高吞吐量和容错能力，Solr可以为每个索引分片增加副本，同时把所有的索引复制到其他的服务器，搭建成一个服务器集群，提高吞吐量；也可以通过缓存来提高查询速度，达到近实时查询，并写入硬盘以达到索引持久化。Solr具有高可靠性、可扩展性和容错性，可提供分布式索引、复制和负载均衡查询、自动故障转移和恢复、集中配置等。Solr为很多互联网站点的搜索和导航功能提供支持。 2.1 Solr特性1）高可靠性。Solr有三个主要的子系统：文档管理、查询处理和文本分析。每一个子系统都是由模块化的管道构成的，通过插件方式实现新功能，这意味着我们可以根据特定的应用需求实现定制。 2）可扩展性。Solr汲取了Lucene速度方面的优点，但因CPU的I/O原则，单台服务器终会达到并发请求的处理上限。为了解决这个问题，Solr提供灵活的缓存管理功能进行扩容，以及通过增加服务器实现增容。 Solr可扩展性体现在两个维度：查询吞吐量和文档索引量。查询吞吐量是指搜索引擎每秒支持的查询数量。文档索引量是指索引文档的大小。为了处理更多文档，我们可以将索引拆分为很小的索引分片，然后在索引分片中进行分布式搜索。 3）容错性。如果索引分片中其中一个索引分片服务器断电，会导致Solr无法继续索引文档和提供查询服务。因此，为了避免此种情况出现，Solr对每一个索引分片添加副本，当其中一个索引分片服务器发生故障时，可以启用副本来索引和处理查询。 2.3 Solr的核心功能1）复制模式。直到Solr7，SolrCloud能够在集群出现问题的时候提供可靠的故障切换，同时要求副本必须保持同步。 2）自动缩放。自动缩放是Solr一个新功能套件，让SolrCloud集群更加简单和自动化。核心是为用户提供一个规则语法，以便定义如何在集群中分发节点、首选项和策略，以便保持集群平衡。 3）无须手动编写Schema。Schemaless模式是一组Solr功能，它们一起使用时，只需索引数据即可快速地构建Schema。 以下这些Solr功能都是通过solrconfig.xml文件实现的。模式管理：在运行时通过Solr接口进行架构修改，这需要使用支持这些更改的SchemaFactory。更多详细信息，请参阅SolrConfig中的SchemaFactory定义部分。 字段猜测：对于未定义的字段，自动根据FieldValue猜测字段属于哪种类型（Boolean、Integer、Long、Float、Double、Date）。 基于字段猜测自动添加字段到Schema中：未定义的字段会根据FieldValue对应的Java类型自动添加到Schema，请参阅Solr字段类型。 建议关闭Schemaless模式。官网不推荐使用此功能，因为如果字段类型不正确，索引也就不能正常查询（例如存储汉字，我们如果不指定FiledType，就无法正常索引到汉字文档）。 4）结构化非文本字段类型。示例中除了文本外，其他字段都是Solr中常用的字段类型。下面对这些常用的字段类型进行讲解。 Solr中常用字段类型的类图如下图所示。 3 Elasticsearch简介Elasticsearch是一个分布式的开源搜索和分析引擎，适用于对所有类型的数据搜索。Elasticsearch是在Apache Lucene的基础上开发而成，由Elasticsearch N.V.（即现在的Elastic）于2010年首次发布。Elasticsearch以其简单的REST风格接口、分布式特性、速度快和可扩展性而闻名，是Elastic Stack的核心组件。Elastic Stack是适用于数据采集、充实、存储、分析和可视化的一组开源工具。人们通常将Elastic Stack称为ELK Stack（代指Elasticsearch、Logstash和Kibana）。目前，Elastic Stack包括一系列丰富的轻量型数据采集代理，这些代理统称为Beats，可用来向Elasticsearch发送数据。 3.1 Elasticsearch的核心概念1.集群 一个集群包含一个或多个节点，用于保存数据。这些节点联合起来提供索引和搜索能力。集群的名称很重要，因为一个节点要加入一个集群，需要配置集群名称。在实际应用中，我们需要确保不同网络环境所使用的集群名称是不同的，否则会导致节点加入其他集群。比如你可以使用logging-dev、logging-stage、logging-prod分别搭建开发、过渡、生产环境。集群只有一个节点，也能正常提供服务。Elasticsearch的集群如下图所示。 2.节点 在集群中，一个节点是一个单独的服务，用来存储数据，为集群的索引和搜索提供支持。集群中的节点也有唯一标识，默认在节点启动的时候会随机指定一个通用唯一标识码（Universally Unique IDentifiter，UUID）。默认情况下，每个节点配置集群名称为Elasticsearch。当在同一个网络环境中，默认启动一些节点，这些节点会组装成一个名为Elasticsearch的集群。如果不使用默认名称，可以为其指定一个名称。节点名称对于集群管理也是很重要的。 3.索引 相对于关系型数据库，索引对应数据库实例。索引中包含许多特征类似的文档。例如，索引可指向用户数据，也可指向产品目录。一个索引需要指定一个名称（必须全部小写）。执行索引、搜索、修改和删除操作时，需要指定对应的索引名称。在一个集群中，我们可以创建多个索引。 4.类型 相对于关系型数据库，类型对应数据库表。一个索引中可以定义多个类型。一个类型可以管理索引中符合特定逻辑的一部分数据。一般来说，类型可定义具有公共字段的文档。例如创建一个博客平台，并且使用一个索引存储所有数据，在这个索引中，可以定义一个类型来存储用户数据，另一个类型来存储博客数据，还可以创建一个类型来存储评论。 5.文档 相对于关系型数据库，文档对应数据库表。文档是能够被索引的基础单元。文档可以存储用户信息，也可以存储产品信息。Elasticsearch中的文档使用JSON格式来存储数据。需要注意，文档必须被索引或分配给索引的类型中。 3.2 Elasticsearch的核心功能1.近实时 Elasticsearch索引是由段组成的。查询一条数据要经过分钟级别的延迟才能被搜索到，瓶颈点主要在磁盘。持久化一个段需要利用fsync()函数确保其写入物理磁盘，但因为涉及I/O操作，比较耗时，不能每索引一条数据就执行一次，所以引入了轻量级处理方式——FileSytemcache，即先将写入Elasticsearch的文档收集到索引缓冲区并重写成一个段，然后再写入FileSytemcache，之后经过一定间隔时间或者外部触发才写入磁盘，但是写入FileSytemcache后就可以打开和查询，保证短时间查询到数据。所以，Elasticsearch是近实时的。 2.分片或副本 在实际应用中可能存在这样的场景，索引存储超过了节点的物理存储容量。为了解决这些问题，Elasticsearch提供了为索引切分成多个分片的功能。当创建索引的时候，我们能够定义索引被分割成多少个分片。每一个分片支持独立索引，可以分配到集群中任何一个节点。使用分片有两个重要原因：允许水平分割文档，分布式存储；多个节点提供查询，提高了吞吐量。一个查询发出后去哪些分片请求数据，这些对于用户来说都是透明的。在网络环境中，节点或分片中的数据可能丢失。Elasticsearch提供了故障转移功能，就是副本。Elasticsearch允许为一个分片创建一个或多个副本。分片和副本又称为主/副分片。使用副分片有两个重要原因：主/副分片不会存储在一个节点中，因此副分片可防止主分片数据丢失导致查询不能继续；当进行搜索的时候，允许搜索所有的副分片，提高了搜索性能。一个索引可以分成多个副分片。每个索引都有主分片（索引切割后的分片，又称原始分片）和副分片（从原始分片复制出来的）。主分片数量和副分片数量在创建索引的时候可以被指定。当索引创建后，我们可以改变副分片数量，但是不能改变主分片数量。因为某个文档分配在哪个分片，是在设置分片数量的时候就已经确定的。如果改变主分片数量，可能导致查询为空。 3.选主算法 Elasticsearch使用的是Master-slave方式，相对于分布式哈希表，可以支持每小时数千节点的加入和离开。但是在相对稳定的网络中，Master模式比较适合。那么在Master-salve模式下，怎么选主算法呢？其实，选择一个合适的主算法对于Elasticsearch是至关重要的。Elasticsearch使用的是Bully算法，功能强大，灵活性高。相对于Paxos算法，Bully算法实现简单，假定所有节点都有唯一ID，对ID排序，任何时候的当前主流程都是参与集群的最高节点ID。Bully算法的特点是易于实现，但是在最大节点ID不稳定的场景下会出现集群假死的情况。Elasticsearch通过推迟选举直到当前的主流程失效的方法来解决假死问题。但是，另一个问题又来了——脑裂。Elasticsearch通过法定得票人数过半来解决脑裂问题。 4.高可用 Elasticsearch使用乐观锁控制并发。因为乐观锁的使用场景是读多写少，而Elasticsearch恰好符合这一场景，如果按照悲观锁的策略，会大大降低吞吐量。Elasticsearch基于版本号进行乐观锁并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突。对于写操作，一致性级别包括quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。即使大多数分片可用，也可能存在因为网络故障使写入副分片失败的情况，此时分片将会在不同的节点重建。对于读操作，replication可以设置为sync，这样主分片和副分片都搜索完成才返回结果。如果将replication设置为async，可以通过设置请求参数_preference为primary来查询主分片，确保文档是最新版本。 4 搜索引擎工具对比1.Solr和Lucene Solr与Lucene并不是竞争对立的关系，而是Solr依存于Lucene，因为Solr底层的核心技术是使用Lucene来实现的。Solr和Lucene的本质区别有以下三点。 1）Lucene本质上是搜索库，不是独立的应用程序，而Solr是应用程序。 2）Lucene专注于搜索底层的建设，而Solr专注于企业应用。 3）Lucene不负责支撑搜索服务所必需的管理，而Solr负责。 所以，Solr是Lucene面向企业级搜索应用的扩展。 2.Solr和Elasticsearch 1）Solr查询语句比Elasticsearch查询语句简单。 2）Solr利用Zookeeper进行分布式管理，而Elasticsearch自身带有分布式协调管理功能。 3）Solr支持更多格式的数据，Elasticsearch仅支持JSON格式。 4）Solr官方提供的功能较多，Elasticsearch注重核心功能。 5）对于一般的搜索，Solr好于Elasticsearch，但在处理实时搜索时效率不如Elasticsearch。 6）Solr专注于文本搜索，Elasticsearch常用于查询、过滤和分组分析统计。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第九章--推荐系统的评价","slug":"SearchAndRec-Chapter09","date":"2021-06-09T23:39:45.000Z","updated":"2021-06-09T15:41:00.036Z","comments":true,"path":"2021/06/10/SearchAndRec-Chapter09/","link":"","permalink":"http://renxingkai.github.io/2021/06/10/SearchAndRec-Chapter09/","excerpt":"","text":"1.推荐评估的目的推荐系统评估与推荐系统的产品定位息息相关。推荐系统是信息高效分发的手段，用于更快、更好地满足用户的不确定需求。所以，推荐系统的精准度、惊喜度、多样性等都是评估的指标。同时，推荐系统要具备稳定性。稳定性可以通过实验评估。在实现方面，是否能支撑大规模用户访问等也是推荐系统评估指标。 推荐系统评估的目的是从上述维度评估推荐系统的实际效果及表现，从中发现优化点，以便能够更好地满足用户需求，为用户提供更优质的推荐服务，同时获取更多的商业利益。 2.推荐系统的评价指标对于一个推荐系统，我们可以从用户、平台方、标的物、推荐系统本身4个维度进行评估，如下图所示。 1.用户维度 用户维度是指从用户的角度出发，用户喜欢什么，系统就推荐什么。从用户维度看，我们可以从准确度、惊喜度、新颖性、信任度、多样性、体验流畅度这几个方面进行评估。 1）准确度指推荐的物品是不是用户需要的。以视频推荐为例，如果用户点击观看了推荐的电影，说明推荐的电影是用户喜欢的，推荐准确度高。这里的准确度主要表示用户的主观体验。 2）惊喜度指推荐给用户一些完全与他们历史喜欢物品不相似，但是用户却喜欢的物品。这些推荐可能超出用户的预期，给用户一种耳目一新的感觉。 3）新颖性指推荐给用户一些应该感兴趣但是不知道的内容。比如，用户非常喜欢某位歌星的歌曲，如果推荐给他一部电影，假设用户从未听说该歌星演过电影，且用户确实喜欢这部电影，那么当前的推荐就属于新颖推荐。 4）信任度指用户对推荐系统或者推荐结果的认可程度。比如，用户喜欢头条推荐的内容，就会持续点击或浏览系统的推送内容。 5）多样性指推荐系统会提供多品类的标的物，以便拓展用户的兴趣范围及提升用户体验，如下图所示。比如，系统推荐了不同风格的音乐，且用户体验效果更好，则认为该系统具有大量的乐曲。 6）体验流畅度指系统与用户交互时，用户体验不会出现卡顿。从系统角度看，要求推荐系统性能更可靠，提供服务更流畅，不会出现卡顿和响应不及时的情况。 2.平台维度 平台维度是指从标的物提供方和用户角度出发，通过衡量双方利益来评价整体效益。因此，我们既可以从标的物提供方进行评价，也可以从用户方的商业价值进行评价，同时可以针对双方进行评价。评价的指标包括商业指标，如大部分互联网产品通过广告赚取的收益。除了关注商业指标外，我们还需要关注用户留存、用户活跃、用户转化等指标。所以从平台维度看，我们可以从以下三类指标评价推荐系统：第一类是用户行为的相关指标；第二类是商业变现的相关指标；第三类是标的物提供方指标。 （1）用户行为的相关指标 用户行为的相关指标包括以下相关指标。比如，PV（Page View）指标（页面访问率或者页面点击率、页面的刷新次数）；日活或月活（周期内活跃用户的数量）指标；留存率（下一个周期留存继续使用的用户）；转化率（期望的行为数与行为总数的商）。 （2）商业变现的相关指标商业变现的相关指标可由涉及的具体商业指标衡量。衡量推荐系统商业价值，需要从产品的盈利模式谈起。目前，互联网产品主要有4种盈利模式：游戏（游戏开发、游戏代理等）、广告、电商、增值服务（如会员等），后三种模式都可以通过优化推荐技术做得更好。推荐技术的优化目标可以以商业表现为最终目标，比如考虑提升系统的曝光与转化，提升用户的留存率、活跃度、延长停留时长等。 （3）标的物提供方指标标的物提供方指标指与商家相关的指标。平台方需要服务好用户和标的物提供方（比如视频网站是需要花钱购买视频版权的）。大部分互联网产品会通过广告赚取收益。 3.标的物维度 当然，我们也可以从标的物视角去评价推荐系统，比如通过覆盖率和挖掘长尾用户的能力去评估。 4.推荐系统维度 推荐系统维度指从自身出发去衡量整个系统的优劣。前面章节在介绍推荐系统时，强调了推荐算法在推荐系统中的重要作用，因此评价推荐系统可以从评价算法出发。在评价过程中，我们可以考虑从以下几个方面进行。 1）准确度是指核心推荐算法的准确程度。在推荐场景下，无论有监督学习还是无监督学习，机器学习模型都有一定的解决实际问题的能力。所以，我们可以从模型解决实际问题的能力等进行评价。比如，在推荐排序中，我们可以使用准确率、召回率和nDCG等指标来评判推荐算法准确度。简单来说，准确率反映的是模型正确预测的结果，召回率反映的是仅考虑预测结果中正召回结果占正确结果的比例，而nDCG考量了最终的排序结果与原始排序结果的差异性。 注意，这里的准确度和用户视角的准确度可以一致也可以不一致。用户视角的准确度强调主观感受，而这里强调客观存在。 2）实时性是指用户的兴趣随时间变化而变化，推荐系统能做到近实时推荐是非常重要的。 3）鲁棒性是指推荐系统及推荐算法不会因为“脏”数据而脆弱，能够为用户提供稳定的服务。从宏观上讲，推荐系统依赖于用户行为日志；从微观上讲，推荐算法也依赖于用户行为日志。如果用户行为日志产生偏差，推荐系统不会因为“脏”数据影响最终的推荐效果。比如，我们可以在系统中引入知识图谱，用知识图谱来纠正因用户行为日志产生的偏差，减小“脏”数据对推荐效果产生的负面影响。 4）推荐系统响应推荐服务的时长以及推荐服务的稳定性。推荐服务的稳定包括推荐是否可以正常访问，推荐服务是否挂起等。 5）高并发能力是指推荐服务在较高频次的用户请求下能正常稳定地运行。补充：在实际生产中，我们遇到的问题往往非常复杂，并且为了让模型能更好地解决当前问题，需要用不同的方法去评价推荐模型。 补充：在实际生产中，我们遇到的问题往往非常复杂，并且为了让模型能更好地解决当前问题，需要用不同的方法去评价推荐模型。 比如，如果在一个应用场景中采用了单文档排序方法，那么我们会偏向于使用准确率与召回率去评价模型。当然，我们也可以选择使用NDCG去评价模型。但是，它对于排序顺序并不敏感，所以评价结果可能不会太好。如果针对强调排序顺序固定或极其敏感的场景，通常建议使用nDCG。 2.1 RMSE和R方 与所有的均方根方法一样，RMSE方法对于异常值比较敏感。通俗地讲，RMSE方法更能准确地评价同样准确率下的不同模型，能够有效地判定哪一个预测结果更可靠。在场景上，如果不苛求模型的准确度，我们希望模型的预测结果更可靠，那么RMSE方法则更适用。 2.2 MAP和MRR MRR（Mean Reciprocal Rank，平均倒数排名）是依据排序的准确度，对查询请求响应的结果进行评估。 2.3 其他相关指标前文介绍了很多方法去评价模型，但是这些评价结果很可能会随着数据的变动而变动，所以，我们就需要一个可以无视数据波动的模型效果评价指标。如果我们把召回设定为TPR，则有，以FPR为横坐标，TPR为纵坐标，随着阈值的变动可以得到一个用来评价分类器性能、在(0，0)与(1，1)之间的线段。 这里要特殊说明一下，以二分类模型举例，分类器训练之后得到一个可以利用固定阈值和样本预测值进行分类的模型。在预测值固定不变的情况下调整阈值，那么分类结果也会随之变动。同理，这个过程中TPR和FPR也会随之变动。将不同阈值下的TPR和FPR的结果展示在坐标系上，最终就可以得到ROC曲线。 AUC则是ROC曲线靠近横坐标侧的面积。因为ROC曲线均为凸曲线，所以AUC的值在0.5~1之间浮动。AUC其实描述的是模型的性能，AUC越大，当前越存在一个合适的阈值使得模型的分类效果越好。另外，这里还要说明一点的是，为什么ROC曲线总是凸曲线？ROC其实取决于TPR和FPR之间的变换关系，如果预测结果为凹曲线，我们只需要调换正负预测关系，则凹曲线自然就变换成了凸曲线。对于AUC低于0.5的模型，我们更偏向于通过调整数据和参数等其他手段，以保证ROC曲线呈现凸曲线。一旦AUC低于0.5，以二分类模型举例，我们可以理解为当前模型一定程度上比随机猜测的结果还要差。 最后，为什么我们要使用ROC和AUC评价指标？很重要的原因是ROC的横纵坐标分别是FPR和TPR，得益于其计算方式，两者对于样本正负比例的变化是不敏感的。这种情况下，ROC与AUC指标更能集中突显模型分类性能的好坏，而几乎不受其他因素的影响。 3 推荐系统的评估实验方法前面的章节已经介绍过推荐系统架构一般包含召回和排序两个阶段。推荐算法存在于推荐系统的两个阶段。推荐算法本质上是一个机器学习问题。首先，我们需要构建推荐算法模型，选择合适并且效果好的算法模型，将算法模型部署到线上推荐业务，利用算法模型来预测用户对标的物的偏好，通过用户的真实反馈，包括是否点击、是否购买、是否收藏等来评估算法效果；同时，在必要的时候和用户沟通，收集用户对推荐系统的真实评价。我们可以根据推荐业务流将推荐系统评估分为三个阶段：离线评估、在线评估和主观评估，如下图所示。与此同时，我们可以将之前介绍的评价指标嵌入各阶段。 3.1离线评估离线评估是算法人员在线下进行实验来检查算法、数据、系统等是否正常的方法。离线评估的主要过程如下。1）从数据仓库提取线上数据，分别用于线下训练和测试。2）对数据进行预处理，并分为训练集和测试集。3）在训练数据集上进行模型训练，在测试集上进行测试。4）计算测试集上模型训练效果，按照一定的指标评估离线效果是否达到上线标准。 离线评估有三大优点。1）不需要对系统有实际控制权。2）不需要用户和内容提供方实时参与。3）在性能满足的前提下，可以大批量地测试多种模型，利于调整及优化算法模型。 但是，离线评估也有一些缺点。1）无法计算部分核心商业指标。2）预测结果与真实结果存在一定差距。 通过离线评估，我们可以将适合评价推荐算法的具体指标应用到离线评估过程中，具体涉及的评估指标如下所示。 1.准确度准确度评估的主要目的是事先评估推荐算法模型是否精准，为选择合适的模型上线提供决策依据。在这个过程中，主要是评估推荐算法是否可以准确预测用户的兴趣偏好。我们可以根据三种不同的范式评估系统的准确度。 第一种范式是将推荐算法看作预测问题。预测对标的物的评分值（比如0～10分）。解决该类型问题的思路：预测出用户对所有没有产生行为的标的物的评分，按照评分从高到低排序。这种思路下，推荐算法可作为评分预测模型。 第二种范式是将推荐算法看成分类问题。推荐可以看作是二分类，将标的物分为喜欢和不喜欢两类；也可以看作是多分类，每个标的物就是一个类，根据用户过去行为预测下一个行为。解决该类型问题的思路一般是：预测出某个标的物属于某个类别的概率，根据概率值进行评估；也可以类似第一种思路，排序形成Top N推荐。 第三种范式是将推荐算法看成一个排序学习问题，利用排序学习的思路来做推荐。这类问题需要学习一个有序列表。 推荐系统的目的是为用户推荐一系列标的物，命中用户的兴趣点，让用户消费标的物。所以，在实际推荐产品中，一般都是为用户提供N个候选集，称为Top N推荐，尽可能地召回用户感兴趣的标的物。上面这三类推荐算法范式都可以转化为Top N推荐。 面针对上述三类推荐范式，介绍一下对应的评估指标。 1）针对评分预测模型，评估推荐准确度的指标主要有：RMSE（均方根误差）、MAE（平均绝对误差）。 2）针对分类模型，评估推荐准确度的主要指标有：准确率（Precision）、召回率（Recall）。关于准确率、召回率的描述，前面的章节已经讲过。简单地说，准确率是指为用户推荐的候选集中有多少比例是用户真正感兴趣的或者在推荐的候选集中有多少比例是用户消费过的标的物；召回率是指用户真正感兴趣的标的物中有多少比例是推荐系统推荐的。一般来说，推荐的标的物越多，召回率越高，准确率越低。当推荐数量为所有标的物时，召回率为1，而准确率为0。 3）针对排序学习模型，评估指标主要有MAP、NDCG、MRR等。 2.覆盖率对于推荐系统，覆盖率都可以直接计算出来。 3.多样性用户的需求容易受外界因素影响，所以系统在推荐时需要尽量保证推荐的多样性。在实际中，我们可以通过聚类标的物和增加不同类别的标的物来保证推荐结果的多样性。多样性指标又分为个体多样性指标和整体多样性指标。个体多样性可用用户推荐列表内所有物品的平均相似度衡量： 4.实时性一般来说，推荐系统的实时性可以分为4个级别：T+1级、小时级、分钟级、秒级。响应时间越短，对整个系统设计、开发、工程实现、维护、监控要求越高。我们可以按照以下的原则设计推荐系统。1）利用用户碎片时间推荐产品，因此推荐系统需要做到分钟级。用户消耗标的物的时间很短。2）用户需要较长时间消耗标的物，因此推荐系统应考虑更长的响应时间，做到小时级或者T+1更合理。3）广告系统有必要做到秒级响应，大多数推荐系统没有必要做到秒级响应。 5.鲁棒性鲁棒性指标主要是评价推荐系统的稳定性。为了提升推荐系统的鲁棒性，我们需要注意以下几点。1）尽量选用鲁棒性较好的模型。2）细化特征工程，通过算法和规则去除“脏”数据。3）避免垃圾数据的引入。4）完善日志系统，有较好的测试方案。 3.2在线评估通常，在离线评估完成后，我们就可以进行在线评估。现在，业界通用的在线评估方式是A/B实验，即新系统与老系统同时在线并分配不同的流量，在一段时间内对比同级别的核心指标，以确定新旧系统的优劣。具体的A/B实验方法如下图所示。 在线评估可以分为两个阶段。 1）第一阶段是从推荐服务上线到用户使用推荐产品阶段，该阶段用户通过使用推荐产品触发推荐服务。这个阶段的评估指标有实时性、稳定性和抗高并发能力。响应及时且稳定是衡量推荐系统优劣的重要指标。这个指标可以通过用户请求推荐服务时，推荐接口提供数据反馈的时间来衡量。响应时间越短越好，一般响应时间要控制在200ms之内。抗高并发能力是指当用户规模很大，或者在特定时间点有大量用户访问，推荐接口能够承载的最大压力。如果同一时间点有大量用户调用推荐服务，推荐系统高并发的压力将非常大。推荐服务在上线前应该做好压力测试，我们可以采取一些技术手段来提高接口的抗并发能力，比如增加缓存等。在特殊情况下，我们应该对服务进行分流、限流、降级等。 2）第二阶段是通过用户行为相关指标等来评估。我们需要在这一阶段站在平台方角度来选用指标，这些指标主要有用户行为相关指标、商业化指标等。以一个简单用户行为漏斗为例，评估示意图如下图所示。 重要的用户行为指标有转化率、购买率、点击率、人均停留时长、人均阅读次数等。一般情况下，用户行为是一个漏斗。我们需要知道从漏斗上一层到下一层的转化率。通过转化率来衡量推荐系统带来的最终价值。 总之，在数据量足够的情况下，我们可以通过线上的A/B实验从各个方面评估推荐系统的效果。 为了能够让推荐模型快速上线，我们需要快速衡量该模型带来的具体价值，即需要一种快速上线的实验方法进行支持。所以，下面再介绍一个快速线上实验框架——Interleaving实验框架。 Netflix认为新的线上评估方法应该适用于两个阶段：第一阶段可以进行批量测试且新方法的敏感度应该更高，即可以使用较小的样本达到传统A/B实验效果。第一阶段的结果可以预测新方法在第二阶段的表现。下图展示Interleaving方法与传统A/B实验的对比。 如上图所示，Interleaving在第一阶段可以快速地筛选算法，最终在A/B实验阶段去验证，这样整体的用户样本的消耗将大大缩小。那么，Interleaving具体是怎么实现的？下图展示了Interleaving第一阶段与传统A/B实验的区别。 如上图所示，Interleaving的排序与传统A/B实验区别就在于混排。通常来说，我们在做A/B实验的时候，虽然不知道当前的推荐结果是来自A组还是B组，但是可以确定当前的推荐结果一定来自相同的组。而Interleaving可以确定的是当前的推荐结果来自A、B组，且交叉互异。这里需要明确的是，Interleaving中A、B组在混排的时候应该平均地给两组以同等概率的优先权，即A1、B1的排序方式数量应该相当于B1、A1的排序方式数量。这种情况下，我们再统计来自不同组点击的核心指标，即可以在较小的样本下初筛算法。统计结果如下图所示。 如上图所示，Interleaving方法达到5%误差率时对样本的需求大致是同等情况下A/B实验的百分之一。 虽然我们这里介绍了Interleaving方法也体现了优于A/B实验的地方，但是Interleaving也有着较为严重的缺点。首先，与传统的A/B实验相比，Interleaving要复杂得多。同时，受限于Interleaving的混排方法，不少指标在Interleaving第一阶段是无法测试的，比如用户留存率。因为所有的推荐栏都是由A、B组混排得到的，所以对于用户整体的影响是无法分组评估的，这也是为什么Interleaving需要第二阶段的A/B实验。这里的问题就在于如果网站关心的核心指标是整体指标如用户留存率，那么A/B实验可能依旧是一个较好的选择，即使它可能需要更长的时间和更多的用户样本。所以，最终选择何种方法进行何种测试还是需要大家在生产中视具体情况而定。 3.3 主观评估用户调查的流程基本上可以整理为：分析现有问题、确定提问和引导过程、选择备选用户、收集用户评价等。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第八章--推荐系统的主要算法","slug":"SearchAndRec-Chapter08","date":"2021-06-01T11:14:40.000Z","updated":"2021-06-01T03:15:02.202Z","comments":true,"path":"2021/06/01/SearchAndRec-Chapter08/","link":"","permalink":"http://renxingkai.github.io/2021/06/01/SearchAndRec-Chapter08/","excerpt":"","text":"本章介绍思路如下：从协同模型推广到矩阵分解，从第5章介绍的LR模型推广到其他线性模型如FM和FFM，从第5章介绍的树模型和集成模型推广到其他树模型和集成算法模型，以及深度学习模型Wide&amp;Deep、Deep FM。 1.矩阵分解基于协同的模型都属于近邻分析模型，但是近邻分析模型又存在一些明显的问题。物品之间的相关性、信息量不因向量维度的增加而线性增加。由于矩阵的维度可能包含类别特征one-hot编码后的属性，矩阵内部的数据较为稀疏，而矩阵维度的变化对近邻分析结果的影响很大，因此一般采用矩阵分解的方式求解近邻分析模型。 矩阵分解的本质是将一个稀疏且维度较高的矩阵拆解为维度较低的两个矩阵的乘积，如下图所示。 假设用户对物品的评价矩阵是Rm×n，即有m个用户，n个物品，则可以分解为：R_{mxn}=P_{mxk}Q_{kxn} 在了解了矩阵分解原理之后，我们再来看看奇异值分解是如何利用矩阵分解解决问题的。 1.1奇异值分解 SVD算法的学习过程如下。 1）用户对物品的评分矩阵R，每条数据是一个训练样本。 2）将R分解为矩阵P和矩阵Q，并随机初始化元素值。 3）用矩阵P和Q计算预测后的评分。 4）计算实际评分和预测评分之间的误差。 5）按照公式（8-6）和（8-7）更新参数并更新其中每个元素。 6）重复步骤3到步骤5，直到达到停止条件（设定迭代次数）为止。 SVD还有一个变种是SVD++算法。前面已经讲到了推荐系统中的用户冷启动问题。比如，主动点评电影或者美食的用户少，显示反馈比隐式反馈少，此时可以考虑利用用户行为的隐式反馈来弥补。我们可以把用户历史行为中的隐式反馈和用户属性加入用户评分矩阵，这就相当于对原来的矩阵进行了扩展，则有： 1.2交替最小二乘矩阵分解算法可以利用随机梯度下降法，也可以利用交替最小二乘（Alternating Least Squares，ALS）法。ALS算法的核心思想是：将矩阵R（用户对于商品的兴趣矩阵）分解为P（用户对于隐特征的兴趣矩阵）和Q（商品与隐特征的兴趣矩阵的转置矩阵）。在式（8-1）中，因为 ALS算法的学习过程如下。 1）初始化随机矩阵Q中的元素值。 2）假设矩阵Q已知，则损失函数中的y为已知量，对损失函数中的x求偏导，求解矩阵P。 3）得到矩阵P后，假设矩阵P已知，则损失函数中的x为已知量，对于损失函数中的y求偏导，求解矩阵Q。 4）交替进行步骤2和步骤3，直到误差达到设定条件为止。 如果对隐式反馈的矩阵分解中的ALS算法进一步改进，如进行加权交替，则ALS算法被称为Weighted-ALS（加权交替最小二乘）法。这里举一个例子进行分析。如果你买了一件比较昂贵的大衣，之后购买心仪的鞋子和裤子的计划可能就会被搁置，但你可能依然会关注这些商品，对应到行为上可能就是多次点击和查看。行为发生的次数是对行为置信度的反映，是加权的一种形式。 1.3贝叶斯个性化排序矩阵分解的本质是预测一个用户对一个物品的偏好程度。在实际应用时，通常会利用矩阵分解的结果进行排序。前面的章节也讨论了排序学习的相关方法，包括单文档方法、文档对方法和文档列表法。SVD和ALS均属于单文档方法。单文档方法只考虑了每个物品，且每个物品是一个孤立的点。 单文档方法的缺点在于只能收集到正样本，所以在求解过程中往往将有缺失值的样本作为负样本，这会降低预测结果准确率，至少对那些数据有缺失值的用户是否喜欢某物品的判断会产生偏差。而贝叶斯个性化排序（Bayesian Personalized Ranking，BPR）算法可以解决该问题。 BPR算法是基于贝叶斯的个性化排序，服从两个假设。 1）每个用户之间的行为偏好相互独立，即用户u在物品i和物品j之间的抉择与其他用户无关。 2）同一个用户对不同物品的排序相互独立，即用户u对物品i和物品j的排序与其他物品无关。 BPR算法关心的是物品对于用户的相对顺序，构造的样本是用户、物品1、物品2以及两物品的相对顺序，如下表所示。 BPR算法学习过程是：在得到推荐分数后，计算正样本和负样本的分数之差，通过这个差值反映用户对于不同物品的偏好程度。正样本是用户看到后有隐式反馈的物品，负样本是用户看到后没有任何反馈的物品，比如用户u对物品1和物品2的推荐分数差为： BPR算法参数的训练过程如下。 2.线性模型逻辑回归模型是基础的线性模型。这里我们会对其他推荐场景中使用到的线性模型进行梳理，主要介绍因子分解机（Factorization Machine，FM）及其变种FFM(Field-aware Factorization Machine)。 下面先介绍一下FM产生的原因：使用逻辑回归模型存在一些问题。逻辑回归模型中大量的特征需要通过人工获得，而且逻辑回归模型认为特征之间不存在依赖关系，但是现在中并非如此。 如果我们考虑最朴素的特征组合，即考虑特征之间的二阶笛卡儿乘积，那么会导致特征维度太多。并且这样组合后的特征并不都是有效的，且组合后的特征非常稀疏，简单说就是这些特征组合后不便于找到符合样本的特征，不足以支持训练出有效的参数。最朴素的特征组合模型表达式如下： 2.1FM模型因子分解机是由Steffen Rendle于2010年提出的一种基于矩阵分解的机器学习算法。目前，该算法广泛地被用到推荐系统以及广告预估模型中。逻辑回归模型认为特征是相互独立的，但是在实际情况下特征之间是存在依赖关系的，因此需要进行特征交叉。 FM的主要目的是解决稀疏特征下的特征组合问题。针对式（8-22）中出现的问题，FM把ωij优化成两个隐因子的向量的点积&lt;vi，vj&gt;形式，如式（8-23）所示： 举一个简单的例子，如果特征A和特征B在一些样本中一起出现过，特征B和特征C在一些样本中一起出现过，那么特征A和特征C无论是否在样本中一起出现过，仍然是有一些联系的。在式（8-23）中，vi是第i维特征的隐向量，隐向量的长度为k(k&lt;&lt;n)，包含k个描述特征的因子， 下图所示是FM模型，图中每一行表示一个特征向量和预测的目标结果。第一个框表示用户矩阵，包括3个用户{Alice(A)，Bob(B)，Charlie(C)}，是one-hot编码，属于稀疏矩阵；第二个框表示电影矩阵，包括4部电影{Titanic(TI)，Notting Hill(NH)，Star Wars(SW)，Star Terk(ST)}，是one-hot编码，属于稀疏矩阵；第三个框是其他人对上面4部电影的评价矩阵，归一化特征；第四个框是用户在一个月内评价的次数，也是one-hot编码，属于稀疏矩阵；第五个框表示用户对上一部电影的评价。 2.2FFM模型FFM把相同特征归于同一个场（Field），交互捕捉不同场之间的数据特征也比较重要。FM中一个特征只对应一个向量，而在实际场景中不同场的特征交互时应该使用不同的向量，这就是FFM（Field-aware FM）的提出动机。FM可以看作是FFM的一个特例，把所有的特征都归属于一个场。所以，FFM模型如下： 3.树模型搜索和推荐至少要分两个阶段：召回和排序。在召回阶段，因为处理的数据量较大，要求处理速度快，所以使用的模型一般不能太复杂，而且特征不需要太多。但是在排序阶段，因为处理的数据一般较少，所以模型要足够精确，可以选择稍微复杂的模型，使用更多的特征进行训练。树模型在排序阶段便是一个不错的选择。我们还可以把弱分类器集成起来组合成一个功能强大的分类器。本节将继续介绍树模型以及集成模型。 3.1决策树模型决策树算法是一种归纳分类算法，它通过对训练集的学习，挖掘有用的规则，对新数据集进行预测。它属于有监督、非参数学习算法，对每个输入使用该分类区域的训练数据计算得到对应的局部模型。决策树模型的基本算法是贪心算法，以自顶向下递归的方式构建决策树，如下图所示。贪心算法是在每一步选择当前状态下最优的路径。 我们可以用以下几种方法构建决策树。 1.ID3算法 ID3算法的核心思想是最大化信息熵增益。所谓最大化信息熵增益，即每次进行下一次分裂时，计算出所有类别对应当前特征的熵，选择能够使得信息熵增益最大的那一个特征类别进行下一步的分裂。假设有一组数据，设D为某一个特征类别，则根据熵的定义可以得到D的熵为： 其中，pi表示第i个类别在整个训练元组发生的概率，在离散随机过程中，可以用i出现的数量除以整个数据的总数量n作为估计值。 由于初始数据可以划分的类别不止一项，于是我们需要对已经划分为D类别的数据再次分类。假设此次的类别为A，则类别A对数据集D划分的条件熵为： 2.C4.5算法尽管ID3算法能够帮助决策下次分裂特征，但其本身存在一个问题：一般会优先选择有较多属性值的类别，因为属性值多的类别相对属性值少的类别有相对较大的信息熵增益。C4.5算法则使用增益率（Gain Ratio）作为选择分支的准则，同时引入分裂信息（Split Information）来惩罚取值较多的分类。其定义为： 3.CART算法 CART假设决策树是二叉树，内部节点特征的取值为“是”和“否”，左分支特征取值为“是”，右分支特征取值为“否”。这样的决策树等价于递归地二分每个特征，将输入空间（即特征空间）划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在给定的输入条件下确定输出的条件概率分布。 CART算法由以下两步组成。 1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。 2）决策树剪枝：通过验证数据集对已生成的树进行剪枝并选择最优子树，这时以损失函数最小作为剪枝的标准。 CART决策树的生成是递归地构建二叉决策树的过程。CART决策树既可以用于分类，也可以用于回归。对于分类而言，CART以基尼系数最小化准则进行特征选择，生成二叉决策树。 CART生成算法如下：切分点作为最优特征与最优切分点。依据最优特征与最优切分点，从现节点生成两个子节点，将训练数据集依据特征分配到两个子节点中。 4）对两个子节点递归地调用步骤1~3，直至满足停止条件为止。 下面再介绍一下CART树剪枝。 剪枝方法的本质是在树模型庞大的叶子节点中，挑选对于模型整体影响过量或不重要的部分，将其乃至之后可能出现的分类整体从模型中删除。删除节点的好处在于：提高了对于同类问题的泛化能力，同时由于剪去了部分中间树叶节点，提高了训练速度。在树模型中，常用的剪枝方式为前剪枝和后剪枝。 前剪枝，也叫预剪枝，是在决策树构造的时候同时进行剪枝。 前剪枝过程如下。 1）按照判断信息熵下降的方式（所有决策树的构建方法都是在无法进一步降低熵的情况下停止创建分支的），设定特征选择的阈值。 2）对每一个特征A，计算其带来的信息不确定性下降的程度，并与事先设定的阈值进行对比，若大于阈值则作为新的特征加到树中，否则舍弃。 3）对所有的特征重复步骤2，直至遍历所有特征。 这种方法存在明显的缺点，就是在不同的模型中，甚至不同的问题中，模型的训练者很难精确地给定阈值。过低的阈值可能会导致剪枝效果不明显乃至基本无效，而过高的阈值又可能导致模型学习能力较差。即使单独调参，由于变量太多，在不断实验过程中，即使最后能找到一个较为适合的阈值，也不可避免地会消耗大量的时间。所以，前剪枝虽然在树模型中应用普遍，但是其表现仍不如后剪枝。 后剪枝本质是对子节点的合并。其原理在于如果子节点合并，熵的增量小于一个范围，则将两个节点合并，且后续节点也重新标示为当前节点的属性。那么，前、后剪枝的最大区别在于后剪枝的熵的判断是基于全局的，而前剪枝的熵的判断其实是基于当前的。两者对于熵的变化的判断能力是完全不同的，导致表现能力不同。 后剪枝过程如下。 1）按照判断信息熵下降的方式，计算每个节点的经验熵。 2）递归地从树的叶子节点往上回缩。计算叶子节点回缩到父节点之前和之后的损失函数值，如果回缩之后的损失函数值小于回缩之前的损失函数值，则进行剪枝。 3）对所有的叶子节点重复步骤2，直至不能继续为止。下面举例介绍CART树的剪枝。 CART树的剪枝主体可以分为两部分，即子树序列的生成以及交叉验证。 1）子树序列的生成：找到一个中间节点，并将后续的所有子节点与叶子节点退回到这个中间节点，这样当前的中间节点就成为一个新的叶子节点，当前新的模型就是原始树模型的一个新的子树模型。而由所有叶子节点由下至上地生成所有子树模型即原始树模型的子树序列。 2）交叉验证：依赖所有子树模型的表面误差增益率（即误差的增加速率），选取多个节点组成的子树与交叉验证集合进行验证，选取误差最小的子树作为最优树的结果输出。 下图所示为所有子树序列T0～Tn的生成过程，其中包含所有可能的剪枝情况。 3.2集成算法模型1.GBDT 梯度提升迭代决策树（Gradient Boosting Decision Tree，GBDT）是一种Boosting算法。Boosting算法的核心思想是：利用前一轮迭代的误差更新训练集的权重，校正前一轮迭代被错误分类的样本，下一轮迭代会将重心放在上一轮分错的样本上。GB（Gradient Boosting）是一个算法框架，即可以将已有的分类或回归算法放入其中，得到一个性能强大的算法。在GB框架中，最常用的学习器是决策树，二者结合则为著名的GBDT算法。GBDT在函数空间利用梯度下降法进行优化。其基本思想是沿着梯度的方向，构造一系列的弱分类器，并以一定权重组合起来，形成最终决策的强分类器。 2.GBDT+LR 一棵树的表达能力很弱，不足以表达多个有区分性的组合特征。多棵树的表达能力更强一些。RF（随机森林）是由多棵树组成的，但预测效果不如GBDT。GBDT+LR模型融合的思想来源于Facebook公开的论文。这篇文章的结论是GBDT+LR效果要优于GBDT和LR各自单独的模型效果。 在这个模型中，GBDT任务是生成高阶组合特征。GBDT生成N棵树，每棵树上都能从根节点走到叶子节点，到了叶子节点非0即1（点击或者不点击）。把每棵树的输出看成一个组合特征，取值非0即1。树i有Mi个叶子，相当于有Mi个组合特征。每棵树采用one-hot编码，一共有(M求和)个维度的新特征，然后将这些新特征作为向量输入逻辑回归模型，得到最终结果。 下图是Facebook公开的GBDT+LR模型示例，图中有两棵树，左树有三个叶子节点，右树有两个叶子节点，最终的特征为5维向量。对于输入x，如果它落在左树第一个节点，编码为[1，0，0]；如果落在右树第二个节点，则编码为[0，1]，所以整体的编码为[1，0，0，0，1]，将该编码作为特征输入LR模型中进行分类。 3.XGB和LGB省略 4.深度学习模型4.1 Wide&amp;Deep模型推荐系统中有一个极具挑战的问题，就是需要让系统同时具有记忆和泛化能力。记忆能力的实现需要系统学习大量物品和特征的共现率，然后利用这些共现率挖掘历史数据的相关性。其在实现上需要对一系列宽泛的跨产品特性进行转换。记忆的优点是可解释性强，缺点是与用户已执行的操作项目直接相关。泛化能力的实现需要系统基于相关性转移，探索之前很少出现或从未出现过的新的交叉特征。其在实现上需要进行更多的特征工作，而且模型越深可能越有效。泛化的优点是可以提高推荐项目的多样性，缺点是当查询矩阵稀疏且秩高时，难以有效地学习低维表示。针对记忆和泛化能力的优劣之处，我们提出Wide和Deep相结合的方式，如下图所示。 前面曾介绍过集成模型，以及XGBoost在工业实践中取得的优异成绩。那么，Wide &amp; Deep模型和集成模型有哪些异同点呢？集成模型中每个模型是单独训练的，Wide &amp; Deep模型是联合训练并且同时优化所有参数。Wide &amp; Deep模型训练方法如下图所示。 Wide&amp;Deep模型，即广度和深度兼顾的模型，其基本思想在于，深度学习模型本身虽然有着较好的泛化能力，但是对于样本提供的直观特点记忆能力较弱；而广度模型，虽然对于训练样本本身的记忆较强，但是缺乏较好的泛化能力。结合两者，同时训练的Wide &amp; Deep模型，由于最终的预测结果是由Wide部分与Deep部分耦合得来的，所以有着更强的表现效果。 1.AdaGrad算法AdaGrad其实是对学习率进行了约束。该算法是将每一个参数的每一次迭代的梯度取平方累加后再开方，然后用全局学习率除以开方后的值，作为学习率的动态更新。 2.FTRL算法 4.2 Deep FM模型随着推荐系统的广泛使用，基于CTR预估的推荐方法被广泛应用。而对于CTR推荐方法来说，最重要的就是理解用户行为背后的隐含的特征。在不同的场景下，低阶组合特征与高阶组合特征都可能对模型产生影响。因此，通用且方便快捷地提取有效的组合特征是CTR模型进化的主要方向。 FM考虑将特征交叉，这样就可以通过每一维特征的隐式变量内积来提取组合特征。虽然在理论上，我们可以无限度地去提取高维特征，但是考虑到模型的计算复杂度，一般不超过二阶的组合特征。这样往往可能错过更多有效的高阶组合特征。 DNN模型通过one-hot编码方式将各种复杂的离散特征扁平化处理成一维向量特征，以便学习理解。随着网络深度的拓展，DNN模型对高维特征的提取也更为有效。但是，由于我们追求的是方便、快捷、高效且通用的方法，因此希望在备选特征不明确的情况下，DNN模型有自动挑选特征的能力，能兼顾所有特征。在现实工作中，这些都会导致one-hot编码后，输入特征过大，网络模型参数过多，大大降低模型的性能，增加训练和使用成本。同时，DNN模型对于不同阶的特征是无法兼顾的。也就是说，随着对高维特征的提取，低维特征将无法有效地影响深度网络的输出结果。 在这种情况下，Deep FM方法应运而生。总体来说，Deep FM模型更像是FM模型与DNN模型的融合。一方面，该模型参考了FFM算法的思想，将不同的特征分到不同的场，然后利用一个全连接层对过大的特征进行压缩。压缩后的特征作为输入，可以有效地控制网络参数。另一方面，将FM计算后的低阶特征组合与DNN计算后的高阶特征组合作为输出，可以更直观有效地表述低阶特征组合与高阶特征组合对于最终结果的输出影响。 由此可以看出，Deep FM既保留了FM对于低阶特征的有效组合和特征筛选功能，又保留了DNN对于高阶特征的挑选功能，同时又避免了输入特征过大的情况，有效地解决了FM、FFM、DNN所带来的问题。 Deep FM可以看作是将Wide &amp; Deep模型中LR模型换成了FM模型。Deep FM包含两部分：神经网络部分与因子分解机部分，分别负责高维特征的提取和低维特征的提取。这两部分共享同样的输入。Deep FM结构示意图如下图所示。 在稀疏特征层，所有特征按照既有的场分组，并分别按照给定的k（即隐向量长度）计算全连接层。所以，全连接层的长度为mk，其中m是特征的场数量。对于FM侧，在经过稠密嵌入层之后，FM层有了来自稀疏特征层准备求和的、带有待训练权重的原始特征，也有了来自稠密嵌入层准备交叉内积的、权重为1的隐向量特征。而对于DNN侧，所有的输入数据均来自全连接层，之后进入正常的隐藏层进行计算。最终，将来自FM侧与DNN侧的结果利用sigmoid激活函数结合，作为模型总的输出结果。 自此，Deep FM算法流程结束，而中间训练的过程中，参数的迭代与之前的FM和DNN并无差异。 在同类高阶特征的提取上，Deep FM有效提高了运算速度；在低阶特征组合与高阶特征组合的使用上，Deep FM更为有效、便捷甚至易懂；在特征工程上，Deep FM更是省去了大量烦琐的工作。当然，Deep FM也并非完美无缺，作为同时训练FM与DNN的结合模型，虽然使用了全连接层压缩输入向量，但模型训练的耗时仍然很长。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第七章--推荐框架及原理","slug":"SearchAndRec-Chapter07","date":"2021-05-19T22:35:50.000Z","updated":"2021-05-19T14:36:15.094Z","comments":true,"path":"2021/05/20/SearchAndRec-Chapter07/","link":"","permalink":"http://renxingkai.github.io/2021/05/20/SearchAndRec-Chapter07/","excerpt":"","text":"1.推荐系统的框架及运行推荐系统关注的三大核心问题，分别是预测、排序和可解释性。预测主要是推断用户对物品的喜好程度。排序是对已经推断出的结果进行排序。可解释性是指对推荐的结果给出合理的解释，甚至可以通过关系图谱的方式展示。 1.1 基本框架一个推荐系统大致可以分为4层，分别为离线层、存储层、近线层和在线层。离线层：不使用实时数据，不提供实时服务。存储层：顾名思义，就是负责存储数据和索引。近线层：使用实时数据，但是不保证实时服务。在线层：使用实时数据，保证实时服务。当然，我们可以把4层的推荐系统简化，去掉近线层从而演变成三层的推荐系统。下图所示是一个典型的Web推荐架构。 最近，比较火热的推荐系统是一种基于信息流的推荐系统。这里的信息流也叫Feed流或者兴趣Feed。顾名思义，Feed流就是将内容按照个人的兴趣组织在一起。基于信息流的推荐系统又可以分为两大类：一类是基于聚合信息流的架构，另一类是基于社交动态信息流的架构。两类基于信息流的推荐系统如下两个图所示。 基于聚合信息流架构借鉴了搜索引擎的架构，在技术上需要一定改造。图7-2所示架构可以划分成几个模块：日志收集、内容发布、机器学习、信息流服务、监控报警。日志收集是所有排序训练的数据来源，要收集的最核心数据是用户在信息流上的行为数据，用于机器学习更新排序模型；内容发布就是用推或者拉的模式把信息流内容从源头发布到受众端；机器学习是利用收集的用户行为数据训练模型，然后为每一个用户即将收到的信息流内容打分；信息流服务是为信息流的展示前端提供服务接口；监控报警是系统的运维标配，保证系统的安全和稳定等。 比较上面两图的架构发现，基于社交动态信息流和基于聚合信息流的推荐系统的不同之处在于产生内容的方式不同，数据分发时所依据的数据来源不同。 虽然基于聚合信息流的推荐系统会逐渐演化成基于社交动态信息流的推荐系统，从图中可以看出两种信息流的架构并不完全一样。我们也可以抽象出一些共有的架构部分。 1.2 组件及功能典型的Web推荐架构主要由4部分组件组成，分别是推荐服务、存储系统、离线学习和在线学习。 推荐服务：用户从Web服务器上获取推荐请求，然后获取系统推荐的物品信息。 存储系统：主要作用是存储必要的数据和索引。比如存储用户特征，包括用户画像数据和用户行为数据；存储物品特征，主要包含物品的属性等；存储推荐算法模型的参数以及物品的索引。 离线学习：利用用户数据进行大量学习，由于通常需要学习的数据量大而且耗时长，因此这部分组件一般在离线环境中运行。 在线学习：利用用户的即时数据，不断实时更新一些模型参数，并逐步对模型进行调整。 1.2.1 推荐引擎是如何工作的前文已经讲解了推荐系统的三大核心问题，同时我们知道推荐算法在整个推荐系统的地位和作用是相当重要的。下面学习推荐引擎是如何工作的。如图7-4所示，推荐引擎从一个大的结果集中通过协同过滤模型或者一些相关性模型或算法进行结果召回，然后把召回的结果集进行排序。排序阶段又可以细分为粗排、精排以及再排序更为细致的阶段。推荐引擎根据不同的推荐机制并利用数据源中的一部分用户数据，分析出一定的规律或者直接预测用户对其他物品或内容的喜好。这样，推荐引擎就可以给用户推荐他可能感兴趣的物品或者内容了。 对比搜索系统和推荐系统可知，搜索系统最重要的目标是降低延迟和提高相关性分析。推荐系统的目标不是帮助用户找到相关内容，而是希望用户消费内容。当然，搜索系统和推荐系统也有很多相似的地方。比如，推荐系统和搜索系统底层技术实现基本上是相同的。基于内容的推荐系统本质上就是一个小的搜索系统。 广告系统是一个特殊的存在。搜索系统和推荐系统都是为人找信息，而广告系统是为信息找人。广告系统在形式上更像推荐系统，在技术实现上又兼有推荐系统和搜索系统两者的特点。其实在技术实现上，我们可以将搜索系统和推荐系统进行完美的统一。 1.2.2 推荐系统的经典问题推荐系统一直存在两个比较经典的问题：探索和利用（Exploration &amp; Exploitation，EE）、冷启动问题。本节主要介绍探索和利用问题。探索指探索未知的领域；利用指根据当前信息，由训练的模型做出最佳的决策。实际上，探索是指做你以前从来没有做过的事情，以期望获得更高的回报；利用是指做你当前知道的、能产生最大回报的事情。 在推荐系统中为了可以准确估计每件物品的响应率，我们可以将每件候选物品展示给一部分用户，并及时收集物品的响应数据，以此对候选物品进行探索。然后，利用响应率估值较高的物品来优化目标。但是探索过程中存在机会成本，如果仅根据当前收集的数据估算物品响应率，那么，实际上候选物品可能并没有机会展示给用户，这是一个权衡和博弈的过程。 如果利用太多，那么模型比较容易陷入局部最优，但是探索太多，模型收敛速度太慢，这就是EE的困境。EE问题的核心是平衡推荐系统的准确性和多样性。所以，解决EE问题的关键是找到一种长期收益最高，但可能对短期奖励（Short-term Reward）有损失的策略。现实中，我们可以用求解多臂赌博机（Multi-Armed Bandit，MAB）的方法来解决EE问题。 Bandit算法来源于历史悠久的赌博学。事情是这样的：一个赌徒要去摇老虎机，走进赌场一看，一排老虎机外表一模一样，但是每个老虎机吐钱的概率不一样。他不知道每个老虎机吐钱的概率分布，那么每次该选择哪个老虎机来最大化收益呢？这就是MAB问题。Bandit算法不是指一个算法而是指一类算法。 下表介绍了几个最常用的Bandit算法。 下表为推荐系统与Bandit算法对应关系 在推荐系统中，我们常采用三类策略解决EE问题，包括贝叶斯方法、极小/极大方法以及启发式赌博方案。这里只介绍前两种方案。 1.贝叶斯方法 贝叶斯方法解决MAB问题如下表所示。MAB问题可以转化成马尔可夫决策过程（MDP）。MDP问题的最优解需要通过动态规划（DP）的方式求解，虽然存在最优解，但是求解的成本极高。MDP是一个研究序列决策问题的框架。其利用状态空间、奖励函数以及转移概率定义了一个序列问题。贝叶斯方法的目标是找到与MAB问题对应的贝叶斯最优解。 下表为使用贝叶斯方法解决MAB问题 2.极小/极大方法 EE问题也可以利用极小/极大方法来解决。极小/极大方法的核心思想是找到一种方案，使该方案的最差性能限定在合理范围内。性能可以由遗憾来衡量。假设臂中奖概率是固定的，那么中奖概率最高的臂就是最优臂。所以在T次拉臂后，遗憾就是拉最优臂T次获得的期望总奖励与根据拉臂方案获得总奖励之间的差值。 在极小/极大方法中，UCB的解决方案最为流行，其通常会不断探索以降低最差性能。 2.推荐系统的冷启动随着越来越多的互联网平台对推荐系统的使用以及推广，用户对于通过推荐系统获取信息的方式也越来越习惯。当用户当前搜索的历史行为为空时，推荐系统面临一个比较独特的状态，即冷启动状态。冷启动问题处理不好会导致推荐的满意度降低。针对新用户，推荐系统如何生成推荐结果，尤其在当下引入新用户的成本相当高的情况下，如何快速让新用户留存下来并转化是非常重要的。所以，对于推荐系统来说，处理冷启动问题是一门学问，也是一个难点。 推荐系统冷启动主要分为三类：用户冷启动、物品冷启动、系统冷启动。冷启动问题的解决方案可以有以下几种，比如利用热门数据、利用用户注册信息、利用第三方数据、利用物品内容属性和利用专家标注数据，如下图所示。下面举例介绍这几类冷启动问题解决方案。 1.利用热门数据热门数据是物品按照一定规则排序得到的排名靠前的数据，反映了大众的偏好。在某些场景下，我们可以先用热门数据作为冷启动问题的解决方案。 2.利用用户注册信息用户注册时，系统会对新用户的信息进行收集。推荐系统可以利用用户基本信息，如年龄、学历等对用户分类，然后根据用户所属分类推荐同类别下其他用户喜欢的物品；利用用户在注册过程中授权的信息，如定位信息、通讯录信息等，推荐给通讯录好友喜欢的物品等；利用用户注册过程中填的兴趣标签，推荐与标签相关物品。 3.利用第三方数据目前，很多网站或者App支持第三方账号登录。用户登录功能支持QQ、微信、邮箱或者第三方账号登录。系统可以获取第三方平台提供的相关信息，这个相关信息可能包括用户本身信息和朋友关系信息。系统通过协同过滤或者聚类等算法计算出用户的兴趣度，弥补用户冷启动所带来的推荐不足。 4.利用物品内容属性在新闻类、咨询类网站中利用物品的内容属性推荐是十分重要的。物品的内容属性分为三大类：物品本身的属性、物品的归纳属性、物品的被动属性。物品本身的属性包括标题、产出时间等。物品的归纳属性是物品的类别属性，包括类别、品牌等。物品的被动属性表示物品的被动行为的属性，如浏览、点击、评论等。由于新物品缺少被动属性，因此在进行推荐时，我们可以根据其本身属性和归纳属性推荐给喜欢同类物品的用户。例如，周杰伦的《说好不哭》这首歌在刚推出时，我们可以根据其本身属性（歌手、发行时间、歌曲简介等）和归纳属性（类型、流派等）找到相关歌曲。 5.利用专家标注数据有些系统在刚建立的时候，既没有用户行为数据，也没有充分的物品内容数据，因此很难进行物品相似度度量。这种情况属于系统冷启动问题，可利用专家标注解决。以Pandora电台为例，从音频信息上解决相似度问题，技术实现难度较大，而仅仅使用专辑、歌手等信息，推荐效果又不是很好。Pandora电台为了更加精准地进行推荐，聘请一批音乐专家对几万名歌手的歌曲从400多个维度去标注，构建每首歌曲的音乐基因向量，然后通过常见的向量相似度算法计算出歌曲的相似度。 3.推荐系统的召回策略前文中讲到大型的推荐系统一般都会有两个阶段——召回和排序阶段。为什么需要召回阶段？首先是因为物品众多，系统无法为每一个用户逐一计算每一个物品的评分，这就需要召回阶段。召回阶段的作用就是圈出一部分物品，以此降低系统计算量。根据不同的业务场景，我们可以选择不同的召回策略。召回策略有很多种，比较重要的有基于行为相似的召回和基于内容相似的召回。 3.1基于行为相似的召回协同过滤算法（Collaborative Filtering Recommendation）是推荐系统最基础和最常用的算法。该算法通过分析用户的兴趣，在用户群中找出与当前用户相似的用户。但是，该算法有一个前提条件，即相似的人对于同一个事物所表现出的兴趣度是相同的。 协同过滤算法包括以下几个步骤：收集用户偏好、找到相似的用户、计算并推荐。 协同过滤算法也可分为两种：一种是基于用户（User-based CF），另一种是基于物品（Item-based CF）。下面具体讲解这两种算法。 1）User-based CF算法的核心思想是利用用户的行为去定义与其相似的用户，即先使用统计方法寻找与当前用户有相同喜好的近邻用户，然后根据近邻用户的行为数据产生推荐结果，如下两图所示。 2）Item-based CF算法的核心思想是根据用户对物品的评价，发现物品间的相似度，根据目标用户的历史偏好将类似的物品推荐给用户，如下两图所示。 在计算两个用户的兴趣相似度过程中，我们可以使用以下几种方法。 1.Jacard相似度Jacard相似度计算如式（7-2）所示： 3.2基于内容相似的召回基于内容相似的召回往往又建立在对内容理解的基础上。它的核心思想是根据推荐物品的元数据或描述内容，发现物品间的相关性，然后基于用户的喜好，推荐给用户相似的物品。 前文中提到过语言模型，这里介绍另一种语言模型Word2Vector。这个模型概念是Mikolov在2013年提出来的。Mikolov在NNLM（Neural Network Language Model）模型的基础上，提出了Word2Vector的算法。Word2Vector有两种训练模式，分别是CBOW和Skip-gram。在结构上，CBOW和原NNLM类似，去掉了隐藏层，使投影层直接映射到了Softmax输出；在原理上，CBOW和原NNLM一样，也是利用被估计词的上下文来预测该词的向量。但是，Skip-gram和CBOW在原理上正好相反，是用某个词来预测该词的上下文。为了减少计算量，Mikolov提出了两套解决方法，一种是Hierarchical Softmax，另一种是Negative Sampling。 Hierarchical Softmax是把输出层改造成基于词频设计的Huffman Tree，用叶子节点表示每个词，通过根节点到词的路径为词编码，从而计算得到每个词的词向量。词频越高，离Tree的根节点越近，则该词更加容易被搜索到。 尽管分层Softmax在计算上已经达到了实用的程度，但是Mikolov依然对计算速度不够满意，于是在简化噪声对比估计的基础上，得到Negative Sampling方法，以代替分层Softmax的Huffman Tree结构。 1.Huffman编码与Huffman tree Huffman Tree是带权重的最优二叉树，即构造一棵二叉树，使带权重的路径长度值最小。权重越大的节点离根节点的路径距离就越短；反之，离根节点的路径距离也就越长。 Huffman Tree的构造方法如下。1）假设存在n个权重值的序列θ={θ1，θ2，…，θn}，每一个权重可以视为一棵单独的树。2）从大到小为权重序列重新排序，找出权重相对最小的两棵树作为左右子树，构造出一棵新的二叉树。我们可以指定左右子树哪个权重更小一些（比如左边比右边小）。新的二叉树的根节点的权重是两个子节点权重的和。3）在原权重序列中删除已经合并的树，并加入新的树。4）重复第2步和第3步，直到序列中只有一棵树为止。如果给出一句话“我”“爱”“北京”“天安门”，它们在整篇文章中的出现频率分别是1、2、3、4，把词出现的频率当作权重来构建一棵Huffman Tree，具体步骤如下图所示。 1）根据构建Huffman Tree的步骤，首先挑取权重相对最小的两个值（1和2）作为左右子树，然后合并构建新的二叉树（权重小的为左子树，大的为右子树），新的二叉树根节点值为3，并删去原来的1和2，用新二叉树代替。2）在新的集合中挑选两个最小的权重为左右子树，即两个3，合并两个子树构成新的二叉树，它的根节点的权重值是6。删去旧的部分，加入新的二叉树，得到第二步。3）在新的集合，4和根节点为6的二叉树中，4较小作为左子树，6较大作为右子树，它们新的根节点为10。删去旧的部分，只留下新的二叉树。我们发现只有新的二叉树存在。新的二叉树即所求的Huffman Tree。我们也可以使用Huffman编码的方式来表示Huffman Tree。下面再来看看Huffman编码的构造方法。这里举一个例子。 Huffman Tree在构造的过程中统一给出左右子树大小的约定，在本节中左子树比右子树的权重值小。如果采用二进制编码的方式，左节点标识为0，右节点标识为1。根据这个规则我们尝试为“我”“爱”“北京”“天安门”4个词找出它们的Huffman编码，如下图所示。 “我”对应的编码为110，“爱”对应的编码为111，“北京”对应的编码为10，“天安门”对应的编码为0；很显然字符出现频率小编码就长，出现频率高编码就短，这样保证了此树的带权路径长度，效果上就是传送报文的最短长度。 2.CBOW-Hierarchical Softmax CBOW（Continuous Bag-Of-Words Model）模型只包括了输入层、投影层和输出层。如果已知当前词是wt，上下文词是wt–2、wt–1、wt+1、wt+2。模型CBOW可以看作是利用上下文wt–2、wt–1、wt+1、wt+2来预测当前词wt的模型，如下图所示。 根据之前的介绍可以知道，计算每个词的词向量只和这个词与其对应的上下文有关系，即与（Context(w)，w）有关。一般，我们可以给定一个范围来限制这个词的上下文Context(w)。目标函数可以用对数似然函数来描述，公式如（7-9）所示： CBOW-Hierarchical Softmax模型框架如下图所示。 如果词典中词的个数为N，那么叶子的节点数也是N，非叶子节点的数目为N–1个。 3.Skip-Gram-Hierarchical Softmax 如下图所示，Continuous Skip-Gram模型和CBOW一样，也包括输入层、投影层和输出层。如果已知当前词是wt，则上下文词是wt–2、wt–1、wt+1、wt+2。Skip-Gram可以看作是利用当前词wt来预测上下文wt–2、wt–1、wt+1、wt+2的模型，和CBOW的输入和预测正好相反。 对于Hierarchical Softmax的Skip-Gram模型来讲，其需要优化的目标似然函数是： 4.推荐系统排序4.1特征选择的方法排序之前，我们需要考虑影响排序的特征。特征选取的优劣最终会影响到用户体验。工业界的认识是：数据和特征决定了机器学习的上限，而模型和算法只是用于无限地逼近这个上限。先来给特征工程下一个定义：特征工程的本质是一项工程活动，目的是从原始数据中提取供算法和模型使用的有效数据。下面给出一张特征工程示意图。 特征工程中特征处理是最核心的部分。特征处理可以分为数据预处理、特征清洗。而我们所说的特征通常可以分为基础特征和组合特征。 基础特征包括但不限于用户的基础信息，比如用户的性别、年龄、身高、生日和注册时间等；用户的输入内容，比如一些平台建议用户填写的兴趣标签、用户自身的描述信息和用户的评论信息等；用户的行为信息，比如用户的登录信息、登录时间段、使用时长、对物品的评价、物品页面的停留信息和物品页面的点击信息等。这些特征又可以根据不同的标签、类别、时间属性和位置信息等再次分割成更细微的特征。我们将这些特征归类为基础特征主要是因为它们通常是在产品日志中直接产生的，其中不少直接对推荐结果产生不可忽略的影响，但是有些不能直接使用，这就需要组合特征的存在。 组合特征主要是通过对基础特征乃至组合特征本身不断再组合的方式产生的特征。组合方法主要包括分箱、分解类别特征再组合、加减乘除、平方、开平方等。在不同的推荐模型下，对特征的选取以及再加工过程也不同。比如业界常用的线性模型LR，在使用的时候其实要求所有选用的特征都与预测的目标线性相关，所以在进行特征工程的时候，对组合特征的使用更为频繁及复杂。而在深度交叉模型中如Deep FM，对高阶组合特征的生成更依赖模型本身。但是，这并不代表深度交叉模型中，特征的选取与特征工程就不再重要，还是需要根据生产场景，选择不同的侧重点进行挖掘。 在生成了特征之后，特征验证也是一个比较重要的工作。由于生产场景的不同，生成的特征中往往存在不用或者暂时不可用的情况，这需要我们在一开始就将这类特征排除，以减少后面的工作，进而优化特征生产的流程等。 1.特征预处理 经过特征提取，我们可以得到未经处理的特征，这些特征数据可能有一些问题，不能直接使用。存在的问题总结如下。1）不属于同一量纲。特征的规格不一致，不能放到一起。2）信息冗余。对于某些定量特征，其包含的信息没有按区间划分。如征婚对象的高度，如果只关心合适、不合适可以转换为1和0表示。3）定性特征不能直接使用。某些机器学习算法只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征值为1，其他扩展特征值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作。对于线性模型来说，其使用哑编码后的特征可达到非线性的效果。4）特征存在缺失值。缺失值需要补充。5）信息利用率低。不同的机器学习算法对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。因为有上面这些问题的存在，我们有一些特别的方法进行特征处理。 （1）无量纲化特征处理 对于无量纲化数据的处理可以采用标准化和区间缩放法进行处理。标准化处理的前提是特征服从正态分布，标准化后的特征服从标准正态分布。区间缩放法是利用边界值信息，将特征值缩放到某个范围。 from sklearn.preprocessing import StandardScaler StandardScaler().fit_transform(input_data) from sklearn.preprocessing import StandardScalerMinMaxScaler().fit_transform(input_data) 2.特征选择 常用的特征选取方法主要包括过滤法、封装法、嵌入法。 过滤法：即按照相关性对各个特征进行评分，设定阈值或者待选阈值的个数，选择特征。例如方差选择法：先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征；相关系数法：将P值作为评分标准，选择K个特征值；卡方检验和互信息法等。 方差选择法实现代码如下： from sklearn.feature_selection import VarianceThreshold#方差选择法，返回值为特征选择后的数据#参数threshold为方差的阈值VarianceThreshold(threshold=3).fit_transform(input_data) 用sklearn中feature_selection库的SelectKBest类结合相关系数选择特征的代码如下： from sklearn.feature_selection import SelectKBestfrom scipy.stats import pearsonr#选择K个最好的特征，返回选择特征后的数据#第一个参数为评估特征是否好的函数，该函数输入特征矩阵和目标向量#输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值，在此定义为计算相关系数#参数k为选择的特征个数SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_ transform(input_data, input.target) 卡方检验实现代码如下： from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2#选择k个最好的特征，返回选择特征后的数据SelectKBest(chi2, k=2).fit_transform(input_data, input_target) 互信息方法也是评价两个变量之间相关性的方法，计算公式如下： 封装法：对于备选特征，每次在模型中选择或者删除部分特征，基于现有的评价标准，利用模型或者评分标准去评价变动特征对结果的影响，反向选择特征。 嵌入法：先使用某些机器学习算法进行训练，得到各个特征的权重，再根据权重从小到大选择特征。 4.2推荐系统的排序过程在拥有了备选数据集和大量确定的特征之后，我们进入推荐系统的排序阶段。推荐排序问题和搜索排序问题完全一致。之前已经介绍过排序学习(L2R)的一些基本理论知识。排序学习可以分为单文档方法、文档对方法和文档列表方法。在实际的应用过程中，我们会把排序模型分为线性模型、树模型、深度学习模型，以及它们之间的组合模型等。业界普遍认为的模型迭代是从早期的线性模型LR，到引入自动二阶交叉特征的FM和FFM，再到非线性树模型GBDT和GBDT+LR，然后到深度学习模型，如下图所示。 这里主要是比较一下传统机器学习模型的优缺点。 1）LR模型的优点是可解释性强。通常，排序模型良好的可解释性是业界比较在意的指标。但是，LR需要依赖大量人工挖掘特征，而且有限的特征组合无法提供较强的表达能力。 2）FM在LR的基础之上做了改进，引入了交叉项作为特征，可以减少人工特征挖掘的过程，捕捉更多的信息。但是，FM模型只能捕捉两两特征间的关系，无法获得更高阶的交叉特征。 3）GBDT是一个提升模型，它通过组合多个弱模型拟合残差得到一个更强的模型。GBDT属于树模型，能够很好地挖掘组合高阶特征，具有一定可解释性。但是，它对高维度稀疏特征、时间序列特征处理得不是很好。 随着业务场景的扩展，在传统模型上优化和收益将会受限。与此同时，海量数据、知识图谱等多维度特征的引入，使传统的排序学习继续向深度学习模型发展。前文中也提到了几个深度学习模型实例，这里具体讲讲深度学习模型优势。 1）强大的模型拟合能力。深度学习模型包含多个隐藏层和隐藏结点，配合非线性激活函数可以模仿神经细胞工作方式去拟合任何函数。 2）强大的特征表征和泛化能力。深度学习模型可以处理很多传统模型无法处理的特征。例如深度学习模型可以直接从海量训练样本中学习到高维稀疏特征的隐含信息，并通过嵌入的方式表征；对于文本、序列特征以及图像特征，深度学习模型均可处理。 3）自动组合和发现特征的能力。华为提出的Deep FM以及Google提出的Deep Cross网络模型可以自动组合特征，代替大量人工组合特征。 当然，深度学习模型也存在一些现实问题。比如深度学习的黑盒属性会带来巨大的解释成本，也会带来一些业务问题。比如，对于负例的快速响应、模型是否能充分学习无从得知。但是，我们相信深度学习一定是推荐系统发展的方向。 5.基于知识图谱的推荐系统知识图谱是认知智能的重要一环，知识赋能的智能推荐将成为未来推荐系统的主流。智能推荐可以表现在多个方面，包括场景化推荐、任务型推荐、冷启动场景下推荐、跨领域推荐、知识型推荐等。 1）场景化推荐。比如在淘宝上搜“沙滩裤”“沙滩鞋”，通过这些搜索词，系统可以推测用户近期去海边度假，可以按照这个场景推荐防晒霜、草帽、遮阳帽等。 2）任务型推荐。用户购买了羊肉卷、火锅底料等，系统可以根据完成涮火锅任务所需物品进行推荐，比如推荐火锅、电磁炉等。 3）冷启动场景下的推荐。这是推荐领域比较棘手的问题。我们可以通过知识图谱解决推荐系统数据稀疏及冷启动问题。 4）跨领域的推荐。现在流量入口成为吸金入口，各大网站纷纷寻找新的模型进行流量变现。做好垂类的知识图谱以及打通多个知识图谱具有一定的经济效益。比如，如果一个短视频用户经常晒风景照片或视频，那么平台可以考虑为该用户推荐一些淘宝的登山装备。再比如百科知识图谱告诉我们九寨沟是个风景名胜区，旅游需要登山装备，登山装备包括登山杖、登山鞋等，从而实现跨领域推荐。 我们知道推荐系统的最大瓶颈是推荐的可解释性差。现实中，图是解释万物的基础。多种关系的交织可以组成一张图。知识图谱正好以关系图将现实中的实体连接起来。所以，知识图谱必定是推荐系统一个强大的技术支持。 我们可以通过三种方式将知识图谱引入推荐系统。依次学习：首先使用知识图谱得到实体向量和关系向量，然后将这些低维向量引入推荐系统，学习得到用户向量和物品向量，如图下图所示。 联合学习：将知识图谱的特征学习和推荐算法的目标函数结合，使用端到端的方法进行联合学习，如下图所示。 交替学习：将知识图谱和推荐算法视为两个分离但又相关的任务，使用多任务学习的框架进行交替学习，如下图所示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"2020法研杯阅读理解赛道第一名方案","slug":"cail-2020-mrc","date":"2021-05-14T11:31:31.000Z","updated":"2021-05-14T07:15:41.665Z","comments":true,"path":"2021/05/14/cail-2020-mrc/","link":"","permalink":"http://renxingkai.github.io/2021/05/14/cail-2020-mrc/","excerpt":"","text":"2020法研杯阅读理解赛道第一名方案2020年法研杯阅读理解竞赛结束了，我们团队在最终排行榜获得了第一名的成绩，去年也参加了，过了一年，还是一只鶸，害，首先感谢各个队友的帮助，接下来是我对参加这次比赛的总结与分享，希望和大家相互学习，多交流。 数据介绍今年的数据集是去年的升级版，去年格式类似SQuAD 2.0，今年格式类似于HotpotQA，不仅文书种类由民事、刑事扩展为民事、刑事、行政，问题类型也由单步预测扩展为多步推理，难度有所升级。具体而言，对于给定问题，只通过单句文本很难得出正确回答，选手需要结合多句话通过推理得出答案。 本任务技术评测训练集包括两部分，一部分为去年的CJRC训练集，一部分为重新标注的约5100个问答对，其中民事、刑事、行政各约1700个问答对，均为需要多步推理的问题类型。验证集和测试集各分别约为1900和2600个问答对，同样均为需要多步推理的问题类型。第一阶段多步推理数据仅提供民事的一部分数据，规模较小，选手可充分利用CAIL2019的数据进行训练。（来自官网介绍） 简单来看个数据就明白了，需要让模型阅读完问题question和文章context之后，不仅回答出问题的答案(答案仍然有四种类型：span、yes、no、unknown)，还需要找出回答出该答案所依据的线索句子。如下例：通过线索句子1和3可以回答出正确的答案“魏7”。 &#123; &quot;_id&quot;: 5048, &quot;context&quot;: [ [ &quot;经审理查明，&quot;, [ &quot;经审理查明，&quot;, &quot;被告人胡x1的犯罪事实与起诉书指控的一致。&quot;, &quot;另查明，&quot;, &quot;案发后被告人胡x1与魏7达成和解协议，&quot;, &quot;并已履行完毕，&quot;, &quot;魏7对被告人胡x1的行为表示谅解。&quot;, &quot;上述事实，&quot;, &quot;被告人胡x1在庭审过程中亦无异议，&quot;, &quot;且有书证被告人胡x1的户籍证明、金乡县公安局刑警大队出具的到案经过、山东省金乡县人民法院移送公安机关侦查函、民事诉状及三份借据复印件、财产保全申请书、山东省金乡县人民法院（2009）金民初字第826-1号民事裁定书、EMS全球邮政特快专递回执联复印件、金乡县人民法院（2009）金8协字826-1号协助执行通知书及送达回证、（2009）金民初字第826号民事调解书及送达回证、调解笔录、协议书、谅解书、立案审批表及申请执行书、法律文书生效证明书、（2010）金执字第256号执行通知书、特快专递回执联复印件、申请书、（2010）金执字第256-2号执行裁定书、改退批条、（2010）金8协字256号协助执行通知书及送达回执、本地机动车详细查询基本信息、证明、金乡县人民法院传票及执行笔录、金乡县人民法院关于查封被执行人胡x1车辆情况说明、鲁H车辆照片复印件、机动车注册、转移、注销登记/转入申请表及委托书、二手车销售统一发票、机动车交通事故责任强制保险单、齐鲁晚报一份、办案说明、机动车单项查询信息，&quot;, &quot;证人魏7、付某甲、徐某、付某乙的证言，&quot;, &quot;被害人胡x9的陈述，&quot;, &quot;被告人胡x1的供述等证据，&quot;, &quot;足以认定。&quot; ] ] ], &quot;question&quot;: &quot;被起诉书指控的人和谁达成了和解协议?&quot;, &quot;answer&quot;: &quot;魏7&quot;, &quot;supporting_facts&quot;: [ [ &quot;经审理查明，&quot;, 1 ], [ &quot;经审理查明，&quot;, 3 ] ] &#125; 评价方式本任务采用F1进行评估。对于每个问题，需要结合案情描述内容，给出回答，回答为Span（内容的一个片段）、YES/NO、Unknown中的一种，并且给出答案依据，即所有参与推理的句子编号。评价包括两部分：1）Answer-F1，即预测答案会与标准答案作比较，计算F1；2）SupFact-F1，即预测句子编号序列会与标准句子编号序列作比较，计算F1。最终为这两部分F1的联合F1宏平均。 比赛流程本次比赛流程仍然和去年一样，分为三个阶段，第一阶段：报名，发放samll数据集，编写、测试模型，结果不计入最终成绩；第二阶段：发放big数据集，结果将计入最终成绩；第三阶段：选手在第二阶段提交的模型中选择三个模型作为第三阶段封闭测试的模型。比赛的最终成绩计算方式：最终成绩 = 第二阶段的成绩 0.3 + 第三阶段的成绩 0.7评测方式仍然和之前一样，需要提供训练好的模型，主办方线上评测，选手看不到测试集。 接下来我也继续按着时间线来分享下我们团队此次比赛中使用的方案和一些经验。 第一阶段拿到数据之后日常进行数据分析，第一阶段主办方给出了1565个样本，文章(context)长度99%分位数为592.0，问题(question)长度99%分位数为34.7，答案(answer)长度99%分位数为23.0；第二阶段公布了5054个样本，文章(context)长度99%分位数为645.0，问题(question)长度99%分位数为40.0，答案(answer)长度99%分位数为30.0。 主办方同时也发布了基于BERT的多任务联合训练的strong baseline，应该是基于Spider-Net改的，代码结构很清晰，是一个很棒的baseline。我们也是接着baseline继续优化，其实是3个子任务的联合训练：阅读任务、答案类型四分类任务、线索句子二分类任务。 我们也是接着baseline继续优化，其实是3个子任务的联合训练：阅读任务、答案类型四分类任务、线索句子二分类任务。 数据增强部分主办方说明可以使用去年的数据，但是去年数据是类似SQuAD 2.0格式，仅有答案和答案类型，并没有提供线索句子，因此线索句子需要自己构造。我们构造方式主要分为两种，第一种方式是分句之后直接在每个句子中find答案文本，如果该句子中包含答案文本，则将该句子作为支持句子。However，这样做其实是有问题的，留个彩蛋，后面说。。。 第二种方式也是基于第一种方式的，我们考虑到不仅仅是包含答案文本的句子可以是线索句子，有些句子虽然不包含答案文本，但是要回答出答案，需要这些句子的推理，其实也是线索句子的。因此，我们将问题和答案拼接[question:answer]，作为句子A，依次去计算拼接后的句子与文章每个句子的TFIDF相似度，分别取相似度最高的前1、2、3条句子作为线索句子(同时也添加了答案文本所在的句子)。 之后的实验发现第一阶段采用第二种方式取TFIDF top2句子效果最好，然而，比赛第二阶段发现，第一种构造数据方式虽然简单粗暴、正负样本也不均衡，但是比第二种方式效果更好，可能是第二阶段发布了较多的训练集原因。 对于阅读任务，有了去年的经验，我们并没有在BERT后面继续添加一些基于Attention的阅读模型，而是直接取出BERT最后两层的sequence_out进行拼接去做答案的抽取，实验表明，比仅取最后一层效果会更好，BERT不同层学习到了不同的特征表示。对于答案类型分类任务，沿用去年的方法，在BERT的最后一层sequence_out后面接CapsNet和答案四分类的Attention层进行答案的分类。对于线索句子分类任务，基本还没做什么大的改进，沿用baseline的方式对每个句子去做2分类，尝试了下简单的添加多层网络，效果并不明显。 将三个子任务的loss加权相加在一起，作为整个任务的loss进行联合训练。第一阶段初期，我们一直使用的是RoBERTa_base预训练模型，也尝试改各种模型，数据做了一周多些，发现线下基本没啥提升，但是排行榜上前排比我们高了3-4个点，我们觉得还是预训练模型的问题，然后换了RoBERTa_large，确实分数也就提上来了。 第二阶段第二阶段我们做的就更细致一些了。 数据部分 接上面说的，“我们第一种方式是分句之后直接在每个句子中find答案文本，如果该句子中包含答案文本，则将该句子作为支持句子。”由于数据中只给出了答案文本，并没给准确的答案开始位置，主办方给的baseline也是find答案文本，然后取匹配到的第一个答案文本位置作为答案开始位置，但是这样其实是有问题的，会出现错误标记的问题。举个例子：下面样例的问题是“谁垫付了后续的政府奖补资金？”，答案文本是“原告”，但是“原告”两个字在context中出现了多次，第一次出现“原告”位置句子为“被告在进贤县文港镇采砂办渡头榨下的采砂场所占的股权转让给原告所有”，明显不是真正答案的位置，真正答案线索句子为“原告先行垫付被告后续政府奖补资金”。如果按baseline的方式，会有不少样本出现这样的答案标注偏差问题，对于阅读理解这样的任务，这种影响其实挺大的。我们根据训练集给的supporting_facts中的线索句子，去重新标定答案，这样可以确保答案是和线索句子相关，提高了真正答案位置的准确率，不至于完全和问题不相关。 &#123; &quot;_id&quot;: 65, &quot;context&quot;: [ [ &quot;经庭审质证、认证，&quot;, [ &quot;经庭审质证、认证，&quot;, &quot;本院认定事实如下：2014年元月18日，&quot;, &quot;原、被告双方签订了一份采砂场股权转让协议书。&quot;, &quot;协议约定：原、被告自签订该协议起，&quot;, &quot;被告在进贤县文港镇采砂办渡头榨下的采砂场所占的股权转让给原告所有，&quot;, &quot;被告不再拥有采砂场的开采权益及股份权，&quot;, &quot;砂场平台以上存砂、石、机械设备、棚房及其他地面建筑物等全部清理出砂场；&quot;, &quot;被告无条件配合市、县采砂办、文港镇政府切割采砂设备和清理砂场；&quot;, &quot;同时，&quot;, &quot;原告先行垫付被告后续政府奖补资金，&quot;, &quot;原告支付后，&quot;, &quot;后续的砂场设备奖补资金归原告所有。&quot;, &quot;协议还就其他事项作了约定。&quot;, &quot;协议签订后，&quot;, &quot;原告按规定给付了被告全部股权、作业房、装载机、吸沙船及挖沙船的转让金175320元，&quot;, &quot;并垫付了后续的政府奖补资金。&quot;, &quot;为此原告具状法院，&quot;, &quot;提出如前的诉讼请求。&quot;, &quot;另查明，&quot;, &quot;原、被告签订协议后，&quot;, &quot;原告从进贤县文港镇采砂办领取了采砂场的部分分红。&quot; ] ] ], &quot;question&quot;: &quot;谁垫付了后续的政府奖补资金？&quot;, &quot;answer&quot;: &quot;原告&quot;, &quot;supporting_facts&quot;: [ [ &quot;经庭审质证、认证，&quot;, 14 ], [ &quot;经庭审质证、认证，&quot;, 15 ] ] &#125; 使用19年数据时，19年数据文章context长度普遍较长，平均约800-900，这样直接切句子时，容易超出max_len=512，因此，我们使用了下截断，将答案开始位置在480长度之后的，进行根据答案开始结束位置前后取150长度作为新的文章context，这样保证了文本长度基本在512之内，扩充了正样本量。仅通过这两个对数据集的修正，我们ans_f1就提高了1个多点，确实证明了正确数据的重要性。 预训练模型方面的尝试：去年做法研杯的时候，当时预训练模型只有中文版的BERT_base和ERNIE 1.0，短短一年时间，NLP预训练模型五花八门，从理解到生成都发布了很多优秀的预训练模型。因此，除了RoBERTa模型之外，我们还尝试了NEZHA和ELECTRA，但是效果都没有RoBERTa好，差距约2-3个点。通过阅读ACL 2020最佳论文提名奖《Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks》，我们也受到启发，继续对下游任务进行预训练，使用的18年的法研杯数据，约3.5G，继续进行MLM任务的预训练，跑了20W步，然后进行测试，发现也没效果，23333….，也尝试了FGM等对抗训练方式，也没有提升。 对于阅读任务，除了第一阶段的拼接BERT最后两层句向量，还在后面接了BiAttention，类似BiDAF的query、context双向attention，然后去做答案抽取。当然也做了其他各种操作，答案验证、接BiGRU、接Highway Net等。 对于答案类型分类任务，除了在BERT的最后一层sequence_out后面接CapsNet和答案四分类的Attention层进行答案的分类，再结合CLS和sequence_out做了层attention，联合四个logit进行答案四分类，也尝试了DiceLoss这些，但并没有提升。 对于线索句子分类任务，先将BERT的sequence_out再次经过SelfAttention，然后使用了线索句子开始和结束的映射，去和输出的所有句子表示做Attention，突出线索句子的表示，最终concat线索句子开始和结束表示，与max、mean方式进行联合分类线索句子。在训练时，非线索句子数量一般远大于线索句子，由于是二分类，使用的是BCE损失，我们也设置了pos_weight，让正样本权重提高一些。 第二阶段用以上方式，我们最好的单模ansf1有83.46，support_f1有78.23，离比赛第二阶段结束还有一周多时，单模取得了线上第四的成绩。最终的模型集成就是不同的下游模型集成了，以及不同的随机种子，然后选取加权权重这样的常规集成方式。 总结&amp;TO DO 首先感谢主办方举办的第二届法研杯机器阅读理解比赛，赛题也更加有趣和挑战性，期待明年阅读任务和数据格式会是什么样子。 其实HotpotQA排行榜上面前排很多模型都是基于图网络的，用图网络做线索句子识别效果会更好，但也由于我们对图网络了解不够，所以没有采用这样的方法，图网络技能以后还是必须得点上。 这次比赛数据质量很重要，阅读理解答案位置的正确标定，真实的线索句子都会带来不小的提升，所以不论竞赛还是工程项目，在数据上做的要足够细致。 通过分析bad case，发现模型对答案文本靠后的答案有的识别不出来，以及如果原文中多次出现了答案文本，比如多次出现“原告”，也抽取不出来，所以可以对这些bad case再做进一步的优化。 参考文献[1] 法研杯2020官方github [2] RoBERTa: A Robustly Optimized BERTPretraining Approach [3] ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS [4] NEZHA: Neural Contextualized Representation for Chinese Language Understanding [5] Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks","categories":[{"name":"比赛","slug":"比赛","permalink":"http://renxingkai.github.io/categories/比赛/"}],"tags":[{"name":"法研杯2021MRC","slug":"法研杯2021MRC","permalink":"http://renxingkai.github.io/tags/法研杯2021MRC/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第六章--搜索系统评价","slug":"SearchAndRec-Chapter06","date":"2021-05-13T15:26:18.000Z","updated":"2021-05-13T07:27:19.508Z","comments":true,"path":"2021/05/13/SearchAndRec-Chapter06/","link":"","permalink":"http://renxingkai.github.io/2021/05/13/SearchAndRec-Chapter06/","excerpt":"","text":"搜索系统的评价维度一般包括性能评价和效益评价。性能评价一般包括时间性能和空间性能。系统响应的时间越短，占用的存储空间越小，系统的性能就越好。但时间优越性和空间优越性一般不能兼得，需要取舍。对于搜索系统而言，我们除了考虑响应时间和存储空间之外，还需要考虑其他重要指标，如排序相关性等，即需要考虑检索结果列表的全面性、准确性等效果指标。效益评价主要用来测量搜索系统的服务或者系统本身投入使用时所获得的收益，包括经济效益和社会效益。但效益评价一般很难量化，因为其具有滞后性和不确定性。 1.搜索系统的评价体系1.1 效率评价效率评价是对性能评价中时间性能和空间性能的概括。响应时间、开销和索引量属于时间性能和空间性能的范畴，是需要重点评测的方面。 1.响应时间和开销 对于任何一个应用系统，响应时间是评价系统的重要指标。搜索系统的响应时间分为两种情形：委托检索和非委托检索。委托检索表示用户送交查询给专业检索人员，由专业人员操作搜索系统进行检索，然后将检索结果返回给用户。非委托检索表示用户自行操作检索系统得到结果。随着网络的快速发展，多数用户采用的是非委托检索。另外，计算响应时间一般是针对某一个查询而言，但查询的长短、复杂度是不同的，所以对应的响应时间也是不同的。 除去网络拥挤、通信等外部因素的影响，影响响应时间的因素如下。1）文档库规模。文档库规模越大，检索时间越长，响应时间也就越长。2）硬件因素。机器配置越高，运行速度越快，响应时间也就越短。3）检索软件。检索软件性能越好，检索时间越短，响应时间也就越短。4）存储设备类型和数据的存储结构。存储设备的访问速度越快，数据的存储结构越合理，检索越容易，响应时间也就越短。在实际应用中，响应时间的估算不可避免地受到网络和工具性能的影响，造成同一检索在不同时间、不同地点、不同检索系统中，响应时间不同，因此有必要在不同时间段、地点等不同环境下多次计算。 存储空间的开销主要是指系统所占用的内存空间和外存空间。当检索系统内存空间有限时需要合理分配，一般情况采用中型计算机，不存在内存不足的问题。不同的文档结构所需的外存空间区别较大，比如正排索引和倒排索引所需的外存空间不同；同样是倒排索引，系统选择布尔检索还是全文检索，所需的外存空间也不同。 2.索引量 所谓“索引量”，就是搜索引擎抓取到页面后，经过分析筛选可以被用户搜索到的页面的数量。所以索引量越大，搜索引擎的数据覆盖范围越广。索引量经常和收录量混淆，收录量是搜索引擎抓取过、分析过的页面。所以从定义看，收录在索引之前，即页面没有被收录就不可能被索引。以百度搜索引擎为例，提高网站索引量的方法如下。 1）提高网站内容的质量，爬虫喜欢原创的文字性内容。2）设置合理的内链，确保爬虫可以顺利爬行。爬虫通过内链爬向网站的各个部分，所以内链越强大，爬虫爬行越顺利，可以给网址排名和收录加分。3）建设高质量的外部链接，因为网页的权重和外部链接的内容质量相关。4）网站首页保持更新，以便搜索引擎认为该网站是活跃的。5）网站不要轻易更换域名。域名更换会导致搜索引擎对网站的信用度和友好度下降，也会影响搜索引擎对网站的收录量。 在知道了索引量的定义之后，我们再来看看如何设定合适的索引量。由于索引量是一个与工程息息相关的概念，这里我们先要厘清索引的使用和存储。 以常见的Lucene系统举例，Lucene系统中索引其实是对固定分析字段拆解的倒排索引结果。索引的内容主要与拆解后的词、词在文本中的位置、文本的标号相关。而实战中，通常注重当前使用的搜索引擎的耗时和稳定性。所以，我们会考虑给一个庞大的搜索集群配备主集群、备份集群以及备用集群等。在不考虑索引量对磁盘空间占用的情况下，索引量其实与搜索集群的性能息息相关。来自搜索集群外部的搜索请求的大致流程如下。 1）请求到达主节点，主节点对请求进行分析重构，分发给子节点。2）子节点遍历底层的索引，将所有与结果相关的数据返回给主节点。3）主节点综合处理所有子节点的返回数据，并返给搜索集群外部。 那么在衡量一个搜索引擎的耗时时，我们可从以下4方面考虑：搜索集群外部与搜索引擎通信消耗的时间、主节点处理消耗的时间、主节点与子节点通信消耗的时间、子节点搜索索引消耗的时间。同等情况下，索引量越大，搜索耗时就越长。同时，更大的索引量对于服务器I/O性能的要求也更高，所以如何选择索引量的大小，需具体情况具体分析。 1.2 效果评价本节的效果评价主要包含：准确率和召回率、平均化和插值以及排序靠前文档的质量。 1.准确率和召回率 准确率和召回率是搜索引擎在效果评价中最常用，也是公众最认可的指标。准确率又叫查准率（Precision Ratio，P），用来衡量检索结果中有多少文献与查询相关；召回率又叫查全率（Recall Ratio，R），用来衡量一次检索中与查询相关的文献有多少被检索出。准确率和召回率的计算如式（6-1）和（6-2）所示。 利用数学语言分析准确率和召回率：假设某搜索系统的数据库中所有文献的总量是L，对于某个查询，a表示被检索出的与查询相关的文献数量；b表示被检索出的与查询无关的文献数量；c表示与查询相关，但是没有被检索出的文献数量，则： 准确率和召回率之间存在什么关系呢？一个理想的搜索系统，应该是P=1、R=1，但实际上不存在这样的搜索系统。一般来说，准确率和召回率之间存在着反变关系，即如果提高准确率，召回率往往就会下降；如果提高召回率，准确率就会下降。两者相互制约。准确率和召回率的关系如下图所示，A点召回率很高，但是准确率很低；D点和A点相反，准确率很高，召回率很低；B、C两点是上述两种情况的折中。 影响搜索系统准确率和召回率的主要因素如下。1）文档库的质量。文档库的文档是否齐全、索引体系是否完善、检索途径的多少都会影响准确率和召回率。2）对检索需求的理解。要想达到较高的召回率和准确率，就应该较好地理解需求，制定相应的检索策略。3）检索语言的一致性。检索的实质是比较查询和数据库文档的一致性，所以需要不同检索人员表达检索主题的语言和数据库文档一致，更需要标识查询和标识文档的语言一致，这样才能够确保检索的准确率和召回率。4）标引的网罗性。对数据库文档主题分析得越透彻，抽出的关键词就越多，检索时能够检出的相关文献就越多，召回率就越高，但准确率就会降低；反之，如果标引是只标注中心主题，检出的文档的准确率就会比较高，但是漏检会增多，召回率会降低。5）检索词的专指性。检索词的词意越狭窄、越具体、越专深，检出的文档就会越对口，准确率就会越高，但命中的文档数量就越少，召回率会降低；反之，如果检索词越笼统、越宽泛，检出的文档数量就会增多，召回率就会升高，但检出的文档不相关的可能性也会增加，准确率会下降。 一个优质的搜索系统应该具备高的准确率和召回率，但并不是每个用户在任何时候都需要高准确率和召回率。用户需求可以分为以下4种情况。1）要求召回率R=1。例如申请专利、发明等，需要对全世界范围的有关文档全面了解，才能做出客观的评价，这时就需要R=1的搜索系统。2）要求较高的召回率。例如编写教材、某技术的发展综述，往往需要较全面地获得有关文档，这时对召回率要求较高，但不一定要求R=1。3）要求较高的准确率。例如要了解某种产品的有关信息，解决某一具体问题，往往只需要了解某一个方面或者相关信息，这时对准确率要求较高。4）对准确率、召回率没有具体的要求。对于某些检索，用户本身不能够做出明确的表达，因此，对准确率和召回率也无法提出确切的要求。如何综合评价准确率和召回率呢？一般使用F值度量（F-measure）综合衡量准确率和召回率。它的好处在于能够使用单一的数字综合反映系统性能，被定义为准确率和召回率的调和平均数，如公式（6-7）所示。使用调和平均数而不是数字平均数的原因是，调和平均数强调较小的值的重要性，数字平均数受极值影响较大。假设一个查询的召回率为1，准确率为0，数字平均数是0.5，但调和平均数为0，更好地评价了搜索结果。 一个查询会返回很多结果，但一般用户关注的数量是有限的，所以在计算准确率和召回率的时候不能按照所有的返回结果去统计，而是可以简单地在一些预定义的位置上计算准确率、召回率来评估此次搜索结果，这种评估方式被称作位置p的准确率（Precision at Rank p）。由于用户比较关心排序靠前文档的数量，最常使用的是p@10或者p@20。第二种评价方法是，当召回率每增加0.1时，计算准确率的变化。这种方式能够评价排序结果中所有相关的文档，不只是排序靠前的文档。第三种评价方法是，计算一个额外的相关文档被检出时，平均准确率的变化情况。这种方法能够衡量所有相关文档的排序结果，同时严格依赖于排序位置靠前的相关文档。 2.平均化和插值 以上描述的评价方法都是针对一次查询结果进行评估的，要想更加客观准确地评价检索算法的优劣，必须使用多个查询结果进行测试。本节将讨论对查询集合进行评估的方法——平均化和插值法。平均准确率（Mean Average Precision，MAP）是综合多个查询结果进行评价的最简单的方法，就是对多次查询结果求平均，这也意味着它假设每个用户都期望找到更多相关的文档。MAP计算方法示例如下图所示，黑色表示查询的相关文档，白色表示查询的不相关文档，查询1的平均准确率=(1.0+1.0+0.67+0.75+0.6)/5=0.804，查询2的平均准确率=(0+0.5+0.33+0.5+0.4)/5≈0.35。MAP=(0.804+0.35)/2=0.577。 MAP评价方法简单，便于计算且有效，能够给出详细的搜索算法的性能，但会丢失很多文档信息。下图为两次查询的准确率–召回率图，其中系列1对应查询1，系列2对应查询2。每个查询对应的准确率–召回率曲线差异较大，不便于比较。为了生成一个能够综合反映查询结果的准确率–召回率图，我们对准确率–召回率的值进行平均化。该平均化的过程是将每次查询的准确率–召回率值转化为标准召回率等级对应的准确率。 标准召回率的等级在0到1之间，增量是0.1。为了获取每个增量上的准确率，我们对上图的准确率–召回率数据进行插值，使每个召回率的增量等级上都有数值。在搜索评价体系中，插值法定义在任何标准召回率等级R处的准确率P为： 3.排序靠前文档的质量 在很多搜索系统中，用户更关注排序靠前的相关文档。我们日常浏览页面时，可能不会往后翻很多页，注意力主要集中在第一页或者前两页。对于一些问答类查询，用户更倾向于直接获得一个明确的结果，这种情况下使用召回率并不合适。因此，关注搜索引擎排序结果中靠前文档的质量，在评价搜索结果的时候至关重要。 前文已经讲过计算位置p的准确率，其中p的取值一般是10或者20。这种方法通过多次评价查询结果的质量，然后给出结论，计算简单。但是，其也存在一些问题，即没有对相关文档的顺序做详细区分，因此需要关注文档排列的顺序。 MRR（Mean Reciprocal Rank）是指多个查询的排名的均值，是国际上通用的对搜索结果进行评价的指标。假设有两次查询，第一次查询的第一个相关文档的排序是2，第二次查询的第一个相关文档的排序是5，那个根据这两次查询对搜索结果进行评价，计算方法是：1/2（1/2+1/5）=0.35。MRR方法主要应用于寻址类检索或者问答类检索。MRR的计算如式（6-9）所示。 对于网络搜索评价来说，DCG（Normalized Discounted Cumulative Gain）是较为常用的方法。这种方法基于两个假设：1）高相关性的文档比边缘相关的文档更重要；2）一个相关文档的排序越靠后，对用户的价值就越低，因为它们很少被用户查看。这两个假设产生了一种新的评价方法——相关性设定等级，其作为衡量文档增益的标准。这种增益是从排序靠前的结果开始计算，但靠后的排序位置上的增益会有折扣。DCG方法就是在一个特定的位置p的前提下，计算文档总的增益。在介绍DCG之前，先描述一下CG（Cumulative Gain），其表示前p个位置累计得到的增益，公式如（6-10）所示。 还有一种比较常用的计算方式——增加相关度影响比重，如公式（6-12）所示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第五章--搜索系统中的主要算法","slug":"SearchAndRec-Chapter05","date":"2021-05-11T16:01:24.000Z","updated":"2021-05-11T08:02:57.274Z","comments":true,"path":"2021/05/12/SearchAndRec-Chapter05/","link":"","permalink":"http://renxingkai.github.io/2021/05/12/SearchAndRec-Chapter05/","excerpt":"","text":"1.搜索和机器学习搜索引擎包含两个阶段：召回和排序。搜索系统中所涉及的机器学习的算法会分布在两个部分。第一部分是对Query及文档的理解过程，因为理解过程中使用了自然语言处理等相关算法。第二部分就是排序学习过程，即将机器学习技术应用到排序阶段， 1.1排序学习传统的检索模型靠人工来拟合排序公式，并通过不断地实验确定最佳的参数组合，以此构成相关性打分函数。机器学习排序与传统的检索模型不同，可通过机器学习获得最合理的排序公式，而人只需要给机器学习提供训练数据，如下图所示。 机器学习排序由4个步骤组成：人工标注训练数据、文档特征抽取、学习分类函数、在实际搜索系统中采用机器学习模型。机器学习排序框架如下图所示。 单文档方法（Pointwise）：处理对象是单一文档，将文档转化为特征向量后，将排序问题转化为机器学习中常规的分类或回归问题。CTR方法是单文档方法的典型应用，相对比较成熟，广泛应用于广告、搜索、推荐中。CTR方法的数学表达式：y=f(x)，其中y的范围为[0，1]，y的值越大表示用户点击率越高。 文档对方法（Pairwise）：相比于单文档方法算法，文档对方法将重点转向文档顺序关系，是目前相对比较流行的方法。其输入是文档对，输出是局部的优先顺序，主要是将排序问题归结为二元分类问题。这时，机器学习方法就比较多，比如Boost、SVM、神经网络等。对于同一Query的相关文档集中，任何两个不同标记的文档都可以得到一个训练实例（di，dj），如果di&gt;dj，则赋值+1，反之为-1。于是，我们就得到了二元分类器所需的训练样本。预测时可以得到所有文档的一个偏序关系，从而实现排序。 文档列表方法（Listwise）：与上述两种方法不同，其将每个查询对应的所有搜索结果列表作为一个训练样例。根据训练样例训练得到最优评分函数F，评分函数F对每个文档打分，然后根据得分由高到低排序，得到最终的排序结果。这种方法的输入是文档集合，输出是排好顺序的列表。文档列表方法排序示意图如下图所示。 三者之间的区别就在于训练数据之间的关系对预测目标的影响不同。简单地说，单文档排序算法是点点之间排序，训练数据之间的关系与最终排序无关。换句话说，样本经过模型训练形成的是一种评分方式，而所有样本按照评分结果由大到小排序即可。模型生成后，每个样本的输出结果是固定的、静态的，不会发生变化。这里典型的单文档排序算法有逻辑回归、树模型等。事实上，所有利用二分类问题归纳的排序算法都可以当作单文档排序。其根本逻辑就在于设定两个分类1、0，在训练样本时考虑每个独立样本与当前分类的关系，生成模型参数。在排序过程中，先计算每个样本当前参数耦合后的结果再总体排序即可。 而对于文档对方法，输入的是文档对。比如现在有三个文档D1、D2、D3，排序为D1&gt;D2&gt;D3，那么输入应该是&lt;D1，D2&gt;、&lt;D2，D3&gt;、&lt;D3，D1&gt;，对应的训练目标是“文档1是否应该排在文档2的前面”。这种方法在模型构造上与大部分单文档方法可以共用原始模型，好处在于模型训练出来的是对应文档组的排序关系，在复杂、高维度、不易解析的情况下，有时会比单文档方法的排序结果更接近真实值。但是其缺点也很明显，其中一个缺点是由于模型输出的两个文档之间有排序先后关系，如果靠前的位置出现错误，那么对于整体排序的影响是远远大于单文档方法。另一个缺点是模型较难对输出结果进行评价且训练困难，由于不同情况下文档之间的关系多种多样，而且不同情况下，不同的输入对训练产生的影响不同，因此很难对模型整体输出结果做出评价。同时，文档对方法一方面增加了标注的难度，另一方面增加了训练的时间。 而文档列表方法与前两者都不同，其考虑的是模型整体的排序结果，输入是一个文档列表，且每一个文档的对应位置都已经锁定。例如，输入是[D1，D2，D3]，那么该方法认为单次样本的输入中，排序为D1&gt;D2&gt;D3。该方法的代表模型有Lamda Rank、Ada Rank等。得益于NDCG等新的评价方式，文档列表排序在模型训练的过程中可以有效地迭代数据。而且由于输入的单个样本是一组标注好的序列，模型在迭代的过程中也更容易贴近用户需求。文档列表排序方法的缺点也有很多。首先，在理想情况下，其确实更容易保证模型的排序结果贴近用户需求，但是这需要前期大量的标注工作或者说对于使用场景有着明显的限制。其次，由于独立样本复杂，模型的训练成本大于其他两种方法。最后，在现实生活中，我们往往很难能确定地输入单个样本的排序。我们还是要根据具体场景选择合适的排序方法。 1.2排序学习示例Bagging和Boosting算法对比如下图所示。 Boosting示意图 Bagging示意图 1.3搜索和深度学习1.3.1 深度神经网络（Deep Neural Network，DNN）DNN模型的优缺点。作为深度神经网络模型，其对于非线性问题的处理优势不言而喻，而在搜索乃至推荐系统中，由于很多时候特征与训练目标间的关系并不清晰，所以DNN模型对高维度特征的提取更是一大优势。这也是有些场景下，DNN的表现要比树模型、FM家族模型要好的原因。但是DNN本身也有不足，如对类别特征的支持不好，随着类别特征的增多，甚至可能产生维度爆炸的情况；对系统算力的要求较高，因为为了能更好地提取高维度特征，模型的深度可能会使算力不足的问题进一步加剧；可解释性较差，随着隐藏层的增多，评估不同特征对模型的影响变得尤为困难。 1.3.2 深度结构语义模型（Deep Structured Semantic Models，DSSM）深度结构语义模型（Deep Structured Semantic Models，DSSM）在计算语义相关度方面提供了一种思路。它的思想很简单，就是将Query和Title的海量点击曝光日志用DNN(Deep Nature Networks)表达为低维语义向量，并通过余弦相似度来计算两个语义向量的距离，最终训练出语义相似度模型。DSSM模型既可以用来计算两个句子的语义相似度，又可以获得低维语义向量表达。 DSSM模型从下往上可以分为三层：输入层、表示层、匹配层，如下图所示。 （1）输入层 输入层的作用是把一个句子映射到一个向量空间（中文可以处理为单字的形式），并输入到DNN中，然后计算Bi-gram或者Tri-gram。 （2）表示层 表示层采用BOW（Bag of Words）的方式，将整个句子的词不分先后顺序地放到一个袋子里，如下图所示。 用Wi表示第i层的权值矩阵，bi表示第i层的偏差项，则第一个隐藏层向量l1（300维）、第i个隐藏层向量li（300维）、输出向量y（128维）可以分别表示为： （3）匹配层 最后，在匹配层用三角余弦计算Query和Doc的相似度。 DSSM用字向量作为输入既可以减少对切词的依赖，又可以提高模型的范化能力，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用单词向量化的方式（如Word2Vecor的词向量）或者主题模型的方式（如LDA的主题向量）来直接做词映射，再把各个词向量累加或者拼接起来。但由于Word2Vecor和LDA都是无监督训练，会给整个模型引入误差，而DSSM采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。 DSSM的缺点是采用词袋模型（BOW），导致丧失了语序信息和上下文信息，而且采用弱监督、端到端的模型，使预测结果不可控。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第四章-1--搜索系统框架及原理","slug":"SearchAndRec-Chapter04-2","date":"2021-05-09T22:00:37.000Z","updated":"2021-05-09T14:01:56.344Z","comments":true,"path":"2021/05/10/SearchAndRec-Chapter04-2/","link":"","permalink":"http://renxingkai.github.io/2021/05/10/SearchAndRec-Chapter04-2/","excerpt":"","text":"1.文本分析在搜索过程中需要对文本进行处理，比如对查询的分析以及建立索引时对文档内容的分析，我们将这部分内容称作“Query理解”。其主要包括Query预处理、Query纠错、Query扩展、Query归一、联想词、Query分词、意图识别、term重要性分析、敏感Query识别、时效性识别等。如下图所示，用户输入一个Query，为了缓解后端压力，搜索系统会先去Cache中查询Query是否被命中，如果被命中，则直接返回该Query的结构化数据。如果没被命中，就需要后续对Query进行一系列处理。首先是简单的预处理，大小写、全半角、繁简体转化以及对过长的Query进行截断处理，接着可能需要先对Query进行分词，使用分词的Term结果进行错误检测，然后再对Query分词做重要性分析和紧密度分析，对无关紧要的词汇做丢词等处理。有了分词Term及其对应的权重、紧密度信息后，搜索系统可以进行意图识别。意图识别包括模糊意图识别和精准意图识别。除此之外，部分搜索场景还需要对Query进行敏感识别及时效分析等其他处理，以及对前面各部分处理后的结果进行人工干预，解决相应的负例。 1.1查询处理1.1.1搜索查询中的“术”对文档处理主要是一个词法分析的过程。词法分析的过程是将字符串（文档中的文本内容）转换成词条的过程，这些词条可以作为索引词条。因此，词法分析的主要目的是识别文本中的词条。 在对英文进行分词的过程中，除了空格分隔符，还有几种特殊的情况：数字、连字符、标点符号和字母的大小写。数字一般不适合用作索引词条，因为对于数字来说，如果不参考上下文，它没有明确的含义。对于连字符来讲，目前常用的处理方法是首先采用一定的规则选出那些对词义有影响的连字符，然后将其他连字符过滤掉。对于文本来讲，标点符号将被全部去除，但对于那些成为单词一部分的标点符号来讲，一般不可以去除。对于字母大小写，其处理方法一般是将文本中的所有词条转换成大写或者小写，但在某些特殊情况下，需要对大小写进行区分。 对于中文的词法分析，最关键的就是中文分词。在中文分词的过程中，有两大难题：一是歧义识别，所谓歧义是指同样的一句话可能有两种或者多种切分方法；二是新词识别，“新词”的专业术语为“未登录词”，也就是那些在字典中没有收录过的词。 中文分词的方法单字切分就是按照中文一个字、一个字地进行分词。二分法是指每两个字进行一次切分。词库分词是用一个已经建立好的词的集合去匹配目标，当匹配到集合中已经存在的词时，就将其切分出来。 中文分词算法现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。 基于字符串匹配的分词方法。它又叫作机械分词方法，是按照一定的策略将待分的字符串与一个充分大的机器词典中的词条进行匹配，若在词典中找到对应字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，基于字符串匹配的分词方法可以分为正向匹配和逆向匹配；按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最小（最短）匹配；按照是否与词性标注过程相结合，又可以分为单纯分词方法、分词与标注相结合的一体化方法。下面介绍两种机械分词方法，即正向最大匹配法和逆向最大匹配法。正向最大匹配法（Forward Maximum Matching Method，FMM）的算法思想是，选取包含6~8个汉字的符号串作为最大符号串，将最大符号串与词典中的单词条目相匹配，如果不能匹配，就削掉一个汉字继续匹配，直到在词典中找到相应的单词为止。其匹配的方向是从左向右。逆向最大匹配法（Backward Maximum Matching Method，BMM）和正向最大匹配算法相似，只是匹配的方向是从右向左，比正向最大匹配的精确度高一些。下图为正向最大匹配和逆向最大匹配的算法示例。 基于理解的分词方法。通过计算机模拟人对句子的理解，达到识别词的效果。其基本思想是在分词的同时进行句法、语义分析，利用句法和语义信息来处理歧义现象。通常，基于理解的分词包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息，以便对分词歧义进行判断，即模拟人理解句子的过程。 基于统计的分词方法。从形式上看，词是稳定的字的组合，因此在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词。换句话说，字与字相邻共现的频率或概率能够较好地反映词的可信度。我们可以对语料中相邻共现的各个字组合出现的频率进行统计，计算它们的共现信息。共现信息体现了汉字之间结合关系的紧密程度。当紧密程度高于某一个阈值时，便可认为此字组可能构成一个词。这种方法只需对语料中的字组出现频率进行统计，不需要词典，因而又叫作无词典分词法或统计取词方法。 1.1.2搜索系统中的“道”上述内容只是搜索查询中的“术”，要想真正理解搜索引擎，还需要深刻理解搜索系统的内核，即搜索系统的“道”。查询理解便是我们需要理解的“道”，它在搜索系统中是一个比较重要的模块。这个模块的主要目的是推断查询，通过提供建议来引导搜索引擎判断出用户的真实意图，并改进查询以获得更好的结果，下图所示。 (1)查询建议 查询建议，也被称为查询下拉建议、查询下拉推荐或者查询自动补全。它是指搜索引擎系统根据用户当前的输入，自动提供一个查询候选列表供用户选择。查询建议在搜索引擎和广告竞价平台中已经是标配的组件。它可以帮助用户明确搜索意图，减少用户的输入并节约搜索时间，提高用户体验。各个搜索系统的查询建议的处理流程基本相同，不同点主要体现在后台的查询候选产生机制上。 查询建议也有一些常用的算法，能够在查询第一步帮助用户获得满意的结果。常见的下拉推荐算法有：1）基于日志的下拉推荐；2）对页面浏览（Page View，PV）数据进行扩展，基于综合指标的下拉推荐；3）基于用户行为的下拉推荐；4）基于Query Session的下拉推荐。下拉推荐的目的和常用算法如下图所示。 全量日志的自动补全法Most Popular Completion，MPC是最常用的基于日志统计信息的方法。如图4-13所示，基于日志的下拉推荐流程主要分为三步：1）在海量的Query日志中，统计一段时间内每个Query的PV和点击数；2）经过相似度计算，得到与用户输入Query相似的候选Query集合；3）在相似Query候选集中，按照Query的PV和点击数进行排序。为提升匹配效率，我们可以通过对历史搜索Query按PV统计量筛选并预处理，然后分别构建前后缀Trie树，以便对输入Query进行前缀及后缀匹配召回。最后对召回的结果进行排序，如果是仅简单按PV量降序排列，可以在Trie树节点中存放PV信息，并用最小堆等结构进行Top N召回。 全量日志的自动补全法方法也存在一些问题：1）对于Top N Query，该推荐方法效果较好，但是对于长尾Query，可能无法挖掘到相似Query，而现实中长尾Query占了很大比例；2）候选Query语义相同，仅仅是词语顺序不同，可能导致推荐位置的浪费；3）推荐Query可能存在质量问题，如一些质量低的Query由于点击或者PV过高而被推荐，导致质量较高的Query不能被展示。 为了改善上述基于日志的下拉推荐存在的问题、增加高质量Query被推荐的机会、减少作弊行为、给出更加合理的Query排序结果，基于综合统计指标的下拉推荐方法被提出，以代替原始的基于日志的下拉推荐方法。基于综合统计指标的下拉推荐方法包括更多维度，如PV、UV、CTR、转化率等，通过多维度对Query排序能够有效防止作弊行为的发生，挖掘到高质量Query。该方法通过逻辑回归将上述指标拟合成一个实数，与基于日志的下拉推荐方法的不同之处在于：1）第一步计算每个Query的综合统计指标，不仅仅是PV值；2）Query综合统计指标不仅考虑了Query的历史PV/点击信息，而且考虑了用户行为信息，使得质量高的Query获得更多的展现机会。 基于综合统计指标的下拉推荐方法解决了高质量Query展现和排序的问题，但该方法还是和基于日志的方法一样，主要依赖Query自身的特征，比如搜索Query和候选Query之间的联系仅仅是两者的前缀相同。这种简单的动态特征没有将搜索Query和候选Query紧密地结合在一起，同时静态特征和动态特征的组合都是基于线性加权的。为了使两者之间建立动态关联，基于CTR的方法被提出。如果搜索Query和候选Query之间的关联越强，它的CTR就会越高，反之则会比较低。CTR预估是通过逻辑回归模型预估Query的CTR来实现的，使用的特征主要有：1）搜索Query和推荐Query的相关特征；2）搜索Query和推荐Query类目的相关特征；3）候选Query综合统计指标的相关特征；4）搜索Query和推荐Query的词性特征；5）搜索Query和推荐Query对应的结果页面特征等。 只依靠短短的Query信息去准确识别用户的意图是不够的，还需要结合用户的一些信息对用户意图进行推测。根据用户的信息建模，比如用户性别、年龄、学历以及兴趣偏好等，结合初排结果再次进行排序。基于用户行为的下拉推荐方法的主要步骤有：1）计算用户和Query的相关个性化特征；2）建立合适的评价体系对这类特征进行权重学习，这里可以使用逻辑回归模型，比如AUC模型。但是一般情况下，用户的某一个场景下的行为信息较少，需要挖掘其他场景下的行为信息作为补充，同时还存在冷启动问题。 另外，点击的URL也可以作为信息源，即使用用户点击的URL对简短的Query信息进行补充和表示。定义一次完整的Query Session包含搜索Query和点击的URL。基于Query Session的下拉推荐方式的主要步骤是：1）将搜索Query以及对应的点击的URL从日志库中提取出来，预处理后聚类；2）给定一个搜索Query，确定所属聚类类别，计算其在这个类别中的排序；3）返回排序靠前的相关联的Query。候选Query的排序由与搜索Query之间的相似度以及所属类别中的支持度决定，该指标通过点击日志计算得到。Query的热度和用户需要的支持度不一定成正比，因此需要将相似度和支持度归一化，线性组合计算Query的排序结果；或者考虑向用户输出两种方法的推荐列表，并让他们调整每种方法的权重。为了计算两个Query之间的相似度，我们会对每个Query中的每个词语向量化。词典是由点击的URL分词后的结果去掉停用词后组成的集合，词语的权重由词语出现的次数和URL点击的次数决定。给定一个Query(q)和一个URL(u)，令Pop(q，u)表示u在q下的热度；Tf(t，u)是词语t在u内出现的次数；q的表示向量为q，q[i]表示Query中的词语在词袋中所处的位置i，其计算公式是： 总和涵盖所有点击的URL。该表示形式通过经典Tf-idf加权中的点击流行度来更改逆文档频率。使用余弦函数计算相似度，如果两个文档的单词出现比例相似（但长度或单词出现顺序可能不同），则认为它们相似。 即使使用同样点击的URL的日志，Query聚类的方法也不尽相同。图4-14表示搜索Query和点击的URL之间的关联，左侧代表查询qi；右侧表示搜索Query对应的点击的URL；Query和URL之间的连接关系eij表示在qi下点击uj；边的权重wij表示整个日志中，在qi下点击uj的总次数。该方法便于寻找相似的Query，两个Query之间共同点击的URL越多，说明两个Query越相似。 单个Query对用户意图识别的信息量是不够的。例如有用户搜索了“小米”，我们并不能判断用户搜索的是小米这个品牌还是食物，但当我们发现用户在搜索“小米”之前还搜索了“智能手机”，就能大致理解用户的意图。所以，除了需要分析用户当前的Query外，对用户Query上下文的分析也会帮助我们理解用户查询意图。我们根据会话数据挖掘用户Query上下文序列，并结合Query的聚类结果构建概念序列后缀树（Concept Sequence Suffix Tree）。当用户提交Query后，推荐系统就可以根据该后缀树快速给出合适的下拉推荐。具体的下拉推荐方案如下图所示。 (2)查询更正 查询更正主要是指Query纠错，也就是对用户在搜索输入时的错误Query进行检测和更正。用户在使用搜索引擎时，可能由于输入法、手误或者理解偏差等造成输入错误，使返回结果不能满足用户需求或者无返回结果。因此，搜索引擎需要对此进行处理，提高搜索的准确率和召回率，为用户提供更好的使用体验。 根据Query中是否包含不在词典中的词语，可以将Query的错误类型分为两种：Non-word和Real-word。Non-word错误一般出现在带英文单词或数字的Query中，不会存在中文错误的情况。所以，中文Query一般只存在Real-word错误，而带英文、数字的Query则可能存在上述两类错误。下图对常见错误类型进行了归类并给出了相应的例子。 Query纠错可以通过噪声信道模型来理解，假设用户原本想输入Qreal，但是经过噪声信道之后，可能输入到搜索引擎中的是Qnoise，对Qnoise进行去噪处理，最大限度地还原为Qdenoise，使得Qdenoise≈Qreal。 已知Qnoise，求解最大可能的Qreal，公式如下： Query纠错一般包括两个子任务：错误检测和错误纠正。其中，错误检测就是识别出错误的位置。对于Non-word类型的错误，我们可以根据词汇是否在维护的词典中进行判断。不过，该方法的效果取决于维护词典的规模和质量。对于Real-word类型的错误，每个词汇都可作为错误候选词。至于错误纠正，即在检测出Query存在错误的基础上对错误部分进行纠正，主要包括纠错候选召回、候选排序选择两个步骤。在进行候选召回时，没有一种策略能覆盖所有错误类型，一般采用多种策略进行多路候选召回，然后在多路候选召回的基础上通过排序模型进行最终的候选排序。在纠正Non-word类型的错误时，搜索系统可以查找词典中与错误词汇最相近的词语。常见的方法有计算最小编辑距离和最大噪声信道概率。在纠正real-word类型的错误时，搜索系统可以从发音和拼写多个角度，查找与错误词汇最相近的词语集合作为拼写建议。常见的方法有计算最大噪声信道概率和分类。 对于英文错误、多/漏字、颠倒错误，搜索系统可以通过编辑距离度量召回。编辑距离表示一个字符串通过插入、删除、替换操作转化为另一个字符串所需的操作次数，例如hapy转化成happy的编辑距离是1。由于搜索Query数量庞大，如果计算Query两两之间的编辑距离，计算量会非常大，因此一般采用启发式策略，比如首字符相同的情况下将长度小于某一值的Query分到一个桶中，计算桶中的Query两两之间的编辑距离。对于上述方式不能够处理的情况，比如顺序颠倒、漏字多字的情况，还可以利用编辑距离满足两边之和大于第三边的特性对多叉树进行剪枝。首先随机选取一个Query作为根节点，然后自顶向下对所有Query构建多叉树，树的边为两个节点Query的编辑距离。给定一个Query，需要找到与其编辑距离小于等于n的所有Query，并自顶向下计算与相应节点Query的编辑距离d，接着只需递归考虑边值在d–n到d+n范围的子树即可。如下图所示，需要查找所有与“十面埋弧”编辑距离小于等于1的Query，由于“十面埋弧”与“十面埋伏”的编辑距离为1，此时只需考虑边值在1–1到1+1范围的子树，因此不用考虑将“十面埋伏怎么样”作为根节点子树。 据统计，英文中80%的拼写错误的编辑距离是1，大多拼写错误的编辑距离小于等于2，基于此可以减少大量不必要的计算。通过最小编辑距离获取拼写建议候选集（Candidate w），再从候选集中选出概率最大的w作为最终的拼写建议，然后基于噪声信道模型，进一步计算候选词w的概率p(w)和在候选词w出现的情况下x的条件概率p(x|w)，通过对语料库统计，即可得到p(w)、p(x|w)。了采用一元词法模型Unigram，还可以推广到二元词法模型Bigram，甚至三元词法模型Trigram及更高阶，以更好地融入上下文信息。 对于等长的拼音字形错误，我们还可以使用HMM模型召回。例如：连一裙→连衣裙，可以将错误Query“连一裙”作为观测序列，正确Query“连衣裙”作为隐藏状态，映射关系可以通过人工整理的同谐音和形近字混淆词表、编辑距离度量召回的相近英文单词以及挖掘好的纠错片段对得到。通过对搜索行为日志统计得到模型参数，然后采用维特比算法对隐藏状态序列矩阵求解最大纠错概率，得到候选纠错序列。下图就是一个HMM模型处理等长拼音字形错误类型的示例。进一步地，我们还可以尝试利用深度学习模型充分挖掘搜索点击行为及搜索上下文来纠错候选召回，如采用Seq2Seq、Transformer、Pointer-Generator Networks等模型进行端到端的生成改写，通过引入注意力、记忆存储等机制以及结合混淆词表进行优化。 1.2 意图理解用户搜索意图的识别是搜索文本分析的重要部分，也是最具挑战的部分。其存在的难点主要有如下几点。1）用户输入Query不规范。由于不同用户对同一事物认知不同，用户在通过自然语言描述时会存在差异，甚至可能会出现Query表达错误和遗漏的情况。2）歧义性和多样性。搜索Query不能明确表达用户真实意图，可能带来歧义；或者用户搜索的内容本身可能有多种含义，比如：“小米”可能是食物，也可能是品牌。 根据用户信息及搜索上下文可实现个性化意图识别，比如针对不同的用户（年龄、性别等），搜索同一个Query的意图可能不一样，用户当前Query的意图可能和上下文Query相关。按照用户意图明确程度，搜索意图识别可以分为精准意图识别和模糊意图识别。 精准意图识别精准意图识别是指用户所表达的意图已经相当明确，可以根据Query锁定一个资源目标。精准意图识别在垂直搜索领域较为常见，以应用商店搜索为例，用户搜索“下载微信”的意图很明确，就是想下载微信App，那么将微信App展示在第一位即可。一般在排序模型拟合较好的情况下对应的精准资源能够排在首位，但是以下情况可能会引起排序不稳定，进而导致对应的精准资源不能排在首位，具体包括：1）长尾Query数据稀疏，模型学习不充分；2）引入用户个性化特征，排序结果因人而异；3）以商业化为导向的排序影响相关性体验。因此，我们需要通过一定策略识别出精准意图并置顶。对于垂直搜索领域，精准意图一般是在给定Query的情况下，找到与其精准对应的item。我们可以通过文本匹配的方式，对&lt;Query，item&gt;对进行精准二分类。常用的分类模型有TextCNN、LSTM以及DSSM等。对于长尾Query且文本完全包含item的情况，由于用户行为量不够丰富，利用分类模型可能无法召回，直接进行文本匹配提取可能存在歧义，因此可以转化成NER任务，通过BiLSTM-CRF等序列标注模型进行item实体识别。另外，针对问答型任务，比如“姚明的身高是多少？”，可以通过召回“姚明”“身高”字样的页面为用户提供答案，但如果能够直接给出这个Query的答案，用户体验会更好。这类问答型任务一般需要结合知识图谱来实现，传统做法是先对Query进行词法、句法以及语义解析，识别出主要实体，再基于这些主题构造相应的查询逻辑表达式，进而去知识库中查询答案。近年来，业内陆续提出将问题和知识库中的候选答案映射成分布式向量进行匹配，以及利用CNN、LSTM、记忆网络等对问题及候选答案建模来解决。 意图分类在进行Query意图分类前，需要先制定一套意图标签用于全面覆盖用户意图需求。这里需要对Query侧和item侧的标签体系进行统一，以便在预测出某个Query的意图标签分布后直接用标签去倒排索引中召回属于这些标签的item。在电商场景下，根据Query识别该商品的类别，按照类别召回该类别下的商品，可以解决Query无结果情况，扩大召回量。由于搜索Query长度较短且意图存在多种可能，因此意图分类可以归为多文本多标签分类任务。在样本选取上，可以通过关联用户搜索行为分布及理解item获得的标签或站点所属行业分类信息等自动构造样本。对于可能存在的样本类别不平衡的问题，我们需要对样本进行上下采样等处理，或采用主动学习等方法进行高效的样本标注。至于模型方面，传统的文本分类主要采用向量空间模型或特征工程表征文本，然后用贝叶斯、SVM、最大熵等机器学习模型进行训练预测。随着Word2vec、GloVe、Fasttext等分布式词向量技术的兴起，传统自然语言处理任务需要做大量特征工程的局面被打破。通过分布式向量对文本进行表示，然后接入其他网络进行端到端分类训练的做法成为主流，如简单又实用的浅层网络模型Fasttext。Fasttext是从Word2vec衍生出来的，其架构和Word2vec架构类似，核心思想是将整篇文档的词及n-gram向量叠加平均后得到文档向量，然后使用文档向量做softmax多分类。相对于Word2vec，Fasttext是在输入层考虑字符级n-gram特征，这在一定程度上解决了未登录词识别问题，并且在输出层支持有监督的任务学习。Fasttext训练简单，且线上接口性能很高，但因为采用相对简单的浅层网络结构，准确率相对较低。为此，我们进一步尝试了一些深度神经网络模型，如：TextRNN、TextCNN、Char-CNN、BiLSTM+Self-Attention、RCNN、C-LSTM、HAN、EntNet、DMN等。这些模型通过CNN/RNN网络结构提炼更高阶的上下文语义特征以及引入注意力机制、记忆存储机制等，有效提高了模型的分类准确率。其实，对于Query短文本分类来说，采用相对简单的TextRNN/TextCNN网络结构就已经能达到较高的准确率。其中，TextRNN通过使用GRU/LSTM编码单元能更好地捕获词序和较长长度的上下文依赖信息，但训练耗时相对较长。TextCNN主要通过不同尺寸的卷积核捕获不同局部窗口内的n-gram组合特征，然后通过max-pooling或kmax-pooling保留变长文本中一个或多个位置的最强特征，并转换为固定长度的向量再做sigmoid/softmax分类。为进一步提高网络性能、加速模型收敛，我们还可以考虑在网络中加入dropout及batch normalize等防过拟合机制，以及在输入层融入Word2vec、GloVe、Fasttext等模型预训练得到的向量。TextCNN在捕获长程依赖信息方面不如TextRNN，但对于长度相对较短的Query的推荐效果相对较好，而且其训练速度及在线接口性能也都比较符合要求。 1.3 其他文本分析方法1.层次聚类 目前，在文本挖掘领域使用最广泛的聚类算法是层次聚类。层次聚类包括凝聚式层次聚类和分裂式层次聚类。凝聚式层次聚类又称为自底向上的层次聚类，是将集合中的每个对象作为一个单独的类别，然后逐渐合并这些类别形成更大的类别，直到满足聚类终止条件为止。分裂式层次聚类又称为自顶向下的层次聚类，它的聚类过程与凝聚式层次聚类相反：首先将集合中的所有对象置于同一个类别，然后对这个类别不断细分，直至达到聚类终止条件。相比凝聚式层次聚类，分裂式层次聚类在分裂过程中所要依据的规则更难确定，并且细节方面的处理过程也更加复杂，因此其适应用范围比较窄。本节采用的是凝聚式层次聚类。 凝聚式层次聚类的基本流程如下。1）将数据集中的每个点都作为一个独立的类别，类间的距离近似等于相应数据点的距离。2）找到聚类最近的两个类别，然后合并这两个类别。3）计算新合并的类别和原来每个类别之间的距离。4）重复第2步和第3步，直到所有的数据都聚到一个类别中，或者满足聚类终止条件（预先设定的类别个数或者距离的阈值）为止。 类别合并的方法有三种，分别是单连接法、全连接法和平均连接法。 单连接法：也叫最短距离法，类间距离用两个类中最近的两个数据点的距离表示。距离作为度量条件，一旦最近的两个类别之间的距离小于预先设定的距离阈值，算法流程结束。 全连接法：也叫最长距离法。与单连接法选两个类中最近的两个数据点的距离作为类间距离相反，它选择将两个类中距离最远的两个数据点的距离作为类间距离。 平均连接法：无论是单连接法还是全连接法都容易受到极端值的影响，造成聚类的不稳定。与上述两种方法不同，平均连接法选取两个类别中所有对象的平均距离作为类间距离。该方法更加合理，但计算较复杂。 2.K均值聚类 K均值聚类是基于样本集合划分的聚类方法，其事先确定样本集合划分的类别数k，将所有样本分到这k个类中，目标是每个样本到所属类别的中心的距离最小。由于每个样本只能属于一个类别，所以K均值聚类是硬聚类。 K均值聚类的基本流程如下。1）从样本集中随机选取k个样本点作为初始聚类中心。2）计算聚类对象到聚类中心的距离，将每个样本指派到与其最近的聚类中心的类中。3）计算当前各个类中的样本的均值，作为新的聚类中心。4）重复第2步和第3步，直到迭代收敛或者满足聚类终止的条件为止。 K均值聚类属于启发式方法，不能保证收敛到全局最优，且初始聚类中心的选择会直接影响聚类的结果，因此在选择初始聚类中心时可以考虑用层次聚类对样本进行聚类，得到k个类时停止，然后从每个类中选取一个与中心聚类最近的样本点。这里类别数k需要事先指定，但实际中往往最合适的k值是未知的，所以需要不断尝试k值聚类，检验聚类的结果，推断出最优的k值。聚类结果的质量可以使用类的平均直径来衡量。一般情况下，类别数变小时，平均直径会增加；类别数超过某个值，且平均直径不变时，这时的k值就是最优值。 3.LDA主题模型 主题模型广泛应用于自然语言处理领域，用于挖掘文本中潜在的主题信息。它是描述主题信息的一系列数学模型，本质是形式化地刻画出主题信息。下面介绍概率主题模型的发展历程——从潜在语义分析模型LSA到概率潜在语义分析模型PLSA，最后讲述目前应用最广泛的LDA主题模型。 （1）LSA模型 在向量空间模型（Vector Space Model，VSM）中，各个单词之间是完全相互独立的，所以很难解决一义多词和一词多义问题。因此，Deerwester等人提出了潜在语义分析（Latent Semantic Analysis，LSA）模型。LSA和VSM模型的相同之处是，使用向量表示词语和文本之间的关系；不同之处是，LSA将词语和文档映射到潜在语义空间，提出单词和文本之间存在主题层这一语义关系，这比以往将单词视为文本的唯一表示元素有了巨大的进步。LSA模型结构如下图所示。 上图中，A矩阵是一个稀疏矩阵，行表示词汇，列表示文档，矩阵中的元素是词汇在文档中出现的次数；等式右边的三个矩阵也有清晰的物理含义。X矩阵是对词进行分类的结果，行表示词汇，列表示语义类别，矩阵中的元素表示词汇在对应类别下的相关性；Y矩阵是对文档的分类结果，行表示主题，列表示文档，矩阵中的元素表示每篇文档在不同主题中的相关性；B矩阵则表示词的类和文档的类之间的相关性。 LSA模型利用奇异值分解的方式，截取了最大的前K个特征值，从而完成了文本从高维单词空间到低维主题空间的降维映射，这种方式对语料集中噪声过滤起到了重要作用。此外，在低维的主题空间中，语义关系相似的单词的主题相同或者相似，所以该模型改善了原模型中一义多词的缺陷。然而，改善后的LSA模型还是无法完全解决原模型中一词多义的问题，还具有计算量过大的缺点，所以后来有研究者提出了用概率潜在语义分析模型来弥补LSA模型的不足。 （2）PLSA模型Hofmann基于LSA模型构建了概率潜在语义分析（Probability Latent Semantic Cnalysis，PLSA）模型。该模型利用文档、主题和单词三层联合发生的概率获得与文本最贴切的语义关系，并采用迭代算法（一般是EM算法）推断文本与主题之间的语义关系，然后采用主题的语义信息表示文本信息。PLSA模型结构如下图所示。 其中，方框代表重复次数，d表示文档，z代表主题，w代表单词，M代表该语料库中文档的总数，N是当前操作的文档所包含的单词数目。PLSA模型不仅达到了降维的目的，而且刻画出了主题层和单词层的关系。但该模型仍不是一个完备的概率生成模型，因为它没有使用概率来描述文本。并且PLSA模型中的模型参数和语料集中的文本数量呈正相关，这就造成了模型的过拟合。 （3）LDA模型 在2003年，Blei等人基于PLSA模型构建了一种三层贝叶斯结构的模型——隐狄利克雷分布（Latent Dirichlet Allocation，LDA）模型。LDA模型是一个真正意义上完备的概率生成模型，是在PLSA模型的基础上扩展了狄利克雷先验超参数α和β，构建出文档层、主题层、单词层的三层贝叶斯结构模型，解决了PLSA模型中过拟合的问题。LDA模型在文本——主题维度中利用狄利克雷分布来完成抽样，并且一致的先验超参数使模型参数不会因语料集中文本数量的改变而受到影响，具有良好的泛化能力。此外，LDA模型建立在严格的贝叶斯理论基础之上，因此具有良好的扩展性。目前LDA模型已广泛地运用于科学研究和实际生产中。 LDA模型假设总体语料库中的所有文本都服从主题空间的多项分布，同时每个主题都服从词项空间的多项分布。LDA模型结构如下图所示。 上图中，θ和φ分别表示文本——主题分布与主题——单词分布，其先验分布都是狄利克雷分布，而α和β则分别为其狄利克雷分布的参数，也称为超参数。一个语料库D中共有M篇文档，每篇文档d由N个数量不定的单词w组成。此外，每篇文档由K个主题的多项分布表示，而每个主题z又由V个单词的多项分布表示。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第四章-2--搜索系统框架及原理","slug":"SearchAndRec-Chapter04-1","date":"2021-05-08T15:50:14.000Z","updated":"2021-05-09T14:02:33.722Z","comments":true,"path":"2021/05/08/SearchAndRec-Chapter04-1/","link":"","permalink":"http://renxingkai.github.io/2021/05/08/SearchAndRec-Chapter04-1/","excerpt":"","text":"1.搜索系统的框架1.1基本框架 搜索系统通常由信息收集、信息存储、信息扩展及搜索计算4部分组成。搜索系统的具体结构上图所示。 （1）信息收集该部分包括但不限于爬虫和线下导入。对于网络爬虫部分，该组件的主要功能是收集全网实时/延时数据（这取决于搜索引擎的应用场景），并对收集的数据进行简单处理，如去重、停用敏感信息、过滤垃圾信息以及生成信息存储部分可以接收的数据，以便存储。线下导入的信息有时是作为爬虫信息的补充，如作为对缺乏规范描述的定义、定理等进行补充的备检信息；有时是作为推理或关系信息的主要依据，如作为对网络爬虫收集的信息进行关系处理的依据。 （2）信息存储该部分包括但不限于文件信息保存及索引保存等。信息存储的主要功能是提供高效的、泛化的、文档间可联动的搜索内容。所以在文件信息保存上，有时会因为场景的原因，将所有文件内容放置在内存中，所以会建立庞大的倒排索引和并发集群以提高搜索的响应效率。对于推理性搜索，为了提高搜索的响应效率，存储组件需要对应用于推理的节点（关键词、字段或者短语描述等）进行特殊处理。 （3）信息扩展除响应效率外，为了提高搜索的准确性，有时我们需要对搜索系统召回的内容进行相应扩展。扩展的目的是尽量避免无结果的搜索。在信息扩展部分，我们可以在爬虫程序中定义文档间关系，甚至定义推理逻辑实现，在信息存储部分可以屏蔽停用词、定义同义词，并在建立索引的时候就支持泛化能力。 （4）搜索计算为了理解搜索输入（通常称为Query）和召回排序，我们需要对Query进行生成和解析并提供召回的评分策略，以便对结果进行排序。 生成Query：搜索系统的输入通常是人类自然语言的输入，其表达形式包括但不限于关键词、短语、句子、文章段落，甚至图片等。所以，在生成Query时，通常会加入意图理解模块，以尽可能理解用户当前检索内容的真实意图。同时，受制于人类语言表达的缺陷，在自然语境下所表述的内容被计算机理解时可能会产生歧义，这也是意图理解模块试图解决的问题。通过意图理解模块，我们通常会得到当前搜索内容的主题或者检索要素。对于检索要素，可能需要进行拓展，这部分拓展主要依赖当前用户的行为和近期全网热度等。最后将整理的全部检索要素生成计算组件可理解的Query并输入计算模块。 解析Query：计算模块将搜索信息从信息存储组件中按照既定的评分标准排序召回。首先，搜索系统需要将Query导入信息延展组件进行拓展。信息拓展的主要目的是加大召回的覆盖率和提高召回的准确度。这里常用的手段与NLP技术相关，除了可以对最基本的同义词、近义词进行打分，还可以对所有相关检索要素的重要程度进行打分，比如检索要素是Java工程师，显然Java的重要程度远远大于工程师，此时就需要对这两个不同的检索要素赋予不同的权重。 信息存储组件负责实时或延时地将搜集到的数据存储起来。对于数据的时效性，我们需要考虑搜索系统的性能及应用场景，而最终选择采取哪种方式进行数据存储，则是由读者实际工作决定。在存储信息时，为了保证信息的高效存储，通用的存储方式有内存存储与索引存储，具体的构建方式会在相关章节详细介绍。在接收到Query之后，信息存储组件会将结果返回到信息延展组件。返回的信息包括但不限于：单个召回结果中检索要素命中的位置信息、单个召回结果中检索要素命中的频次信息、整体召回结果的统计与分布等。这里之所以需要返回大量的信息，是因为我们在考虑搜索系统整体准确度的时候，还需要对结果进行排序。而排序阶段的特征数据或者评分数据主要由信息存储组件提供。所以，有时候信息存储组件返回的数据量可能过大。在结构上，一些搜索系统倾向于将部分信息延展组件与信息存储组件甚至计算组件整合到一起，从而在返回结果的同时结束部分计算结果的统计。部分情况下，信息存储组件也会将部分结果返回至信息延展组件，主要是考虑到更为精确、细粒度的搜索结果可能导致召回不足。这就需要在信息延展组件中，对可能发生的召回不足的情况制定补救措施，重复进行搜索、收集搜索结果的流程，直至召回满足或无法召回为止。最后将搜索结果返回至计算组件，进行最终的排序并将结果展示给用户。 1.2搜索引擎的工作方式搜索引擎分为4部分，搜索器、索引器、检索器及用户接口，如下图所示。 搜索器。搜索器的功能是日夜不停地在互联网中漫游，搜集信息。它要尽可能多、尽可能快地搜集各种类型的新信息，还要定期更新已经搜集的旧信息，以免出现死链。搜索器有两种搜集信息的策略：第一种策略是从一个起始URL集合开始，顺着这些URL中的超链接（Hyperlink），以宽度优先、深度优先或启发式循环地在互联网中搜集信息。它会沿着任何一个网页中的URL集合“爬”到其他网页，重复这个过程，并把搜集到的所有网页存储起来。第二种策略是按照域名、IP地址划分Web空间，每个搜索器负责一个子空间的穷尽搜索。 索引器。索引器的功能是理解搜索器所搜索的信息，通过分析索引程序对收集的网页进行分析，提取相关网页信息（包括网页所在URL、编码类型、页面内容包含的关键词、关键词位置、生成时间、大小、与其他网页的链接关系等），根据相关度算法进行大量复杂计算，得到每一个页面的页面内容及超链接中每一个关键词与页面内容的相关度（或重要性），然后用这些相关信息建立网页索引数据库。 检索器。检索器的功能是根据用户查询在索引库中快速检出文档，对文档内容与查询的相关度进行评估，并对将要输出的结果进行排序，实现某种用户相关性反馈机制。检索主要过程如下：检索器对用户接口提出的查询要求进行递归分析，在用户接口中一般采用基本语法来设置检索条件。 用户接口。用户接口的作用是输入用户查询、显示查询结果、提供用户相关性反馈机制。其主要目的是方便用户使用搜索引擎，高效率、多方式地从搜索引擎中得到有效、及时的信息。用户接口的设计和实现使用了人机交互的理论和方法，以充分适应人类的思维习惯。用户输入接口可以分为简单接口和复杂接口两种。 2.数据收集及预处理2.1爬虫网络爬虫是搜索系统中很关键也很基础的组件，负责数据收集。通用的爬虫框架如下图所示。其工作原理是首先从互联网网页中选出一部分质量较高的网页的链接作为种子URL，把这些种子URL放到待抓取URL队列中，由爬虫从待抓取URL队列中依次读取，并通过DNS解析URL，把链接地址转化为网站服务器对应的IP地址。然后将IP地址和网页相对路径名称交给网页下载器，由网页下载器把网页内容下载到本地。这样做一方面可以将页面内容存储到网页库中，等待索引建立等后续处理；另一方面将下载页面的URL放到已经抓取的URL队列中，对已经爬取的页面进行记录，可以防止重复爬取。对于刚下载的网页，抽取其中的URL链接，并检查已抓取的URL队列，将没有抓取过的URL放入待抓取URL队列中，不断循环上述步骤直至待抓取的URL队列为空，即爬虫系统完成一轮完整的爬取流程。 该框架适用于绝大多数的爬虫系统，但不能代表全部。根据应用场景的不同，网络爬虫大概分为以下4种：1）通用型网络爬虫，负责爬取全网的资源，一般适用于大型搜索引擎；2）批量型网络爬虫，按照预先设置好的抓取目标和范围爬取资源，当爬虫完成目标后，就停止爬取；3）增量型网络爬虫，针对互联网中有变化的网页，比如新增、修改或者删除的网页，及时响应更新到本地网页库中；4）垂直型网络爬虫，关注特定主题或者特定行业的网页，难点是如何识别网页内容是否为特定主题或行业。 优秀的爬虫应该具备的特性主要有3点。1）高性能。此处的性能主要是指网页的下载速度，常用的评估爬虫性能的方式是以爬虫每秒能够下载的网页数量为指标，单位时间内下载的网页数量越多，爬虫的性能越高。2）健壮性。爬虫在遇到突发情况时，要能够做出正确的处理。当服务器宕机时，健壮的爬虫应该能够在服务器重启后，恢复之前抓取的内容和数据结构。3）友好性。友好性主要包括两个方面：一方面是保护网站私密性；另一方面是减少被抓取网站的网络负载。对于部分网站所有者不想被爬取的网页，网站所有者需要设置协议告诉爬虫哪些内容不能够抓取。常用的方法有：设置爬虫禁抓协议和网页禁抓标记。爬虫禁抓协议是指由网站所有者生成一个文件robot.txt，将其放在网站服务器根目录下，这个文件指明了哪些目录下的文件不允许抓取。友好的爬虫在抓取该网站的内容之前，应该先读取robot.txt文件并遵守协议。如果是某个网站不想被抓取，可以使用网页禁抓标记，即在网页的HTML代码里加入meta name=“robots”标记，其中content字段指出允许或者不允许爬虫做哪些行为，主要分为两种情形：一种是告诉爬虫不要索引该网页内容，以noindex标记；另一种是告诉爬虫不要抓取网页包含的链接，以nofollow标记。 评估爬虫的质量标准一般有三个：抓取网页的覆盖率、抓取网页的时效性和抓取网页的重要性。1）覆盖率，是指抓取网页的数量占互联网内网页总数量的比例，比例越高，覆盖率越高。2）时效性，是指爬虫对修改、删除的网页的反应速度，尤其是针对已经过期的网页，及时更新网页库。3）重要性直接影响搜索的准确率。 上图中待抓取的URL序列在爬虫系统中是关键部分。那么，URL序列顺序是如何确定的呢？将新下载页面中包含的链接直接追加到队列末尾，即宽度优先遍历策略，这是一种常见方式，但并不是唯一方式，还有非完全Pagerank策略以及大站优先策略。1）宽度优先遍历策略虽然没有明显地考虑网页重要性，但实验表明该方法效果很好，对很多链入的网页较为友好，并且链入的网页质量也较高。2）非完全Pagerank策略。Pagerank是一种著名的链接分析方法，最早应用于网页重要度排序，因此可以利用其对URL进行排序。由于其是一个全局算法，而爬虫下载的网页只是部分，所以由其计算的得分不一定可靠，但我们仍然可以借鉴它的思想：将已经下载的网页和待抓取的URL集中起来形成网页集合，在该集合内计算Pagerank，计算完成后，按照结果对待抓取的URL排序，然后依次抓取。3）大站优先策略。以网站为单位衡量网页的重要性，我们认为大站往往是知名企业，其网站质量一般比较高，所以一般优先搜索大站。 上文讲了如何对待抓取的URL排序，我们还需要考虑如何对已抓取的内容快速更新。常见的策略有：历史参考策略、用户体验策略和聚类抽样策略。1）历史参考策略。假设过去更新频繁的网页以后更新也会频繁，所以可以通过其历史更新情况，预估网页何时更新。2）用户体验策略。由于用户一般只对排名靠前的网页感兴趣，所以即使网页过期，如果不影响用户体验，那么晚些更新也是可以的。所以，判断一个网页什么时候更新，取决于网页的内容变化给排序结果带来的影响。3）聚类抽样策略。前两种策略都比较依赖网页历史信息，但在现实中，保存历史信息会增加搜索引擎负担，同时首次抓取的网页并不存在历史信息，因此以上两种方法都有缺陷。聚类抽样策略认为：网页具有一些属性，如网页内容、链接深度以及Pagerank值等，根据这些属性可以预测更新周期，而且，具有相似属性的网页也具有相似的更新周期，因此可以根据这些属性进行网页归类。同一类别的网页具有相同的更新频率。 2.2数据清洗爬虫在下载了网页之后，需要对这些数据进行清洗，其中最关键的部分就是对网页的去重处理（据统计，完全相同的页面大约占全部页面的20%）。处理相似网页的好处：1）可以节省存储空间，留出充足的空间存储有效网页；2）通过对以往数据的分析可以预先发现重复网页，在之后的网页收集中避开这些网页，提高网页收集效率；3）如果用户点击了死链接，可以将用户引导到一个内容相同的页面，进而有效提高用户体验。 在实际工作中，网页相似检验一般是在爬虫阶段完成的，具体流程如下图所示。 通用的网页去重流程如下图所示。对于给定的文档，首先通过一定的特征抽取方法，从文档中抽取一系列表示文档主旨的特征集合，之后通过算法直接查找相似文档。对于搜索引擎中数以万计的文档，算法的执行速度是需要重点关注的，因此很多高效的算法会在特征集合的基础上，对信息进一步压缩，如采用信息指纹相关算法将特征集合压缩成新的数据集合。（新的数据集合包含的元素数量远小于特征集合的数量，但同时也造成了信息丢失，所以需要衡量压缩率和准确度之间的平衡。）把文档压缩成信息指纹后，就可以进行相似度计算了。 经实验证明，SimHash算法是应用最广泛且表现最好的去重算法之一，是一种局部敏感哈希框架的实现特例。该算法流行的原因是：两个文档越相似，其对应的两个哈希值也就越接近，且哈希值的计算明显比文本计算快很多，同时也节省空间。 SimHash算法步骤如下。 对需要判断的文本分词形成该文本的特征单词，然后生成去掉噪声词的单词序列，并为每个词加上权重。 通过哈希函数将每个特征单词映射成固定长度的二进制表示。 利用权重改写特征的二进制向量，将权重融入向量中，形成一个实数向量。假设某个特征单词的权重是5，对二进制向量进行改写：若比特位数值是1，则改写成5；若是0，则改写成–5。改写完的特征累加后获得一个表示文档整体的实数向量。 再次将实数向量转换成二进制向量，规则如下：如果对应位置的数值大于0，设成1；如果小于等于0，则设置成0。得到两个文档的二进制数值后，利用海明距离来计算文档的相似度。 具体例子可以看下图。 2.3存储空间及分布设计搜索系统中最重要的数据结构是索引结构，索引结构越合理意味着查询越快捷。设想如果我们直接对一篇篇文本进行扫描，费时费力。如果我们建一个前向索引，依然是依次扫描每一个文档，对于多次出现的字符串不一一扫描，对于同一文档内的字符串查找采用二分查找的方式，这样可加快匹配速度，但这远远不够，还需要一个倒排索引的数据结构，让查询面向词和文档集，使搜索任务更快捷和简单。对于单个查询词，搜索就是字典查找的过程，不需要扫描所有文档。倒排索引的过程如下图所示。 这里说明一下排序过程。排序就是扫描每篇文档产生的文档号、单词号、出现位置这个三元组。按照单词号重新排序，单词号相同则按照文档号排序，单词号和文档号都相同则按照出现位置排序。 Trie树，又称单词查找树或键树，是一种树形结构，也是哈希树的变种形式。其典型应用是统计和排序大量的字符串，但不限于字符串。Trie树的核心思想是用空间换时间，利用字符串的公共前缀来降低查询时间的开销，以达到提高效率的目的。其包含三个特性：1）根节点不包含字符，除根节点以外，每一个节点都只包含一个字符；2）从根节点到某一个节点，路径上经过的字符连接起来为该节点对应的字符串；3）每个节点的所有子节点包含的字符都不相同。字符串and，as，bee，bus，cn，com构建的Trie树如下图所示。 在上图所示的Trie树中，root表示根节点，不代表任何字符，从第二层开始，白色表示分支节点，灰色表示叶子节点。从根节点到叶子节点，把路径上经过的字符连接起来，会构成一个词。而叶子节点内的数字代表该词在字典中所处的链路（字典中有多少个词就有多少条链路），具有共同前缀的链路称为串。树中的词只可共用前缀，不能共用词的其他部分。树中的任何一个完整词，一定是从根节点开始到叶子节点结束。所以，检索是从根节点开始到叶子节点结束，这种检索方式由于公共前缀的词都在同一个串中，能够大幅度缩小查询词汇的范围，同时从一个字符到下一个字符比对，不需要遍历该节点下所有子节点。因此，Trie树的时间复杂度仅和检索词的长度m有关：O(m)。综上可知，Trie树主要利用词的公共前缀缩小查词范围，通过状态间的映射关系避免所有字符的遍历，从而达到高效检索的目的。这种优势同样是需要付出代价的：1）由于结构需要记录更多的信息，因此Trie树的实现稍显复杂；2）Trie树词典不仅需要记录词，还需要记录字符之间、词之间的相关信息，因此在构建字典时必须对每个词和字逐一进行处理，而这无疑会减慢词典的构建速度；3）公共前缀虽然可以减少一定的存储空间，但Trie树相比普通字典还需表达词、字之间的各种关系，实现更加复杂，因此实际空间消耗相对更大。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第二章--搜索系统","slug":"SearchAndRec-Chapter02","date":"2021-04-26T21:44:20.000Z","updated":"2021-04-26T13:51:58.043Z","comments":true,"path":"2021/04/27/SearchAndRec-Chapter02/","link":"","permalink":"http://renxingkai.github.io/2021/04/27/SearchAndRec-Chapter02/","excerpt":"","text":"仅用于学习!!! 1.介绍搜索引擎是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，将用户检索到的相关信息展示给用户，为用户提供检索服务。搜索引擎包括4个接口，分别是搜索器、索引器、检索器和用户接口。 搜索器的功能是在互联网中漫游，负责发现和搜集信息。 索引器的功能是理解搜索器所搜索的信息，从中抽取出索引项，输出用于表示文档以及生成文档库的索引表。 检索器的功能是根据用户的查询在索引库中快速检出文档，并进行文档与查询的相关度评价，对将要输出的结果进行排序，实现某种用户相关性反馈机制。 用户接口的功能是输入用户查询、显示查询结果、提供用户相关性反馈机制。 具体的搜索引擎架构示意图如图2-1所示。 2.搜索引擎的分类搜索引擎可以分为以下4类：全文搜索引擎、元搜索引擎、垂直搜索引擎和目录搜索引擎。下面对这4类搜索引擎进行具体介绍。 全文搜索引擎。计算机通过扫描文章中的每个词，对每个词建立索引，记录词汇在文章中出现的次数和位置信息。当用户进行查询时，计算机按照事先建立好的索引进行查找，并将结果反馈给用户。按照数据结构的不同，全文搜索可以分为结构化数据搜索和非结构化数据搜索。对于结构化数据，全文搜索一般是通过关系型数据库的方式进行存储和搜索，也可以建立索引。对于非结构化数据，全文搜索主要有两种方法：顺序扫描和全文检索。顺序扫描，顾名思义，按照顺序查询特定的关键字，这种方式耗时且低效；全文检索需要提取关键字并建立索引，因此，搜索到的信息过于庞杂，用户需要逐一浏览并甄别所需信息。在用户没有明确检索意图情况下，全文检索方式效率稍显不足。Google和百度都是典型的全文搜索引擎。 元搜索引擎。按照功能划分，搜索引擎可以分为元搜索引擎（Meta Search Engine）和独立搜索引擎（Independent Search Engine）。元搜索引擎是一种调用其他独立搜索引擎的搜索引擎，其能对多个独立搜索引擎进行整合、调用并优化结果。独立搜索引擎主要由网络爬虫、索引、链接分析和排序等部分组成；元搜索引擎由请求提交代理、检索接口代理、结果显示代理三部分组成，不需要维护庞大的索引数据库，也不需要爬取网页。元搜索引擎具体实现逻辑如图2-2所示。请求提交代理就是将请求分发给独立搜索引擎。元搜索引擎可以按照用户需求和偏好请求实际需要调用的独立搜索引擎，该方式能够有效提升用户查询的准确率和响应效率。检索接口代理是将查询内容转化成独立搜索引擎能够接受的模式，并且保证不会丢失必需的语义信息。结果显示代理是元搜索引擎按照用户的需求采用不同的排序方式对结果进行去重、排序。元搜索引擎常用的排序方式有：相关度排序、时间排序、搜索引擎排序等。元搜索引擎的整体工作流程如下：用户通过网络访问元搜索引擎并向服务器发出查询，服务器接收到查询内容后，先访问结果数据库，查询近期记录中是否存在相同的查询，如果存在，返回结果；如果没有，将查询进行处理后分发到多个独立搜索引擎，并集中各搜索引擎的查询结果，结合排序方式对结果进行排序，生成最终结果并返给用户，同时保存现有结果到数据库中，以备下次查询使用。保存的查询结果有一定的生存期，超过一定时间的记录就会被删除，以保证查询结果的时效性。 垂直搜索引擎。垂直搜索引擎是针对某个行业的专业搜索引擎，是搜索引擎的细分和延伸，对特定人群、特定领域、特殊需求提供服务。它的特点是专业、精确和深入。垂直搜索引擎将搜索范围缩小到极具针对性的具体信息。垂直搜索引擎的结构与通用搜索系统类似，主要由三部分构成：爬虫、索引和搜索。但垂直搜索的表现方式与Google、百度等搜索引擎在定位、内容、用户等方面存在一定的差异，所以它不是简单的行业搜索引擎。用户使用通用搜索引擎时，通常是通过关键字进行搜索，该搜索方式一般是语义上的搜索，返回的结果倾向于文章、新闻等，即相关知识。垂直搜索的关键字搜索是放到一个行业知识的上下文中，返回的结果是消息、条目。对于有购房需求的人来说，他们希望得到的信息是供求信息而不是关于房子的文章和新闻。 目录搜索引擎。目录搜索引擎是网站常用的搜索方式，类似于书本章节目录。该搜索方式是对网站信息整合处理并分目录呈现给用户，整合处理的过程一般需要人工维护，更新速度较慢，而且用户需要事先了解网站的基本内容，熟悉主要模块，所以应用场景越来越少。 3.推荐系统协同过滤目前，基于协同过滤的推荐是推荐系统中应用最广泛、最有效的推荐策略。它于20世纪90年代出现，促进了推荐系统的发展。协同过滤的基本思想是聚类。比如，如果周围很多朋友选择了某种商品，那么自己大概率也会选择该商品；或者用户选择了某种商品，当看到类似商品且其他人对该商品评价很高时，则购买这个商品的概率就会很高。协同过滤又分为三种：基于用户的协同过滤、基于项目的协同过滤和基于模型的协同过滤。 1）基于用户的协同过滤的基本思想是首先找到与目标用户兴趣相似的用户集合，然后找到这个集合中用户喜欢并且没有听说过的物品推荐给目标用户。下图是基于用户的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户C喜欢商品A、商品C和商品D，用户A和用户C具有相似的兴趣爱好，因此把商品D推荐给用户A。 2）基于项目的协同过滤的基本思想是基于所有用户对推荐对象的评价的推荐策略。如果大部分用户对一些推荐对象的评分较为相似，那么当前用户对这些推荐对象的评价也相似。然后，将相似推荐对象中用户未进行评价的商品推荐给用户。总之，基于项目的协同过滤就是根据用户对推荐对象的评价，发现对象间的相似度，根据用户的历史偏好将类似的商品推荐给该用户。图2-8是基于项目的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户B喜欢商品A、商品B和商品C，用户C喜欢商品A，通过这些用户的喜好可以判定商品A和商品C相似，喜欢商品A的用户同时也喜欢商品C，因此给喜欢商品A的用户C也推荐了商品C。 3）基于模型的协同过滤的基本思想是基于样本用户的喜好信息训练一个推荐模型，然后根据实时的用户喜好信息进行推荐。其和上述两种协同推荐的不同点在于先对已有数据应用统计和机器学习的方法得到模型，再进行预测。常用的方法有机器学习方法、统计模型、贝叶斯模型和线性回归模型等。 基于协同过滤推荐的优点有：1）可以使用在复杂的非结构化对象上；2）能够发现用户新的兴趣爱好，给用户带来惊喜；3）以用户为中心的自动推荐，随着用户数量的增加，用户体验也会越来越好。缺点在于：1）存在冷启动问题，即在没有大量用户数据的情况下，用户可能不满意获得的推荐结果；2）存在稀疏性问题，即用户大量增长的同时，评价差异性会越来越大，推荐对象也越来越多，导致大量的推荐对象没有经过用户评价，部分用户无法获得推荐结果，部分推荐对象无法被推荐。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第三章--知识图谱","slug":"SearchAndRec-Chapter03","date":"2021-04-26T21:44:20.000Z","updated":"2021-05-14T07:14:03.235Z","comments":true,"path":"2021/04/27/SearchAndRec-Chapter03/","link":"","permalink":"http://renxingkai.github.io/2021/04/27/SearchAndRec-Chapter03/","excerpt":"","text":"仅用于学习!!! 1.介绍搜索引擎是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，将用户检索到的相关信息展示给用户，为用户提供检索服务。搜索引擎包括4个接口，分别是搜索器、索引器、检索器和用户接口。 搜索器的功能是在互联网中漫游，负责发现和搜集信息。 索引器的功能是理解搜索器所搜索的信息，从中抽取出索引项，输出用于表示文档以及生成文档库的索引表。 检索器的功能是根据用户的查询在索引库中快速检出文档，并进行文档与查询的相关度评价，对将要输出的结果进行排序，实现某种用户相关性反馈机制。 用户接口的功能是输入用户查询、显示查询结果、提供用户相关性反馈机制。 具体的搜索引擎架构示意图如图2-1所示。 2.搜索引擎的分类搜索引擎可以分为以下4类：全文搜索引擎、元搜索引擎、垂直搜索引擎和目录搜索引擎。下面对这4类搜索引擎进行具体介绍。 全文搜索引擎。计算机通过扫描文章中的每个词，对每个词建立索引，记录词汇在文章中出现的次数和位置信息。当用户进行查询时，计算机按照事先建立好的索引进行查找，并将结果反馈给用户。按照数据结构的不同，全文搜索可以分为结构化数据搜索和非结构化数据搜索。对于结构化数据，全文搜索一般是通过关系型数据库的方式进行存储和搜索，也可以建立索引。对于非结构化数据，全文搜索主要有两种方法：顺序扫描和全文检索。顺序扫描，顾名思义，按照顺序查询特定的关键字，这种方式耗时且低效；全文检索需要提取关键字并建立索引，因此，搜索到的信息过于庞杂，用户需要逐一浏览并甄别所需信息。在用户没有明确检索意图情况下，全文检索方式效率稍显不足。Google和百度都是典型的全文搜索引擎。 元搜索引擎。按照功能划分，搜索引擎可以分为元搜索引擎（Meta Search Engine）和独立搜索引擎（Independent Search Engine）。元搜索引擎是一种调用其他独立搜索引擎的搜索引擎，其能对多个独立搜索引擎进行整合、调用并优化结果。独立搜索引擎主要由网络爬虫、索引、链接分析和排序等部分组成；元搜索引擎由请求提交代理、检索接口代理、结果显示代理三部分组成，不需要维护庞大的索引数据库，也不需要爬取网页。元搜索引擎具体实现逻辑如图2-2所示。请求提交代理就是将请求分发给独立搜索引擎。元搜索引擎可以按照用户需求和偏好请求实际需要调用的独立搜索引擎，该方式能够有效提升用户查询的准确率和响应效率。检索接口代理是将查询内容转化成独立搜索引擎能够接受的模式，并且保证不会丢失必需的语义信息。结果显示代理是元搜索引擎按照用户的需求采用不同的排序方式对结果进行去重、排序。元搜索引擎常用的排序方式有：相关度排序、时间排序、搜索引擎排序等。元搜索引擎的整体工作流程如下：用户通过网络访问元搜索引擎并向服务器发出查询，服务器接收到查询内容后，先访问结果数据库，查询近期记录中是否存在相同的查询，如果存在，返回结果；如果没有，将查询进行处理后分发到多个独立搜索引擎，并集中各搜索引擎的查询结果，结合排序方式对结果进行排序，生成最终结果并返给用户，同时保存现有结果到数据库中，以备下次查询使用。保存的查询结果有一定的生存期，超过一定时间的记录就会被删除，以保证查询结果的时效性。 垂直搜索引擎。垂直搜索引擎是针对某个行业的专业搜索引擎，是搜索引擎的细分和延伸，对特定人群、特定领域、特殊需求提供服务。它的特点是专业、精确和深入。垂直搜索引擎将搜索范围缩小到极具针对性的具体信息。垂直搜索引擎的结构与通用搜索系统类似，主要由三部分构成：爬虫、索引和搜索。但垂直搜索的表现方式与Google、百度等搜索引擎在定位、内容、用户等方面存在一定的差异，所以它不是简单的行业搜索引擎。用户使用通用搜索引擎时，通常是通过关键字进行搜索，该搜索方式一般是语义上的搜索，返回的结果倾向于文章、新闻等，即相关知识。垂直搜索的关键字搜索是放到一个行业知识的上下文中，返回的结果是消息、条目。对于有购房需求的人来说，他们希望得到的信息是供求信息而不是关于房子的文章和新闻。 目录搜索引擎。目录搜索引擎是网站常用的搜索方式，类似于书本章节目录。该搜索方式是对网站信息整合处理并分目录呈现给用户，整合处理的过程一般需要人工维护，更新速度较慢，而且用户需要事先了解网站的基本内容，熟悉主要模块，所以应用场景越来越少。 3.推荐系统协同过滤目前，基于协同过滤的推荐是推荐系统中应用最广泛、最有效的推荐策略。它于20世纪90年代出现，促进了推荐系统的发展。协同过滤的基本思想是聚类。比如，如果周围很多朋友选择了某种商品，那么自己大概率也会选择该商品；或者用户选择了某种商品，当看到类似商品且其他人对该商品评价很高时，则购买这个商品的概率就会很高。协同过滤又分为三种：基于用户的协同过滤、基于项目的协同过滤和基于模型的协同过滤。 1）基于用户的协同过滤的基本思想是首先找到与目标用户兴趣相似的用户集合，然后找到这个集合中用户喜欢并且没有听说过的物品推荐给目标用户。下图是基于用户的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户C喜欢商品A、商品C和商品D，用户A和用户C具有相似的兴趣爱好，因此把商品D推荐给用户A。 2）基于项目的协同过滤的基本思想是基于所有用户对推荐对象的评价的推荐策略。如果大部分用户对一些推荐对象的评分较为相似，那么当前用户对这些推荐对象的评价也相似。然后，将相似推荐对象中用户未进行评价的商品推荐给用户。总之，基于项目的协同过滤就是根据用户对推荐对象的评价，发现对象间的相似度，根据用户的历史偏好将类似的商品推荐给该用户。图2-8是基于项目的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户B喜欢商品A、商品B和商品C，用户C喜欢商品A，通过这些用户的喜好可以判定商品A和商品C相似，喜欢商品A的用户同时也喜欢商品C，因此给喜欢商品A的用户C也推荐了商品C。 3）基于模型的协同过滤的基本思想是基于样本用户的喜好信息训练一个推荐模型，然后根据实时的用户喜好信息进行推荐。其和上述两种协同推荐的不同点在于先对已有数据应用统计和机器学习的方法得到模型，再进行预测。常用的方法有机器学习方法、统计模型、贝叶斯模型和线性回归模型等。 基于协同过滤推荐的优点有：1）可以使用在复杂的非结构化对象上；2）能够发现用户新的兴趣爱好，给用户带来惊喜；3）以用户为中心的自动推荐，随着用户数量的增加，用户体验也会越来越好。缺点在于：1）存在冷启动问题，即在没有大量用户数据的情况下，用户可能不满意获得的推荐结果；2）存在稀疏性问题，即用户大量增长的同时，评价差异性会越来越大，推荐对象也越来越多，导致大量的推荐对象没有经过用户评价，部分用户无法获得推荐结果，部分推荐对象无法被推荐。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"No Answer is Better Than Wrong Answer A Reflection Model for Document Level Machine Reading Comprehension 笔记","slug":"emnlp2020-NoAnswerisBetter","date":"2021-04-19T17:29:46.000Z","updated":"2021-04-22T07:03:45.608Z","comments":true,"path":"2021/04/20/emnlp2020-NoAnswerisBetter/","link":"","permalink":"http://renxingkai.github.io/2021/04/20/emnlp2020-NoAnswerisBetter/","excerpt":"","text":"论文链接AbstractNatural Questions（NQ）数据集给机器阅读理解带来了新的挑战：答案不仅具有不同的粒度（长、短），而且具有更丰富的类型（包括无答案、是/否、单span和多span）。本文针对这一挑战，系统地处理了各种类型的答案。特别是，我们提出了一种新的方法称为Reflection Net，它利用两步训练过程来识别无答案和错误答案的情况。通过大量实验验证了该方法的有效性。在撰写论文时（5月）。2020年12月20日），我们的方法在长答案和短答案排行榜*上均获得前1名，F1得分分别为77.2和64.1。 1 IntroductionNatural Questions(NQ) 数据集的答案提供了两级粒度：长答案和短答案；因此需要模型去在文档和段落级别寻找答案，除了长短答案，还有无答案样本，多span短答案样本，YES/NO答案类型。一些研究人员提出了pipeline方式去抽取短答案，先对长答案进行抽取，然后再抽取短答案。虽然这种方法是合理的，但由于长答案和短答案是分开建模的，因此可能会失去它们之间固有的相关性。还有其他使用联合训练长短答案的方法，以前的方法已经被证明能有效地提高NQ任务的性能，但是很少有工作关注这个QA集合中丰富答案类型的挑战。我们注意到，51%的问题在NQ集中没有答案，因此，模型准确预测何时输出答案是至关重要的。对于其他答案类型，如多span答案或yes/no答案，尽管它们在NQ集合中的百分比很小，但不应忽略它们。相反，在实践中，更倾向于一种能够很好地处理各种答案类型的系统设计。 本文中，我们着重处理无答案类型，我们首先训练所有答案类型的MRC模型，然后，利用训练好的MRC模型对所有训练数据进行推理，训练出第二个模型，称为Reflection model，以预测的答案、上下文和MRC头部特征作为输入，预测出更准确的置信度，从而区分正确答案和错误答案。使用二阶段训练有三个原因： 首先，MRC置信度计算的常用方法是基于logits的启发式方法，这种方法不规范，不同问题之间的可比性不强 其次，在训练长文档MRC模型时，由于负实例比正实例多，所以对负实例进行了大量的下采样。但在预测时，MRC模型需要对所有实例进行推理。这种训练数据分布差异和预测结果表明，MRC模型可能会被一些负面实例所迷惑，并用高置信度的分数预测错误答案。 第三，MRC模型学习了问题的表示、类型和答案之间的关系，而答案不知道预测答案的正确性。 我们的第二阶段模型解决了这三个问题，类似于成为其名称来源的反射过程。通过大量实验验证了该方法的有效性。在撰写论文时（5月）。2020年12月20日），我们的方法在长答案和短答案排行榜*上均获得前1名，F1得分分别为77.2和64.1。 2 ApproachReflection模型结构如图1所示，由MRC模型和Reflection模型构成，分别用于答案预测和答案置信度预测。 2.1 MRC Model使用预训练模型作为MRC模型，滑动窗口用于处理长文本文章，然后将问题与文章片断组成一起去构造一个样本，正样本片段中包含答案，负样本片段中不含答案，由于长文本中负样本较多，我们对负样本使用下采样。 MRC模型的输出为span和答案类型，包含$l=(t,s,e,ms)$，t是答案类型，s和e是答案开始结束位置，答案类似是多span时，我们使用BIO去标注多答案(ms)，使用Transformer作为Encoder，最终得到$h(x)$ 答案类型分类使用[CLS]进行分类，单答案span使用最小化开始和结束的位置进行，多答案类型使用序列标注思想，直接将隐状态通过线性层进行分类BIO，并没使用CRF解码。最终MRC模型的loss: 除了预测答案，MRC模型需要输出置信分数， xs,xe,x1是被预测的开始、结束和[CLS]字符 2.2 Reflection ModeReflection Mode的目标是一个更精确的置信度得分，区分正确答案和两种错误答案（见第3.4节）。第一种方法是预测一个has-ans问题的错误答案，第二种方法是预测一个no-ans问题的任何答案 Training Data Generation为了生成Reflection Model的训练数据，我们使用训练后的MRC 模型去推理全量数据 对于属于每一个问题的所有实例(feature)，我们只根据其置信度得分选择预测答案前1名的实例。 所选实例、MRC预测答案、其对应的如下头部特征和正确性标签（如果预测答案与真实答案相同，则标签为1；否则为0）一起成为反射模型的训练案例。 Model Training 如图1(a)所示，我们初始化反射模型使用MRC训练好的参数，学习率比MRC模型的学习率小几倍。为了收到重要的MRC模型的状态信息，我们从MRC模型的top层抽取顶层特征当它预测答案时，如表2所示。 顶层特征与[cls]标记的隐藏表示连接，然后是用于最终置信度预测的隐藏层。 反射模型将所选实例x和预测答案作为输入。具体地说，我们创建了一个以答案类型和答案位置标记为元素的字典Ans。我们将应答类型标记添加到[cls]字符中，将位置标记添加到相应的位置字符中，并将空标记添加到其他字符中。 反射模型的隐藏层表示为： 然后和顶层特征拼接[CLS]字符表示$h^r(x_1)$，如下式： 最后获得的置信度分数和二分类损失如下： 式中，如果MRC模型的预测答案（基于x）正确，则y=1，否则为0。对于推理，MRC模型需要为每个问题预测一个文档的所有滑动窗口实例，而反射模型只需要推理一个包含MRC模型预测的最终答案的实例。因此反射模型的计算量很小 3 ExperimentsNQ 训练集307,373；验证集7,830;测试集7842 for lb. 3.1 Implementation先在SQuAD 2.0上ft一遍，再ft NQ 3.2 Baselines DocumentQA DecAtt + DocReader BERTjoint 4 Ablation Study 5 Related WorkMRCAnswer Verifier6 Conclusion本文提出了一种系统的方法来处理MRC中的丰富答案类型。特别是，我们开发了一个Reflection Model 来处理无答案/错误答案的情况。其关键思想是根据预测答案的内容、上下文和状态，训练第二阶段模型，预测预测答案的置信度。实验表明，该方法在NQ集上达到了最新的结果。由F1和R@P=90在长、短的回答上，我们的方法都超过了之前的SOTA。消融研究也证实了我们的方法的有效性。","categories":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"}],"tags":[{"name":"MRC NQ","slug":"MRC-NQ","permalink":"http://renxingkai.github.io/tags/MRC-NQ/"}],"author":"CinKate"},{"title":"Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge 笔记","slug":"paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge","date":"2021-04-19T17:12:54.000Z","updated":"2021-04-19T09:15:17.033Z","comments":true,"path":"2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/","link":"","permalink":"http://renxingkai.github.io/2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/","excerpt":"","text":"论文链接Abstract答案选择和知识库问答（KBQA）是问答（QA）系统的两个重要任务。现有方法分别分开解决这两个任务，这需要大量的重复工作，而忽略了任务之间的丰富的相关信息。在本文中，我们基于以下动机，通过多任务学习（MTL）同时解决答案选择和KBQA任务。首先，答案选择和KBQA都可以视为排序问题，一个在文本级别，另一个在知识级别。其次，这两个任务可以互惠互利：答案选择可以结合知识库（KB）中的外部知识，而KBQA可以通过从答案选择中学习上下文信息来进行改进。为了实现共同学习这两个任务的目标，我们提出了一种新颖的多任务学习方案，该方案利用从各个角度学习到的多视图注意力来使这些任务能够相互交互以及学习更全面的句子表示形式。在多个真实的数据集上进行的实验证明了该方法的有效性，并提高了答案选择和KBQA的性能。同样，从不同的表示角度来看，多视图注意力方案在组合注意力信息方面被证明是有效的。 Introduction多任务学习在NLP领域是广泛的，然而QA中的MTL研究较少。本文中，我们探索了利用MTL去同时处理答案选择和KBQA，这两种任务都可以视为排序任务，一个在答案文本级别，另一个是知识级别。答案选择是在多个答案片段中选出一个最合适的答案，KBQA专注于在KB中抽取出相关的知识，去回答问题。大多多任务学习模块，划分共享层和特殊任务层，共享层在各个任务中共享信息，特殊任务层对不同任务是不同的。大多模型忽视了共享层和特殊任务层之间的交互，并且忽视了不同任务之间的交互。因此，本文提出了MTL从不同方面去学习多视角的attention，能够使不同的任务互相交互。具体而言，我们从任务特定的层中收集注意力信息，以在共享层中学习更全面的句子表示形式。 此外，多视图注意力机制通过结合单词级和知识级信息来增强句子的表示学习。 也就是说，通过使用多视图注意力机制，可以在不同任务之间共享和传输单词级和知识级的注意力信息。 根据实验，与单独答案选择和KBQA任务相比，联合学习可以显着提高每个任务的性能。 实验结果还表明了多视图注意力方案的有效性，并且每个视图的注意力都做出了贡献。本文主要贡献如下： 我们探索用于选择答案和知识库问题解答的多任务学习方法。 知识级别的KBQA任务可以改善答案选择任务，而词汇级别的答案选择任务可以增强KBQA任务。 我们提出了一种新颖的多任务学习方案，该方案利用多视图注意力机制来桥接不同的任务，该任务将特定于任务的层的重要信息集成到共享层中，并使模型能够交互式地学习单词级别和知识 级表示。 实验结果表明，答案选择和KBQA的多任务学习优于最新的单任务学习方法。 此外，基于多视图注意力的MTL方案进一步提高了性能。 Multi-Task Learning for Question Answering 图一为多任务QA网络的架构(包含AS和KBQA)。 Task-specific Encoder Layer 首先将预处理后的句子编码成向量表示。不同的QA任务在数据分布和底层表示上应该是不同的。因此，每个任务都配备了一个用于问答的特定任务编码器，每个特定任务编码器都包含一个单词编码器和一个知识编码器来学习完整的句子表示，如图2所示 Word Encoder. 输入为词向量，使用BiLSTM去捕获全文信息，得到Hw。 Knowledge Encoder. 由于知识序列是由一系列标记化的实体或关系名称组成的，因此后一种学习过程需要基于高级知识的表示。针对这一问题，我们将CNN应用到知识序列上，其中大小为n的filters在知识嵌入矩阵上滑动以捕获局部n-gram特征，每次移动都会计算一个隐藏层向量。由于实体长度不确定，因此，会使用不同大小的卷积核来计算，然后使用Dense层来获取知识表示Hk 将Hk和Hw进行拼接，$H_q=[H_{W_q}:H_{K_q}]$，$H_a=[H_{W_a}:H_{K_a}]$，分别为问题和答案的隐藏表示。 Shared Representation Learning Layer与任务特定编码层的输入相比，整句表示具有更丰富的语义，与其他任务的分布更相似。因此，我们整合来自所有任务的编码向量，并通过一个高级共享的Siamese-Bi-LSTM来生成最终的QA表示$S_q$和$S_a$，然后进行平均池化，并且使用了一些词级别特征$x_{ol}$，最终表示为$x=[s_q,s_a,x_{ol}]$ Task-specific Softmax Layer 使用softmax最终分类。 Multi-Task Learning 最小化以上目标函数，$\\lambda$为不同任务的权重 Multi-Task Model with Multi-View Attention为了增强潜在表征空间中不同QA任务之间的交互作用，我们提出了一种多视角注意机制，从任务特定层和共享层提取重要信息。 Multi-View Attention Scheme如图3所示，与其他注意共享方案不同的是，我们不仅从任务特定层提取注意力，而且还将来自共享层的信息进行组合。此外，我们从词汇和知识两个角度获得注意信息，因为词汇水平和知识水平的信息可能共同促进表示学习。具体来说，我们计算了五种注意观，包括词、知识、语义、知识语义和共同注意力。 Semantic View &amp; Knowledge Semantic View使用mean/max池化去获取句子的语义表示 ExperimentDatasets &amp; PreprocessingAS: YahooQA,TREC QAKBQA: SimpleQuestions,WebQSP Multi-Task Learning Results Related WorkAnswer SelectionMulti-Task LearningConclusion本文研究了同时解决答案选择和知识库问答问题的多任务学习方法。我们提出了一种新的多任务学习方案，该方案利用从不同角度学习的多视角注意，使这些任务能够相互作用，并学习更全面的句子表示，包括词视角、知识视角、语义视角、知识语义视角和共注意力视角。在几个广泛使用的QA基准数据集上进行的实验表明，答案选择和知识库问答的联合学习方法明显优于单任务学习方法。同时，多视角注意策略能有效地从不同的表征视角收集注意信息，提高整体表征学习效果。","categories":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"}],"tags":[{"name":"MTL MRC","slug":"MTL-MRC","permalink":"http://renxingkai.github.io/tags/MTL-MRC/"}],"author":"CinKate"},{"title":"Text Style Transfer via Learning Style Instance Supported Latent Space 阅读笔记","slug":"ijcai2020-StyIns","date":"2021-02-06T21:41:25.000Z","updated":"2021-02-06T13:47:23.342Z","comments":true,"path":"2021/02/07/ijcai2020-StyIns/","link":"","permalink":"http://renxingkai.github.io/2021/02/07/ijcai2020-StyIns/","excerpt":"","text":"本文为biendata视频笔记，仅用来学习，侵删。 开源地址 Background风格转换：在不改变原来句子语义情况下，将原句子x和目标风格sj输入，生成含有目标风格sj的句子 Applications 文本润色，将非正式句子转为正式句子 评论、对话的自动生成 风格特征写作 Previous Paradigms风格转换，需要表示句子内容和target style。 解耦方式(Distentage)。将encoder之后的隐状态，拆出风格表示特征c和文本表示特征h，将h和target style同时输入到decoder中，生成y。这种方式的缺点：内容保留不太好；优点：风格表示容易控制、灵活的。 基于Attention的Seq2Seq。使用Seq2Seq结构，使用attention将x和y去对齐，并且在生成y时候，使用目标风格style embedding去监督生成对应风格的句子。cycle loss:先用目标风格sj监督x生成y，然后再用先前风格si将y再生成回去x’，x’和x计算相似loss。可以保证迁移之后的句子仍然保留原来的语义。同时使用判别器去判别迁移后的y是否真的有目标风格，促进风格迁移生成！优点：保留词级别的信息，几乎没有语义丢失。缺点：风格的信号比较差，风格迁移效果不太好。 Multi-Generator。通过不同generator来实现不同风格上的style生成。相当于每个decoder就代表了各自的风格。为了保留内容，在使用另一个decoder重新构造生成x’。优点：效果很好！缺点：资源需求比较多，需要多个seq2seq。 Locate and Replace。找出句子中和风格相关的词，进行替换，保留和风格无关的词。优点：较为精确，能够实现较好的风格转换和内容保留。缺点：需要一个风格词表；对于超出词义的风格迁移，效果不好，上升到更深的语义级别效果不好！ Motivation结合Attention-based Seq2Seq和潜在风格空间Latent Style Space，去构建更好的风格迁移和内容保留。 Methodology之前的VAE方法，假设每个句子是独立的，每个句子都去单独输出隐状态风格z，不能充分考虑全局信息 K为K个句子都包含同样的风格j。一个原句子encoder编码原句子x为H，一个风格encoder(将K个句子都包含同样的风格j的句子集)进行风格编码为z，一个decoder对H和z进行解码。 style encoder使用生成式流模型，假设初始为高斯分布z0，经过多次复杂变换之后，生成更为复杂的分布zt，能更充分表示风格。 TrainingUnsupervised Training Cycle Consistency Loss:先将x转为y再转回去x’，计算x’和x的交叉熵。 Reconstruction Loss:给定原句x和原句风格s，希望能重新生成原句x Adversarial Style Loss:提供style相关的判别对抗lossSemi-Supervised Training ExperimentDataset Metrics Results 人工评测 Cases Conclusion","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"CoCon A Self Supervised Approach for Controlled Text Generation 阅读笔记","slug":"CoCon","date":"2021-02-01T16:30:22.000Z","updated":"2021-02-01T08:31:57.396Z","comments":true,"path":"2021/02/02/CoCon/","link":"","permalink":"http://renxingkai.github.io/2021/02/02/CoCon/","excerpt":"","text":"论文链接Abstract基于Transformer的LM展现了显著的自然语言生成能力。由于其巨大的潜力，控制这些LMs的生成文本正受到关注。虽然已经有了不少研究去控制被生成文本的高级属性(情感、主题等)，但仍然缺少对内容上词级别、短语级别的准确控制。本文中，我们提出了Content-Conditioner(CoCon)在细粒度级别，去使用目标内容控制一个语言模型的输出。在我们的自我监督的方法中，CoCon学习帮助LM完成一个部分观察到的文本序列，通过调节LM中保留的内容输入。通过实验，我们证明了CoCon可以自然地将目标内容合并到生成的文本中，并以Zero-shot的方式控制高级文本属性。 1 Introduction基于Transformer的LM在海量语料上进行训练，然后去预测下一个token通过对数似然损失(log-likehood)。控制LM的输出逐渐吸引了人们的注意。像从零开始训练一个修改过的LM来合并目标文本属性[这样的方法成本很高，而对特定属性的预训练LM进行微调则限制了文本控制的范围。PPLM没有修改LM的架构，通过属性来控制被生成的文本。尽管在控制诸如主题和情感之类的高级文本属性方面是有效的，但是相同的目标属性可能会生成在单词和短语级别具有显著不同的内容文本样本，从而为对LM生成的文本的内容进行更细粒度的控制留下一个gap。为了举例说明情绪控制的例子，一篇以“彼得”这样的名字的提示文本开头的文章后面可以是描述彼得性格的文本，关于他在某个特定时间正在做什么，甚至是关于“彼得广场”这样的非人类实体，以积极的方式。对LMs输出内容的更精细控制可以为生成符合事实或没有不适当内容的文本铺平道路。 我们提出的Content-Conditioner （CoCon），以缩小这一差距，通过指导预训练的LMs的文本输出同时包含目标内容。基本上，CoCon包括三个部分：1）编码器，2）解码器和3）CoCon块。CoCon采用预训练LM作为编码器和解码器，通过CoCon块将目标内容合并到编码文本表示中，然后将内容条件表示传递给解码器进行生成。为了训练CoCon块，我们提出了一种自监督学习方法，其中训练数据由预训练LM本身生成的文本样本组成。通过将每个文本序列分成两段（[xa；xb]），CoCon学会了通过将xb本身作为内容输入来帮助LM重建丢失的后段（xb）。 我们提出了CoCon的损失函数和content masking，在产生高质量文本的同时，对来自不同来源的内容进行限制。由于CoCon块的大小是LM的一小部分，并且没有对LM的权重进行微调，因此训练成本明显低于从头开始训练LM。对比于strong baselines，CoCon也可以进行高等级的文本属性控制(例如主题和情感)，通过zero-shot的方式。此外，CoCon在同化多个目标内容方面具有通用性，其内容调节的强度可以在推理过程中通过内容偏差项进行灵活的调整。本文中，我们使用GPT-2作为LM。我们主要贡献如下： 我们提出CoCon的基于条件的内容语言生成方法。 我们引入了一种自我监督学习方法，在这种方法中，CoCon在给定未来token的信息时学习完成文本序列。 通过消融试验和与PPLM和CTRL等强基线的比较，我们研究了CoCon如何有效地影响文本生成的内容，以及如何有竞争性地控制主题和情感等高级文本属性。 我们展示了CoCon在整合多种内容方面的多功能性，文本生成中内容调节的灵活性，以及它与其他控制方法（如PPLM）的互补性。 2 Related Work之前的属性控制的文本生成方法大多基于RL和GAN进行训练。与CoCon不同，这些方法中对预定属性的要求限制了生成文本的可能类型。 CTRL是最近的一种方法，它通过使用控制代码生成受控的流畅文本，这些控制代码是在生成过程中预先添加到文本中的元数据。虽然它使用GPT-2结构生成高质量的文本，但是它的控制代码在训练过程中也是预先确定的。 最接近我们的工作是PPLM，它试图通过相对较小的“可插拔”属性模型来控制已经预训练的LM上的文本。尽管PPLM的灵活设计也支持受控生成，而无需像CoCon中那样对LM进行再培训或微调，但我们的方法旨在将生成控制在更局部的内容级别，而不是高级的文本属性。另一个核心区别在于训练，CoCon的自监督学习免除了对标记数据的需要，例如用于训练PPLM的属性鉴别器模型的数据。PPLM中的加权解码试图通过在解码步骤中提高目标词的概率来控制输出文本，但已被证明产生不连贯文本。 文本风格转换是通过将文本从一种风格转换到另一种风格来控制文本属性的相关领域。一些这样的研究使用自动编码器来分离文本的风格和非风格的潜在表征。这种分离使得文本在保留大部分内容的同时，能够在潜在空间改变样式。另一项工作是识别与文本语料库中特定风格相关的n个属性标记，并通过替换它们来编辑文本的风格。从本质上说，风格转换改变现有的文本，而不是生成文本，需要预定义的属性。 3 Content Conditioner (CoCon)Motivation在基于语言模型的文本生成任务中，通常使用自回归的方式进行生成： 之前的控制文本生成任务中，p(x)可以通过目标属性或者控制代码进行控制文本的情感或者主题： 虽然这些方法生成是流畅的，并且可以很好地与目标属性对齐，但是输出文本${x_t,...,x_l}$是在全局属性（例如情绪/主题）级别上控制的，而不是在更局部的内容（例如单词/短语）级别上控制的。由于存在大量可能的${x_t,...,x_l}$候选项，这些候选项将与先验文本和目标属性很好地对齐，因此在随机字符采样过程中，生成的文本样本包含非常不同的内容。这激发了一种对输入目标内容c进行条件处理的方法，以便对文本生成进行更细粒度的控制： Model Architecture 我们提出的CoCon（图1）通过在我们的实验中加入一个基于预训练Transformer的语言模型（LM）GPT-2，在保持流畅性的同时控制生成文本的内容。 LM的生成可以分为两个独立的部分：编码器（enc）和解码器（dec）。编码器充当一个特征提取器，接收输入序列的嵌入并在断点处输出其中间表示，即$h_{:t−1}=enc(x_{:t−1})$。随后，解码器接收该表示并输出下一字符的logit，即$o_t＝dec(h_{:t−1})$，从而产生 从等式4中，我们可以看到表示（h）是控制下一个字符logits（o）的表示。实际上，我们通过CoCon块用目标内容输入（c）对h进行条件化，从而转换h 我们参数化CoCon模块将其作为一个单层的Transformer模块，其中包含一个attention层和 FFN层。与典型的LM attention层类似，Query（Q）、Key（K）、Value（V）矩阵通过表示$h_{:t−1}$上的线性变换来计算，其中$Q,K,V\\in R^{(t-1)xd}$，d是表示的维数。为了关注内容表示$h_{l_c}^{(c)}$，还计算内容键和值$K^{(c)},V^{(c)}\\in R^{l_cxd}$，并在计算CoCon注意输出之前concat到原始注意矩阵： 最后的CoCon输出通过位置前馈层进行计算。通过连接到t−1之前的表示并将其传递给解码器（dec），下一个logit以及随后的字符$\\hat x_t$现在由c进行调节： Multiple Content Inputs对于多个目标内容的输入，处理方法较为简单，将多个目标内容的K和V进行拼接再计算即可。 Strength of Content Conditioning在CoCon的注意机制中，我们可以通过偏置W（等式6）中与内容输入（c）相对应的注意权重来改变内容对输出文本的调节程度。更具体地说，可以通过注意对内容值$(V^{(c)})$的softmax加权来改变c对输出文本的影响。在生成期间，可以选择向内容注意权重$W_{:,:l_c}\\in R^{(t-1)xl_c}$添加正偏项（τ内容），以增加V（c）的影响，促进内容调节，而负偏项可以相反地降低内容调节效果。我们讨论了变化τ含量的例子在4.4节中。 3.1 Self-Supervised Learning我们可将一个序列$x=\\{x_1,...,x_{t-1},x_t,...,x_l\\}$分为两部分：$x^a=\\{x_1,...,x_{t-1}\\}$和$x^b=\\{x_t,...,x_{l}\\}$，即$x=[x^a;x^b]$。在现实世界中，$x^b$的替代品可能有很多，可以流利地从$x^a$开始。再加上文本采样的随机性，这意味着，如果没有关于$x^b$的信息，仅用LM从$x^a$重建完整x的概率可能很低。 Self Reconstruction Loss 自重构损失$L_{self}$就是让控制文本$c=x^b$，之后要生成的就是$x^b$自己，这一步是让模型能够学习结合控制文本的内容。 Null Content Loss 无内容损失$L_{null}$，即令$c=null$，这时候CoCon就退化为一个简单的语言模型，为了能够生成流畅的文本。 Cycle Reconstruction Loss 循环重构损失$L_{cycle}$通过两个不同文本互为控制文本来生成质量更高的文本。假设现在有两个不同的文本：$x=[p;q]$和$x&#39;=[p&#39;;q&#39;]$，进行如下操作： 首先将p’作为引导文本，将q作为控制文本，生成新文本$q^1=f(p&#39;,q)$。显然，q1的目的是在和p’保持语言流畅，且尽可能包含q的内容。 再将p作为引导文本，将q1作为控制文本，生成新文本$q^2=f(p,q^1)$，这是让q2和p保持流畅，同时尽可能包含q1的内容。 既然q2包含q1的内容，并且q1包含q的内容，要么就是q2包含q的内容，且要和p保持流程，此时就是q本身。 因此，循环重构损失就是以q为真值去优化q2，反过来也可以以q’为真值去优化q’2 Adversarial Loss 最后用到常用的对抗损失$L_{adv}$，让生成的文本更接近真实的文本。 Full Training 组合以上四个loss即为最终需要优化的目标： 4 Experiments本文在三个可控文本生成任务上进行实验，情感可控生成，主题可控生成，文本可控生成。 文本可控生成 文本引导的文本生成评估指标有BLEU、NIST、METEOR、PPL和Dist-1/2/3。由于没有现成的该任务的数据集，故本文随机从预训练的GPT-2中采样3000个样本，均匀分为三组，每组的控制文本c的长度为5,10,20个BPE长度，所生成的句子x长度都是100。把它们作为测试集。训练集则是随机从GPT-2所产生的句子中获取。此外，CoCon还在大小为250K的Webtext.上训练，以探究不同训练数据来源的影响。结果如下表所示。可以看到，CoCon在结合控制文本c.上比GPT2好太多。在这个实验。上，不加一些损失会有更低的PPL以及更高的BLEU、NIST、METEOR等值，比如去掉$L_{null}$或者$L_{adv}$模型的BLEU值会更高，这是因为这两个损失相比之下更关注生成文本的流畅度而不是与控制文本c相结合。 主题可控文本生成 在主题可控任务上，比较的基线模型有PPLM和CTRL，都是当前很强的模型。为验证主题相关性，训练一个分类器用于评估主题程度。下表是实验结果： 情感可控文本生成 情感可控生成任务采用二分类情感，下表是实验结果，和主题任务类似，CoCon在情感相关度上显著优于其他模型，这说明CoCon能够更加紧密地控制文本c。 实例分析 下图是不同的$t_{content}$生成的不同文本，显然，值越大就越和控制文本相关，但是越容易生成不相关的内容。 多个控制文本输入的结果实例： 5 Conclusion我们提出了CoCon为了细粒度的控制基于神经网络的生成文本。CoCon可以被有效训练通过一个自监督的方式，并且它与已经生成高质量文本的预训练语言模型兼容。通过我们的实验，CoCon被证明能够顺利地将目标内容合并到生成的文本中，并控制高级文本属性。这种新的控制方式为更真实、更安全的文本生成带来了希望，促进了神经文本生成在实际应用中的应用。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"PLUG AND PLAY LANGUAGE MODELS A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION","slug":"iclr-2020-PPLM","date":"2021-01-28T21:44:04.000Z","updated":"2021-01-31T14:27:13.189Z","comments":true,"path":"2021/01/29/iclr-2020-PPLM/","link":"","permalink":"http://renxingkai.github.io/2021/01/29/iclr-2020-PPLM/","excerpt":"","text":"论文链接Abstract在大型文本语料库上训练的大型基于transformer的语言模型（LMs）已经显示出无与伦比的生成能力。然而，如果不修改模型结构或对特定属性数据进行微调，并且需要大量的再训练成本，那么控制生成语言的属性（例如，切换主题或情感）是困难的。我们提出了一个简单的可替代方案：可控制语言生成的即插即用语言模型（PPLM）。它将预训练的语言模型与一个或多个简单的属性分类器相结合，这些属性分类器无需对语言模型进行任何进一步的训练就可以指导文本生成。在我们提出的规范场景中，属性模型是简单的分类器，由用户指定的bag of words或单个学习层组成，其参数比LM少100000倍。采样需要前向和后向传递，其中来自属性模型的梯度推动LM的隐藏激活，从而指导生成。模型样本展示了对一系列主题和情感风格的控制，广泛的自动化和人工注释评估显示属性对齐和流畅性。PPLMs的灵活性在于，可以使用不同属性模型的任何组合来指导文本生成，这将允许本文给出的示例之外的多种创造性应用。 1 INTRODUCTION基于Transformer的PLMs在大量无标注的语料中训练，然后使用对数似然损失函数进行训练。然而，一旦这样的模型被训练，在不修改模型结构以允许额外的输入属性或对属性特定的数据进行微调的情况下，控制生成文本的属性就变得很困难。 可控的文本生成需要对$p(x|a)$进行建模，a是想要的可控属性，x是需要被生成的文本。然而，生成模型只需要去学习$p(x)$。在计算机视觉领域，Nguyen et al.（2017）的即插即用生成网络（PPGN）开发了一种生成具有不同属性的图像的机制，方法是将判别器（属性模型）$p(a|x)$与基本生成模型$p(x)$插在一起，并从结果的$p(x|a)\\propto p(a|x)p(x)$中采样，有效地创建条件生成模型从任何提供的属性模型动态建模。以类似的方式，我们提出了用于条件语言生成的即插即用语言模型（PPLM），它将一个或多个简单的属性模型p（a|x）组合在一起，可以是一个Bag-of-Words（BoW）或单层分类器的形式，也可以是一个预训练的无条件语言模型p（x）。我们从得到的组合模型中取样，在潜在的表示空间中遵循梯度，这种方式受到了（MALA）的启发。 优化是事后在激活空间中执行的，因此不需要重新训练或微调。文本控制是细粒度的，强度参数决定属性影响多大的强度；强度为0则完全恢复原始模型p（x）。这种设计允许极大的灵活性：用户可以将最先进的生成模型与任意数量的属性控制器结合起来，生成模型可能很大，而且很难训练。属性模型可能更易于训练或未经训练，并且在推理过程中可以灵活地组合多个控制器。在本文中，我们演示了使用GPT-2 345M参数模型作为通用语言模型p（x）的PPLM方法，但是该方法适用于任何基于transformer的文本生成器的任何表示空间，并且允许与任何属性模型p（a|x）组合。 我们演示了使用多个属性控制器的受控生成，这些控制器在生成过程中组装和组合，每个控制器具有不同的强度，充当一组“控制旋钮”，将生成的文本调整为所需的属性（参见表1中的示例），我们关键的贡献如下： 我们提出了用于控制语言生成的即插即用语言模型，讨论了它与现有工作的关系，以及如何从PPLM中进行采样。 我们证明了对一系列属性的文本生成控制，包括7个主题，每个主题使用一个BoW定义，以及1个简单的情感判别器。我们使用自动评估（分别训练的困惑和情绪模型）和人类评估（属性相关性和流畅性）来量化有效性。所有的评估都指向PPLMs生成属性控制的流畅文本的能力 我们对比了CTRL和GPT-2，我们的方法，没有任何LM训练，在属性相关性和流利性方面都是标杆，并且经常优于基线 我们表明，PPLM方法可以用来文本消除毒性，遵循负梯度模型训练，生成有毒内容是可能的，同时也可以用其检测毒性。我们还展示了PPLM如何用于结构受限的故事写作 2 RELATED WORKControlled generation当前的控制文本生成方法大多涉及到使用RL方法ft预训练模型，训练GAN或者训练条件生成模型。与我们方法不同之处，这些方法并不是即插即用，因为整个模型需要针对每个特定属性分别进行微调。我们的方法不需要对任何条件生成模型进行再训练，语言模型和条件模型都可以灵活组合。 Noisy Channel Modeling不少人使用香农噪声信道理论为了提升Seq2Seq建模，他们的方法翻译源语言句子y到一个目标语言句子x通过首先对前向模型的采样$p_{forward}(x|y)$，然后基于概率$p_{backward}(x|y) \\propto p(x)p(y|x)$重排样本。PPLM对样本进行打分使用相同的基本公式，但由于我们没有前向模型$p_{forward}(x|a)$，我们依赖于潜在空间的更新。作为一个基线，我们使用$p(x)$作为前向模型，然后重排序。我们将看到，在某些场景中工作得相当好，而在其他场景中工作得很差 Weighted decoding有的研究者使用判别器或者BoW去控制语言生成，为了考虑用于解码的评分函数，对解码过程进行了修改。See等人（2019）注意到，使用加权解码（WD）进行控制是困难的，通常会导致牺牲流畅性和连贯性。此外，Ghazvininejad et al.（2017）强烈依赖于从特定主题的一组关键字中取样，并且不允许以不必包括一组关键字的方式偏向于主题的生成。类似的，Baheti等人提出了一种解码策略用于在对话系统中生成感兴趣的回复，使用BoW和词向量，随机采样方法可以被使用去受限于模型的生成到确定的关键词和主题。我们使用带权重的解码作为基线。 Text Style Transfer在语言建模之外，语篇风格迁移是一个相关的研究课题。有的研究者训练VAE for 风格迁移依赖于学习区分风格和内容的潜在表示。Li et al.证明了一种基于条件生成模型的简单方法的有效性，该方法将与属性相关的n-gram替换为与所需属性对应的n-gram。我们的方法与上面方法一个关键的不同之处在于，我们使用了一个线下的判别器，基于此鉴别器执行优化。最近，Lample等人（2019年）采用了一种从无监督语言翻译到风格转换的方法，在这种方法中，去噪自动编码器的训练目标包括重建损失和回译损失的加权组合。虽然上述方法在风格转换任务上取得了令人印象深刻的成功，但主要的焦点是不受控制的语言生成，而且这些方法也不是即插即用的。 3 PLUG AND PLAY LANGUAGE MODELS 3.1 LANGUAGE MODELING WITH TRANSFORMERS给定一个句子$X={x_0,...,x_n}$，语言模型被训练去计算序列$p(X)$的无条件概率。这个概率可以应用递归链式法则进行表示为乘积形式： 本文中，我们使用Transformer进行语言分布的建模。令$H_t=[(K_t^{(1)},V_t^{(1)}),...,(K_t^{(l)},V_t^{(l)})]$为由key-value组成的历史矩阵，从第i层0到t的时间序列。 3.2 STEERING GENERATION: ASCENDING $log p(a|x)$为了去控制语言模型的输出，在每个生成步t，我们转移历史$H_t$在两个梯度方向上，一种倾向于在条件属性模型$p(a|x)$下，更高的对数似然属性a；另一种是倾向于未修改语言模型p（x）的更高对数似然。将这些因素与可变乘数结合起来，为我们提供了一个可控的“旋钮”，以指定的强度在给定的情感方向上引导生成。参数更新仅限于Ht，而不限于其他激活模型，因为未来的预测仅通过Ht依赖于过去（注意，Ht由在时间t之前生成的所有Tramsformer的key-value组成）。在Ht空间经过每一步会导致模型激活的逐渐变化——这可能被认为是对过去的逐渐重新解释——从而引导下一代朝着理想的方向发展。 将∆Ht作为对Ht的更新，这样，使用（Ht+∆Ht）生成的文本会改变生成文本的分布，从而更可能拥有所需的属性。∆Ht初始化为零，并使用属性模型的梯度进行更新，该属性模型测量生成的文本具有所需属性的程度（例如正性）。我们重写属性模型$p(a|x)$为$p(a|Ht+∆Ht)$然后基于以下公式去更新∆Ht： 3.3 ENSURING FLUENCY: ASCENDING $log p(x)$上一节中描述的方法能够生成针对特定判别器的文本，但如果不加以检查，当文本进入低概率区域时，将很快导致不切实际的敌对或愚蠢的示例（Szegedy et al.，2013；Nguyen et al.，2015）。为了解决这个问题，我们从两个方面使用了无条件语言模型，以确保流利度保持在或接近无条件语言模型的水平（这里是GPT-2）。 Kullback–Leibler (KL) Divergence除了上述步骤之外，我们还更新∆Ht以最小化修改和未修改语言模型的输出分布之间的KL差异。实际中，这是通过在使用渐变之前将数量相加来实现的，尽管可以将其可视化为两个单独的步骤，如图2所示。我们用一个标量λKL来缩放KL系数，在实践中，将这个超参数设置为0.01通常可以很好地在各种任务工作。 Post-norm Geometric Mean Fusion除了KL散度，我们还是用了后范数融合方法，这并不直接影响∆Ht；相反，它只是将生成的文本与无条件的p（x）LM分布联系起来。 3.4 SAMPLING AND RANKINGPPLM中的属性模型$p(a|x)$提供两种功能：首先，基于对数似然提供一个能排序想要样本的分数；其次，在潜在空间中执行更新的梯度上升方向。前者可用于生成r个样本，并对其进行排序，以选择最佳样本。除了使用更新的采样外，这还可以作为属性控制的附加方法。此外，为了避免重复性、低质量文本的问题（Holtzman et al.，2018），我们计算Dist-1、Dist-2和Dist-3分数的平均值（对于生成的文章），这是重复性的指标（Li et al.，2015），然后丢弃平均分数低于阈值τ的样本。 4 EXPERIMENTS, RESULTS, AND EVALUATION4.1 EVALUATION METHODS AND ABLATION STUDY我们评估两个属性：PPLM是否生成满足所需属性（主题或情感）的文本，以及当我们加强对属性的控制时，其文本质量是否恶化。注：我们可以随时将控制旋钮调低到零，以禁用属性控制，并达到原始模型的流畅性。如果需要，用户可以在推理时调整旋钮，直到在属性强度和流利性之间达到所选择的折衷。我们使用自动方法和人工注释器进行评估 4.2 BoW ATTRIBUTE MODELSBoW的一些结果 4.3 DISCRIMINATOR ATTRIBUTE MODELS当仅用BoW一些词难以表示的属性词，就可以使用判别器来进行属性控制。我们在输入句子x和相应标签yx的数据集上训练鉴别器，以下是结果。 4.4 LANGUAGE DETOXIFICATION使用大量互联网数据训练的语言模型反映了数据中存在的偏见和歧视。Wallace et al.（2019）最近的一篇论文进行了对抗性攻击，当给定一个经过仔细优化的触发器字符串作为前缀时，GPT-2会产生种族主义输出。他们还发现，当简单地使用“黑人”作为前缀时，2%的GPT-2样本包含明显的种族歧视。其他前缀（如“亚洲人”或“犹太人”）被提及，但没有报告百分比。我们进行实验并报告基线毒性百分比为10%（“亚洲人”）、12%（“犹太人”）和8%（“黑人”）。Wallace等人（2019年）发布的代码库产生了对抗性触发，平均毒性百分比为63.6%。更多细节见第S13节。 通过引入毒性分类器作为属性控制模型，并用负梯度更新潜在空间，PPLMs可以很容易地适应语言解毒。我们对来自毒性评论分类挑战（Jigsaw）的毒性数据训练了一个单层分类器，并表明与其他PPLM-Discrim方法具有相似的超参数设置，它在自然提示和对抗性触发条件下都能很好地工作。自然诱发的毒性百分比分别为6%、4%和10%，而对抗性诱发的毒性百分比则急剧下降到平均4.6%，具有统计学意义。注释程序和百分比和p值的完整表格的详细信息见表S23和第S13节。请注意，解毒语言的模型也可能被恶意用于生成有毒语言，这是我们在第S6节中简要讨论的主题 4.5 CONTROLLED STORY WRITING我们探索辅助性故事写作的受控生成（Peng et al.，2018；Luo et al.，2019；Yao et al.，2019；Fan et al.，2018）。使用不受控制的LMs辅助艺术创作可能很困难。为了有助于结构，我们使用了预先定义的故事骨架，通常用于即兴创作。我们用PPLM填充这些前缀之间的空白。见表S20和表S21中的示例。 5 CONCLUSION我们提出了PPLM，一个即插即用的方法可控的语言生成，容易组合到一个巨大的预训练LM和一个BoW模型，或者一个轻量级的容易去训练的判别器。PPLM实现了对属性的细粒度控制通过一个简单的基于梯度的采样机制。由于PPLMs在保持流利性的同时能够灵活地控制生成，因此它在支持下一代语言模型方面具有很大的潜力。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"Style Transfer from Non-Parallel Text by Cross-Alignment 阅读笔记","slug":"nips2017-style-transfer","date":"2021-01-26T09:09:18.000Z","updated":"2021-05-20T06:59:30.247Z","comments":true,"path":"2021/01/26/nips2017-style-transfer/","link":"","permalink":"http://renxingkai.github.io/2021/01/26/nips2017-style-transfer/","excerpt":"","text":"论文链接Abstract本文主要研究基于非平行文本的文体转换。这是一个包括机器翻译、破译和情感改写在内的一系列问题的例子。关键的挑战是将内容与风格等其他方面分开。我们假设一个共享的潜在内容分布在不同的文本语料库中，并提出了一种利用潜在表达的精确对齐来进行风格转换的方法。从一个语体转换过来的句子应该和从另一个语体转换过来的例句相匹配。我们证明了这种交叉比对方法在三个任务上的有效性：情感改写、单词替换密码的破译和词序恢复。 1 Introduction机器翻译、文本摘要类似的任务需要大量的平行语料进行训练，但是在一些NLP生成任务中，我们仅有非平行或单语言的数据。类似于文本破译或者风格迁移，在这些任务中，我们必须保留源语句的内容，但要使语句与所需的表示约束（例如，样式、明文/密文）保持一致。 我们的任务较有挑战性：我们只假设访问两个句子的语料库，尽管呈现的风格不同，但内容分布相同。我们的目标是证明这种内容的分布等价性，如果仔细加以利用，就足以让我们学会将一个文体中的句子映射到一个文体无关的内容向量，然后将其解码成一个内容相同但文体不同的句子。 本文中，我们提出了一个精细的句子级表示对齐方法在文本语料之间。我们通过学习一个编码器，它以一个句子和它的原始样式指示符作为输入，并将其映射到与样式无关的内容表示。然后将其传递给与样式相关的解码器进行渲染。实际上，更丰富的潜在内容表示更难在整个语料库中对齐，因此它们提供了更多的信息内容约束。此外，我们从交叉生成（风格转换）的句子中获取额外的信息，从而得到两个分布对齐约束。例如，风格转换成否定句的肯定句，作为一个总体，应该与给定的否定句相匹配。我们在图1中说明了这种交叉对齐 我们使用三个NLP任务去证明我们方法的有效性：情感改写、文本破译和词序恢复。在这三个任务中，模型都是使用非平行语料进行训练。 2 Related WorkStyle transfer in vision 抽取内容和风格特征等，使用GANS network(CoupledGANs、CycleGAN)。虽然我们的方法具有类似的高级体系结构，但自然语言的离散性不允许我们重用这些模型，因此需要开发新的方法。 Non-parallel transfer in natural language在NLP中的生成任务大多需要平行语料，类似于(机器翻译、摘要生成等)。我们的方法很接近于不使用平行语料，但是使用训练中的非直接信号进行辅助。(VAEs，操作隐状态的表示以控制对应的情感生成等)。虽然我们的模型建立在分布式交叉对齐的基础上，以实现风格转换和内容保留，但可以以相同的方式添加约束。 Adversarial training over discrete samples最近离散样本中的对抗训练，大多使用RNN。在我们的工作中，我们采用了Professor-forcing算法（Lamb等人，2016年），该算法最初是为了弥补教师在训练过程中的强迫和测试过程中的自我反馈之间的差距。这样的设计符合我们的风格迁移场景which called cross-alignment。 3 Formulation 这个命题基本上是说不同风格生成的X应该足够“独特”，否则风格之间的转换任务就没有很好的定义。虽然这看起来微不足道，但即使对于简单的数据分布，它也可能不适用。下面的例子说明了在不同的模型假设下，转移（和恢复）是如何变得可行或不可行的。正如我们将看到的，对于某一风格的Y，z的分布越复杂，恢复传递函数的可能性就越大，寻找传递函数就越容易。 3.1 Example 1: Gaussian 3.2 Example 2: Word substitution这里考虑另一个例子，当z是一个双语语言模型，风格y是一个正在使用的词汇表，它将每个“内容词”映射到它的表面形式（词汇形式）。如果我们观察同一语言z的两个实现x1和x2，那么转移和恢复问题就变成了推断x1和x2之间的单词对齐。 这就是一个简单的语言破译或者翻译版本。除此之外，回复问题仍然是很困难的。为了解释这一问题，假设$M_1$和$M_2$是$X_1$和$X_2$的概率估计，寻找单词对齐与去寻找一个排列矩阵P，满足$P^TM_1P≈M_2$是相似的，可以表示为一个优化问题： 同样的公式适用于给定M1和M2作为两个图的邻接矩阵的图同构（GI）问题，表明确定P的存在性和唯一性至少是GI困难的。幸运的是，如果M作为一个图足够复杂，那么搜索问题可能更容易处理。例如，如果作为一个集合的每个顶点的关联边的权重是唯一的，那么可以通过简单地匹配边集合来找到同构。这个假设在很大程度上适用于我们的场景，其中z是一个复杂的语言模型。我们在结果部分以经验证明了这一点。 上述例子表明，作为潜在内容变量的z应该承载最复杂的数据x，而作为潜在风格变量的y应该具有相对简单的效果。我们将在下一节中相应地构建模型。 4 Method由于图像中的数据是连续的，所以可以直接进行迁移学习，由于自然语言的离散性，我们不能直接进行训练；需要我们在潜在空间进行操作。由于x1和x2是给定潜在内容变量z之后条件独立。 这样表示建议我们使用一个自编码模型，特别地，一个风格迁移任务将x2迁移到x1涉及到两步：编码步和解码步。编码步骤负责推断x2的内容$z-p(z|x_2,y_2)$：解码步骤负责：生成对应的迁移事务从$p(x_1|y_1,z)$,本文中，我们使用神经网络近似学习和训练$p(z|x,y)$和$p(x|y,z)$。 正如我们的生成框架所设想的那样，为了使翻转风格成为一个有意义的迁移，$X_1$和$X_2$的内容空间必须是连续的。为了限制x1和x2是从相同的潜在内容分布p（z）生成的，一种选择是应用可变自动编码器（Kingma和Welling，2013）。VAE先使用一个先验的概率密度$p(z)$，例如$z-N(0,I)$，然后使用KL散度去标准化后验$p_E(z|x_1,y_1)$和$p_E(z|x_2,y_2)$ 然而，正如我们在上一节中所讨论的，将z限制为简单且均匀的分布，并将大部分复杂性推给解码器，对于非并行数据的风格迁移可能不是一个好的策略。相比之下，标准的自动编码器只是将重建误差最小化，鼓励z携带尽可能多的x信息。另一方面，它降低了p（x|y,z）中的熵，这有助于在y1和y2之间切换时产生有意义的风格转换。在不显式建模p（z）的情况下，仍然可以强制p（z|y1）和p（z|y2）的分布对齐。为此，我们介绍了自动编码器的两种受约束的变体。 4.1 Aligned auto-encoder省去了对p(z)作显式假设并使后验值与p(z)一致的VAEs，我们将$p_E(z|y_1)$和$p_E(z|y_2)$相互对齐，从而得到如下约束优化问题。 在实践中，拉格朗日松弛的原始问题，而不是优化。我们引入了一个对抗性鉴别器D来校准不同类型z的聚集后验分布（Makhzani et al.，2015）。D旨在区分这两种分布： 我们使用带有GRU单元的单层RNNs来实现编码器E和生成器G。E获取一个初始隐藏状态为y的输入句子x，并输出最后一个隐藏状态z作为其内容表示。G生成一个以潜在状态(y,z)为条件的句子x。为了对齐$z_1=E(x_1,y_1)$和$z_2=E(x_2,y_2)$的分布，鉴别器D是具有单个隐藏层和sigmoid输出层的前馈网络。 4.2 Cross-aligned auto-encoder第二种变体是交叉对齐自动编码器，它直接将一种风格的迁移样本与另一种风格的真实样本对齐。 在生成性假设下，$p(x_2|y_2)=\\int_{x_1}p(x_2|x_1;y_1;y_2)p(x_1|y_1)dx_1$，因此x2（从左侧取样）应表现出与被迁移的x1（从右侧取样）相同的分布，反之亦然。与我们的第一个模型类似，第二个模型使用两个判别器D1和D2来排列群体。D1的任务是区分真实x1和被迁移的x2，D2的任务是区分真实的x2和被迁移的x1。 对抗训练由G生成的离散样本由于梯度阻断无法进行反向传播，虽然很多人使用强化学习，但由于高方差的采样梯度导致收敛并不稳定。首先，我们不使用单个采样词作为生成器RNN的输入，而是使用词上的softmax分布。具体地说，在从G(y1,z2)迁移x2的生成过程中，假设在时间步t处输出logit向量是vt。我们将其峰值分布softmax（vt/γ）作为下一个输入，其中γ是温度参数。 其次，我们使用Professor Forcing（Lamb et al.，2016）来匹配隐藏状态序列，包含输出信息且平滑分布的输出词。也就是说，到判别器D1的输入是由实例x1强制的（1）G（y1；z1）教师的隐藏状态序列，或者由先前的软分布自馈的（2）G（y1；z2）教师的隐藏状态序列。 交叉对齐自动编码器的运行过程如图2所示。注意，交叉对齐加强了潜在变量z在生成器G的递归网络上的对齐。通过对齐整个隐藏状态序列，它可以防止z1和z2的初始不对齐在递归生成过程中传播，因此转移的句子可能会结束在远离目标的某个地方目标域。 我们使用卷积神经网络实现D1和D2序列分类（Kim，2014）。算法1给出了训练算法。 5 Experimental setup Sentiment modification 我们的第一个实验是以改变潜在情绪为目标的文本改写，这可以看作是否定句和肯定句之间的风格转换。我们在Yelp餐厅评论上进行实验，利用与每个评论相关的现成用户评分。按照标准惯例，评分高于三分的评审被认为是积极的，低于三分的评审被认为是消极的。当我们的模型在句子层次上运行时，数据集中的情感注释在文档层次上提供。我们假设文档中的所有句子都有相同的情感。这显然过于简单化了，因为有些句子（例如背景）是情绪中立的。考虑到这样的句子在长评论中更为常见，我们过滤掉超过10句的评论。我们进一步过滤剩下的句子，去掉那些超过15个单词的句子。结果数据集有25万个否定句和35万个肯定句。将出现少于5次的单词替换为“”标记后，词汇量为10K。作为基线模型，我们与Hu et al.（2017）的对照control-gen模型进行了比较。 为了定量评估转换的句子，我们采用了一种基于模型的评估标准，类似于用于图像传输的评估标准（Isola et al.，2016）。具体地说，我们根据预先训练好的情感分类器来衡量一个转移句有多少正确的情感。为此，我们使用Kim（2014）中描述的TextCNN模型。在我们简化的风格转换数据集上，它几乎达到了97.4%的完美准确率。 虽然定量评估提供了一些传输质量的指标，但它并没有涵盖这一代任务的所有方面。因此，我们还对从测试集2中随机选取的500个句子进行了两次人类评估。在第一次评估中，评委们被要求根据句子的流畅性和情感程度对生成的句子进行排名。流利程度从1分（不可读）到4分（完美），而情感类别是“积极”、“消极”或“两者都不是”（可能是矛盾的、中性的或无意义的）。在第二个评价中，我们对转移过程进行了比较评价。以随机顺序向注释者展示源句和系统的相应输出，并询问“哪一个转移句在语义上等同于具有相反情感的源句？”？”. 它们可以都令人满意，A/B更好，也可以都不令人满意。我们为每个问题收集两个标签。标签协议和冲突解决策略可在补充材料中找到。请注意，这两个评估不是多余的。例如，一个系统总是独立于源句生成语法正确、情感正确的句子，在第一个评估设置中得分较高，但在第二个评估设置中得分较低。 Word substitution decipherment我们的第二组实验涉及字词替换密码的破译，这在NLP文献中已有探讨（Dou和Knight，2012；Nuhn和Ney，2013）。这些密码将明文（自然语言）中的每个单词根据1:1替换密钥替换为密码令牌。解密任务是从密文中恢复明文。如果我们能够访问并行数据，这任务是微不足道的。然而，我们有兴趣考虑一个非并行破译方案。为了训练，我们选择200K个句子作为X1，并对不同的200K个句子集应用替换密码f得到X2。虽然这些句子是非平行的，但它们是从评论数据集中的相同分布中提取的。开发集和测试集有100K个平行句D1和D2。我们可以使用Bleu分数定量比较D1和转移（解密）D2（Papineni et al.，2002）。 显然，这个破译任务的难度取决于替换词的数量。因此，我们根据替换词汇表的百分比来报告模型性能。注意，转移模型不知道f是一个词替换函数。他们完全从数据分发中学习。 除了有不同的迁移模型外，我们还引入了一个简单的基于词频的解码基线。具体来说，我们假设X1和X2之间共享的单词不需要翻译。其余的单词根据频率进行映射，任意断开连接。最后，为了评估任务的难度，我们报告了在平行语料库上训练的机器翻译系统的准确性（Klein et al.，2017）。 Word order recovery我们最后的实验集中在单词排序任务，也称为bag翻译（Brown et al.，1990；Schmaltz et al.，2016）。通过学习原始英语句子X1和混叠英语句子X2之间的语体迁移函数，该模型可以用来恢复混叠句子的原始语序（或者反过来随机排列句子）。非平行训练数据和平行测试数据的构造过程与单词替换破译实验相同。同样，传输模型不知道f是一个随机函数，完全从数据中学习 6 Results 7 Conclusion之前的迁移学习都是使用平行语料，本文工作中，我们将其定义为翻译问题，并使用非平行的语料。我们的方法使用强制的分布表示对齐去优化神经网络，我们使用情感迁移任务、文本破译、单词排序等任务证明我们方法的有效性。本文也引出了一个问题：when can thejoint distribution p(x1; x2) be recovered given only marginal distributions?，我们认为解决此问题可以促进情感迁移在CV和NLP中的发展。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"Towards Fine-grained Text Sentiment Transfer 笔记","slug":"FGST","date":"2021-01-12T23:53:55.000Z","updated":"2021-01-19T08:27:34.718Z","comments":true,"path":"2021/01/13/FGST/","link":"","permalink":"http://renxingkai.github.io/2021/01/13/FGST/","excerpt":"","text":"论文链接Abstract本文专注于细粒度的情感迁移(FGST)，该任务的目标是在保留原始语义内容的同时，修改输入序列以满足给定的情感强度。不同于传统的情绪传递任务，它只反转文本的情绪极性（正/负），FTST任务需要更细致和细粒度的情绪控制。为了解决这个问题，我们提出了一个新的Seq2SentiSeq模型。具体地，通过高斯核层将数字情感强度值并入解码器以精细地控制输出的情感强度。此外，针对并行数据不足的问题，提出了一种循环强化学习算法来指导模型训练。在这个框架中，精心设计的奖励可以平衡情感转换和内容保存，同时不需要任何ground truth的输出。实验结果表明，该方法在自动评价和人工评价方面均优于现有方法。 1 Introduction为了对文本生成进行更细致、更精确的情感控制，我们转向细粒度文本情感转移（FTST），它在保持语义内容不变的情况下，修改一个序列以满足给定的情感强度。例子如下： FTST任务有两个主要挑战。首先，在生成句子时很难实现情感强度的细粒度控制。以前关于粗粒度文本情感迁移的工作通常对每个情感标签使用单独的解码器（Xu et al.，2018；Zhang et al.，2018b）或将每个情感标签嵌入单独的向量（Fu et al.，2018；Li et al.，2018）。然而，这些方法对于细粒度文本情感迁移是不可行的，因为目标情感强度值是一个实际值，而不是离散的标签。第二，平行数据在实践中是不可用的。换句话说，我们只能访问标有细粒度情绪评级或强度值的语料库。因此，在FTST任务中，我们不能通过ground truth输出来训练生成模型。 针对上述两大挑战，我们提出了两个相应的解决方案。首先，为了控制生成句子的情感强度，我们提出了一种新的情感强度控制序列对序列（Seq2Seq）模型Seq2SentiSeq。它通过高斯核层将情感强度值引入到传统的Seq2Seq模型中。通过这种方式，该模型可以鼓励在解码过程中生成情感强度接近给定强度值的词。其次，由于缺乏并行数据，我们不能直接用极大似然估计（MLE）对模型进行训练。因此，我们提出了一种循环强化学习算法来指导模型训练，而不需要任何并行数据。设计的奖励可以平衡情感迁移和内容保存，同时不需要任何地面真相输出。 本文主要贡献如下： 提出了一种情感强度控制的生成模型Seq2SentiSeq，通过高斯核层引入情感强度值，实现对生成句子的细粒度情感控制。 为了适应非并行数据，我们设计了一种循环强化学习算法CycleRL来指导模型的无监督训练。 实验表明，该方法在自动评价和人工评价两方面均优于现有系统。 2 Proposed Model2.1 Task Definition给定一个输入序列x和一个目标情绪强度值$v_y$，FTST任务的目标是生成一个序列y，该序列y不仅表达了目标情绪强度$v_y$，而且保留了输入x的原始语义内容。在不损失通用性的前提下，我们将情绪强度值$v_y$限制在0（最负）到1（最负）之间阳性。 2.2.1 Encoder使用双向RNN对句子进行编码，获取每个词的表示$h_i$。 2.2.2 Decoder给定输入句子的隐层表示$\\{h_i\\}_{i=1}^m$和目标情感强度值$v_y$，decoder目标去生成序列y,不仅与x描述相同的语义，同时表达出了情感$v_y$。 为了在解码过程中达到控制情感的目的，我们首先在原有的语义表示之外，在每个词中嵌入一个额外的情感表示。语义表征表征词的语义内容，情感表征词表征情感的强度。形式上，解码器在时间步t的隐藏状态st计算如下： 与传统的Seq2Seq模型（Bahdanau et al.，2014）类似，整个词汇表的语义概率分布计算如下 情绪概率度量生成序列的情绪强度与目标vy的接近程度。通常，每个词都有特定的情感强度。例如，单词“好”的强度约为0.6，“好”的强度约为0.7，“好”的强度约为0.8。然而，当涉及到之前生成的词时，当前生成词的情感强度可能完全不同。例如，短语“不好”的负强度约为0.3，而“非常好”的负强度约为0.9。也就是说，每个词在时间步t的情感强度应该由情感表示$E_s$和当前解码器状态$s_t$两者决定。因此，我们定义了一个情感预测的函数$g(E_s,s_t)$直观地说，为了实现情感的细粒度控制，情感强度更接近目标情感强度值$v_y$的词应该被赋予更高的概率。 受Luong et al.（2015）和Zhang et al.（2018a）的启发，为了支持情感强度接近vy的词，我们引入了一个高斯核层，该层以vy为中心放置高斯分布。具体而言，情绪概率表示为： 为了平衡情感转换和内容保存，将整个词汇表的最终概率分布pt定义为两个概率分布的混合 2.3 Training: Cycle Reinforcement LearningFTST任务的一个严重挑战是缺乏并行数据。由于ground truth输出y是不可观测的，因此不能直接用极大似然估计（MLE）进行训练。为此，我们设计了一种循环强化学习（CycleRL）算法。算法1概述了训练过程。两个奖励旨在鼓励改变情绪，但保留内容，而不需要平行数据。下面介绍Seq2SentiSeq模型的两个奖励和相应的梯度的定义。 2.3.1Reward Design我们为FTST的两个目标(sentiment transformation and content preservation)分别设计两个rewards，然后使用一个全局reward r去平衡两个目标和指导模型训练。 Reward for sentiment transformation.一个预先训练的情感评分器用来评估样本句子$\\hat{y}$与目标情感强度值$v_y$的匹配程度。具体而言，情绪转化的回报公式如下： Reward for content preservation.直观地说，如果模型在内容保存方面表现良好，则很容易对源输入x进行反向重构。因此，我们将内容保存的报酬设计为基于生成的文本$\\hat{y}$和源情感强度值$v_x$的模型重构x的概率。 Overall reward.最终整体的reward。 3 Experimental Setup3.1 Dataset我们在Yelp数据集5上进行实验，该数据集包含大量的产品评论。每一篇评论都被分配了一个从1到5的情绪等级。由于细粒度评分中人与人之间的标签不一致更为严重，因此我们对Jaccard相似度大于0.9的句子进行平均评分。然后，将平均评级标准化为0到1之间的情绪强度。其他数据预处理同沈等（2017）。最后，我们得到了总共64万个句子。我们随机分为630k训练，10k验证，500k测试。尽管训练数据集的情绪强度分布不均匀，但该框架由一个均匀的数据扩充组成，该扩充生成的句子强度来自区间[0，1]，步长为0.05，以指导模型训练（算法1中的步骤6）。 3.3 Baselines细粒度系统旨在修改输入句子以满足给定的情感强度。Liao et al.（2018）构建伪平行语料库来训练一个模型，该模型是一个修正的VAE和一个耦合组件的组合，该组件对伪平行数据进行建模，并具有三个额外的损失L。此外，我们还考虑了SCSeq2Seq（Zhang等人，2018a），这是在对话生成中提出的一种特异性控制的Seq2Seq模型。为了适应这种无监督的任务，提出的CycleRL训练算法被用来训练SC-Seq2Seq模型。 粗粒度系统旨在反转输入的情绪极性（正/负），这可以被视为情绪强度设置为低于平均值（负）或高于平均值（正）的特殊情况。我们将我们提出的方法与以下最先进的系统进行了比较：CrossAlign（Shen et al.，2017）、MultiDecoder（Fu et al.，2018）、DeleteRetrieve（Li et al.，2018）和Unpaired（Xu et al.，2018）。 3.4 Evaluation Metrics3.4.1 Automatic Evaluation Content (BLEU) Fluency (PPL) Sentiment 3.4.2 Human Evaluation4 Results and Discussion 5 Related Work近年来，关于无监督情绪传递的文献越来越多。这项任务的目的是翻转一个句子的情感极性，但保持其内容不变，没有平行数据。然而，对于情感的细粒度控制的研究却很少。Liao et al.（2018）通过启发式规则利用伪并行数据，从而将此任务转化为监督任务。然后提出了一种基于变分自动编码器（VAE）的模型，首先将内容因子和源情感因子分离，然后将内容因子和目标情感因子结合起来。然而，伪并行数据的质量并不理想，这严重影响了VAE模型的性能。与之不同的是，我们在训练过程中通过反译（Lample等人，2018b）动态更新伪并行数据（等式12）。 NLP的其他一些任务也对控制文本生成的细粒度属性感兴趣。例如，Zhang et al.（2018a）和Ke et al.（2018）提出控制对话生成的特异性和多样性。我们从这些文章中借鉴了一些想法，但我们的工作动机和提出的模式却与之相去甚远。主要区别在于：（1）由于情感依赖于局部语境，而特异性独立于局部语境，因此我们的模型中有一系列的设计来考虑局部语境（或先前生成的词）st（例如，公式1，公式3）。（2） 由于缺乏并行数据，我们提出了一种循环强化学习算法来训练所提出的模型（第2.3节）。 6 Conclusion我们提出了一个Seq2SentiSeq模型来控制生成句子的细粒度情感强度。为了在没有任何并行数据的情况下训练所提出的模型，我们设计了一种循环强化学习算法。我们将所提出的方法应用于Yelp评论数据集，获得了自动评估和人工评估的最新结果。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"图神经网络相关学习","slug":"graph-embedding","date":"2021-01-09T10:00:56.000Z","updated":"2021-01-11T03:09:40.943Z","comments":true,"path":"2021/01/09/graph-embedding/","link":"","permalink":"http://renxingkai.github.io/2021/01/09/graph-embedding/","excerpt":"","text":"只用于记录学习，图侵删。 图学习算法分类结构图 Deep Walk &amp; Node2vec GraphSAGE","categories":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"}],"tags":[{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"}],"author":"CinKate"},{"title":"Style Transformer：Unpaired Text Style Transfer without Disentangled Latent Representation 笔记","slug":"style-transfomer-paper","date":"2021-01-06T21:31:33.000Z","updated":"2021-01-11T03:19:15.909Z","comments":true,"path":"2021/01/07/style-transfomer-paper/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/style-transfomer-paper/","excerpt":"","text":"论文链接Abstract在未配对的文本风格转换中，对潜空间中的文本内容和风格进行消解是一种普遍现象。然而有两个主要的问题存在现在的大多数神经网络中：1)从一个句子的语义中完全剔除风格信息是很困难的；2)基于递归神经网络（RNN）的编码器和解码器，在潜在表示的中介下，不能很好地处理长期依赖的问题，导致非文体语义内容保存不好。在本文中，我们提出了Style Transfomer，它不需要假设源句的潜在表示，而是在Transformer中配置注意机制，以实现更好的风格转换和内容保留。 1.Introduction语篇风格转换的任务是改变语篇的风格属性（如情感），同时在语境中保留与风格无关的内容。由于文本风格定义比较模糊，构建相同内容但文本风格不一样的句子对是比较困难的。因此文本风格迁移研究主要关注在未成对的文本句子。 神经网络成为了主流的文本风格迁移工具，主要使用Seq2Seq结构，encoder负责编码句子到一个vector，decoder生成不同风格的文本但是保留了原来的内容。这些方法都在专注于如何分清潜在语义空间中的content和style。由于没有成对的句子，对抗loss被使用到潜在空间去促进潜在空间中的风格信息。 现在面临的一些困难： 在潜在语义空间中，区分content和style是困难的， 区分content和style是没有必要的，一个好的风格迁移器，其实不用区分content和style，最好进行直接改写并输出。 受限的vector表示，会使得较长句子编码损失信息 为了理清潜在空间中的content和style，所有的现存方法都假设输入句子被编码到一个固定大小的潜在空间。这些方法因此不能使用attention机制去加强输入句子中的信息。 大多数模型使用RNN去作为encoder和decoder,对于较长句子编码效果并不好。 本文使用Transfomer作为基础模块。贡献如下： 我们提出一个创新的训练算法，不假设需要理清输入句子的content和style，因此，模型可以使用attention机制去提升性能。 首次使用Transfomer在风格迁移任务中。 在两个数据集中我们模型效果都很好，特别地，在content保留中，Style Transfomer取得了较大性能的提升。 2.Related Work3.Style Transformer3.1 Problem Formalization假设有K种不同的数据集Di,可以定义K种不同的风格，文本风格迁移目标是给一个随机的自然语言句子x和一个想要的风格s，需要让机器重写句子x成为新的句子y,句子y包含新的风格s,同时又保留了x的文本信息(语义)。 3.2 Model Overview本文目标是学习一个映射函数f(x,s) ，x是一个自然语言句子，s是一个风格控制变量。函数f的输出是迁移之后的句子y。 3.3 Style Transformer NetworkTransformer是一个标准的encoder-decoder结构，对于一个输入句子x=(x_1,x_2,...,x_n) 编码器encoder将输入映射到连续表示z=(z_1,z_2,...,z_n) 解码器根据条件概率和自回归的方式生成输出句子：p_\\theta(y|x)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 在每一时间步t，下一个token通过softmax分类器来计算：p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;)=softmax(o_t) ot是Transfomer decoder的输出。 为了能够控制风格，我们在Transformer Encoder中加入了一个额外的风格向量:Encoder(x,s,\\theta_E) 因此，网络的输出变为:p_\\theta(y|x,s)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 我们定义网络的预测输出为f_(x,x) 3.4 Discriminator Network因为大部分情况下缺少并行语料来进行有监督训练，我们提出了使用判别网络去从非平行语料中学习监督信息。我们构建了两种方法 为了保留内容信息，当我们将迁移语句$\\hat{y}=f_\\theta(x,\\hat{s})$送到带有原始标签s的Style Transfromer中时，我们训练网络重构原始输入语句x 对于风格控制，我们训练了一个判别网络去更好地控制被生成的句子。 判别网络是另一个TRM encoder,学习区分不同句子的风格。Style TRM网络接收风格监督信息从这个判别网络中。我们实验了两种判别网络： Conditional Discriminator 一个句子x和一个风格s被输入到判别网络$d_{\\phi}(x,s)$判别器需要输出句子x是否含有相关的风格。在判别器训练阶段，数据集x中的真是句子和重构句子$y=f_{\\theta}(x,s)$被标记为正样本，迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为负样本。在Style TRM训练阶段，网络$f_\\theta$被训练最大化正样本概率当输入$\\hat{y}=f_\\theta(x,\\hat{s})$和$\\hat s$到判别器中。 Multi-class Discriminator 仅有一个句子x被输入到判别器$d_{\\phi}(x)$，判别器的目标是去回答此句话的风格。更具体地说，判别器是具有K+1类的分类器。前K类代表K种不同的风格，最后一类代表$f_\\theta(x,\\hat s)$生成的数据，也常被称为假样本。在判别器训练阶段，我们标记真实句子x和重构句子$y=f_{\\theta}(x,s)$为相关的风格，至于被迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为类别0。在Style TRM学习阶段，我们训练网络$f_\\theta(x,\\hat{s})$最大化代表风格$\\hat s$的概率。 3.5 Learning Algorithm该模型的训练算法可分为两部分：判别器学习和风格变换网络学习。架构图如图2所示 3.5.1 Discriminator Learning我们训练我们的判别器来区分真实句字x和重构句子$y=f_{\\theta}(x,s)$与迁移句子$\\hat{y}=f_\\theta(x,\\hat{s})$。损失函数为交叉熵算法细节： 3.5.2 Style Transformer Learning根据$s=\\hat s$和$s!=\\hat s$分为两种情况。 Self Reconstruction 对于$s=\\hat s$，输入句子x和风格s都是来于相同数据集，可以直接训练Style TRM，去重构输入句子，通过最小化-log likehood: L_&#123;self&#125;(\\theta)=-p_&#123;\\theta&#125;(y=x|x,s) 对于$s!=\\hat s$，没有监督数据，不能直接获得loss，因此提出两种不同的训练loss： Cycle Reconstruction为了鼓励生成的句子保留输入句子x中的信息，我们将生成的句$\\hat{y}=f_\\theta(x,\\hat{s})$以x的风格输入给Style TRM，并训练我们的网络通过最小化负对数似然来重构原始输入句子： Style Controlling如果我们只训练我们的Style TRM从转移句$\\hat{y}=f_\\theta(x,\\hat{s})$重构输入句x，网络只能学习将输入复制到输出。为了处理这个退化问题，我们进一步为生成的句子添加了风格控制损失。即将网络生成的句子$\\hat y$输入到判别器中，以最大限度地提高$\\hat s$的概率。对于conditional discriminator，Style TRM的目标是在使用样式标签$\\hat s$输入到判别器时最小化类1的负对数似然性： 在multi-class discriminator的情况下，Style TRM以最小化对应类型的风格$\\hat s$的负对数似然： 组合loss之后，Style TRM学习过程如下： 3.5.3 Summarization and Discussion与GANs的训练过程类似（Goodfellow et al.，2014），在每个训练迭代中，我们首先执行$n_d$步鉴别器学习以获得更好的鉴别器，然后训练我们的Style TRM$n_f$步以提高其性能。算法3对训练过程进行了总结 由于自然语言的离散性，对于生成的句子$\\hat{y}=f_\\theta(x,\\hat{s})$，我们不能通过离散样本直接传播来自判别器的梯度。在我们的实验中，我们还观察到Gumbel-Softmax技巧会减慢模型的收敛速度，并且没有给模型带来太多的性能改进。基于以上原因，我们将fθ生成的softmax分布视为一个“软”生成句子，并将该分布输入给下游网络，以保持整个训练过程的连续性。当使用这种近似时，我们也将我们的解码器网络从贪婪解码切换到连续解码。也就是说，在每一个时间步，我们将整个softmax分布（等式（2））输入到网络，而不是将在先前预测步骤中具有最大概率的令牌馈送到网络。解码器利用这个分布，从输入的嵌入矩阵计算加权平均嵌入。 4 Experiment4.1 DatasetsYelp Review Dataset (Yelp) Yelp数据集是由Yelp数据集挑战提供的，由带有情绪标签（负面或正面）的餐馆和商业评论组成。在之前的工作之后，我们使用了Li等人（2018）提供的数据集。此外，它还为测试集提供人类参考句。 IMDb Movie Review Dataset (IMDb) IMDb数据集由在线用户撰写的影评组成。为了获得高质量的数据集，我们使用了Maas等人（2011）提供的极性电影评论。基于此数据集，我们通过以下步骤构建了一个高度极性的句子级风格转换数据集：1）在原始训练集上微调一个BERT（Devlin et al.，2018）分类器，在测试集上达到95%的准确率；2）将原始数据集中的每个评论分成几个句子；3） 通过我们微调的BERT分类器过滤掉置信阈值低于0.9的句子；4）删除不常见单词的句子。最后，这个数据集包含366K、4k和2k个句子，分别用于训练、验证和测试. 4.2 Evaluation目标转移句应该是一个流利的、内容完整的、具有目标风格的句子。为了评估不同模型的表现，我们在前人工作的基础上，比较了生成样本的三个不同维度：1）风格控制，2）内容保留(BLEU)，3）流畅性(perplexity)。 4.3 Training Details在所有的实验中，对于编码器、解码器和鉴别器，我们都使用了四层TRM，每层有四个注意头。Transformer中的隐藏大小、嵌入大小和位置编码大小都是256维。另一个包含256个隐藏单元的嵌入矩阵用来表示不同的风格，作为输入语句的额外标记输入到编码器中。并且位置编码不用于样式标记。对于鉴别器，类似于Radford et al.（2018）和Devlin et al.（2018），我们进一步在输入中添加标记，并将相应位置的输出向量馈送到表示鉴别器输出的softmax分类器中。 4.4 Experiment Results实验结果一些迁移例子 5.Conclusions and Future Work本文我们提出了Style Transfomer，实验结果在两个数据集中展示出了我们模型比之前的SOTA模型有竞争力的结果，特别地是，由于我们提出的方法没有假设一个分离的潜在表示来操纵句子的风格，我们的模型可以在两个数据集上得到更好的内容保留。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"常见的评估指标","slug":"metrics","date":"2021-01-06T19:29:52.000Z","updated":"2021-01-11T03:10:07.812Z","comments":true,"path":"2021/01/07/metrics/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/metrics/","excerpt":"","text":"准确率，精确率，F1值，ROC，AUC，P-R，mAP(图来自网络，侵删)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"}],"tags":[{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"}],"author":"CinKate"},{"title":"搜索中的NLP技术","slug":"searchandnlp","date":"2020-12-26T10:25:23.000Z","updated":"2021-01-06T11:33:26.871Z","comments":true,"path":"2020/12/26/searchandnlp/","link":"","permalink":"http://renxingkai.github.io/2020/12/26/searchandnlp/","excerpt":"","text":"搜索技术除了涉及基础的搜索算法，也涉及到很多NLP技术，本文转载于，只是做个学习记录，侵删。 推荐系统被捧为目前算法领域的主流，推荐系统不需要用户主动进行操作就能获取自己喜欢的东西，但是实际上，搜索系统在很长一段时间占据了重要位置，大到百度的大搜，小到音乐、视频、电商、应用商店等，都有各种各样的搜索引擎，这些搜索搜索能更为精准直接的满足用户需求，即使是推荐系统如日中天，目前也仍会有搜索的一席之地。今天我来为大家介绍，搜索系统中涉及的算法问题，也让大家了解，搜索中需要什么算法。 数据预处理模块这个很好理解吧，用户query进来，一般要做如下处理: 大小写转化。 删除标点符号（当然有的分析会保留标点，但是建议在这个场景下还是去掉更好）。 繁体转简体。 数字处理，转中文，转阿拉伯数字，甚至罗马字等，部分情况要删除数字。 长度截断。（总不能让用户输入一本西游记，然后你还在整吧？） 理解模块query理解其实是一个非常重要的模块，好的query理解能为后续的工作提供很大的支持。这部分往往不直接应对下游，而是按需使用，有点像辅助吧。 分词。基操。 关键词抽取。会涉及丢词、排序等功能。 命名实体识别。一方面协助用户识别，另一方面可能会涉及数据库查询匹配的内容。在垂搜中比较常见，大搜也有但是相比之下没那么精准和常见。 紧密度分析，避免由于切词出现错误导致词义偏移的问题，这个其实并不少见，尤其是在人名上，这个是别的精准度会很低，近期的如“武大靖”，会被分为“武大 靖”，“曾舜晞”直接被分为了3个字，挺头疼的。 非法信息过滤。例如黄色、暴力血腥、反动等。 改写模块改写模块其实非常关键，这是连接用户query和数据库底层数据的桥梁，数据库的存储有特定的形式，但是用户不会按照你的底层数据结构去写，例如，用户不见得会输入和平精英，而是吃鸡，数据库里可不见得会存吃鸡对吧，所以这块很重要。 同义词改写。上面的吃鸡就要改写为和平精英，这个需要通过同义词挖掘等方式去构造词典实现。 拼音改写。数据库是罗密欧与朱丽叶，但是用户输入的是罗密欧与朱莉业，拼音改写其实颇为常见，用户经常由于找不到需要的结果或者不知道应该需要哪个，于是直接输入后开始搜索。 前缀补全。非常常见，用户输入射雕，射雕英雄传就要出了，这个一般的方法也是构造词典，另外有一个很重要的需要了解的就是前缀树，这个能让你查询的时间非常低的水平（只和query长度本身有关）。 丢词和留词。结合上述关键词提取和命名实体识别完成，有些不必要的词汇需要被删除，例如“李小璐到底怎么了”，整个句子只有李小璐是关键词，其他词如果也通过and逻辑召回，就没有信息召回了，这时候其实可以直接删除或者将降级到or逻辑。留词和丢词相反，丢词如果是做减法，留词就是做加法。 近义词召回。这个召回不是从数据库中召回，而是召回近义词，具体的方法是通过embedding方法转化词汇，然后通过ball tree、simhash的方式召回与之意思相近的词汇，该模式虽然比较激进，但是能一定程度增加召回，有一定效果。 意图识别。与其说是意图，我更喜欢理解为这是直接针对底层数据结构产生的需要解决的问题，query这是一条，但是数据库有好几个，我们要去哪个数据库搜，这是需要识别的，而这个数据库的设计往往和品类、意图有关，找酒店、找景点都是不同的，所以此时就要进行意图识别，一般地方法是抽象为文本分类，但是很多时候语义本身是无法体现出真实意图的，例如少年的你，语义上其实很难分析出，有时候更复杂的会夹带一些实体识别、词性、词典之类的信息。 召回模块结合命名实体识别、改写结果，然后开始召回，模式比较多，包括但不限于下面形式： Elastic Search，著名的搜索平台ES，有些时候甚至简单的搜索平台直接用它管整个搜索引擎但是要精做，就只能把它当做底层的数据库和搜索平台。 关系数据库。MySQL之类的，但是在我的理解速度和并发性都不是特别好。 Redis。KV形式的数据库，速度很快，但是Key匹配必须是精确匹配，这需要改写模块具有很强的能力。 说白了，就是把用上面流程处理过的query放到数据库里面查，这个其实就是召回。 排序模块内容是召回回来了，其实怎么展现给用户呢？这里是需要深度挖掘用户的实际需求的，很多时候甚至需要做个性化，但是更多时候是我们能够得到的只有短短的一句用户query，那么，我们就要好好利用好这个query，来做好排序，让用户喜欢的（当然，还有广告商喜欢的哈哈哈）东西放在前面，实际上就是一个学习排序的问题了，这个我之前也做过一些简单的介绍，那么在特征层面，可以考虑如下的信息： query层面： 本身的embedding，后续迭代可以走elmo后逐渐形成的pre-train的模式。 词性、实体、词权重、offset等序列标注得到的结果。 document层面。 如果文档本身有文本，最好是标题类的，也把embeding引入，和query层面那种一样。如果只有query文本和document文本，其实就是个文本相似度模型了对吧。 综合层面： query和document的ctr、cqr、bm25，句向量余弦相似度等匹配信息。 其他层面： 意图识别结果。 模型上，DSSM系列似乎是比较流行的方法，但是提取一些特定的特征，有点的时候简单的LR、XGBOOST就能一定程度把问题解决好了。 而在排序模块中，还会涉及一些硬规则等。 搜索引导模块query suggestion是一个上述搜索过程非常类似的模块，只不过处理的结果大部分是放在离线，在线是指一个查字典的过程，那么离线部分，其实是做了一整个搜索过程的：预处理、query理解、改写、召回、排序。 预处理：和上面的预处理类似，不赘述。 query理解，和上述内容类似，但是有的时候会简化，直接进入改写模块。 改写模块，会进行容忍度更高的前缀匹配，去找回一些用户可能会喜欢的内容，这时候往往还会带上统计特征，从整体层面上看用户喜欢的内容，毕竟不同时间用户喜欢的可能不同，例如对热点新闻的偏好。 召回和排序模块。召回模块不是召回数据了，而是召回相似的doc信息，也可以是相似的历史用户query，尤其是后者，如果是后者，则要确认历史用户query是有结果的。 其他辅助模块显然，整个搜索系统远远不止这些内容，在算法视角下，其实还需要很多辅助模块协助我们进行算法开发。 日志模块。无论是线上的运行日志，还是线下的模型实验和离线模型运行，都需要日志协助，对于线上运行和离线模型运行而言，出现错误可以方便追溯，找到ERROR出现的时间和具体问题，而线下的模型试验能方便我们计算运行时间、找到bug，而非每次训练模型的时候功亏一篑才来找问题。 实验模块。由于算法策略存在很多不确定性，无论是算法内存占用和时间，还是算法实际结果，因此需要利用AB实验的方式来验证，快速进行分析，对有问题的算法及时下线检查原因。 数据分析和报表模块。结合实验模块，需要对日常用户数据进行分析，这个比较好理解，不赘述。 特征生产模块。特征生产涉及两块，一个是线上的实时计算，对于一些实时数据，是需要即时生成的，例如微博热搜里面就需要一些诸如5分钟搜索量之类的特征，这个只能从实时计算模块中获取；另一个是离线模块，为了进行离线模型训练，需要定时将生肉（原始数据）转化为特征，方便进行下一步计算，如果这个能变成日常任务，那开发人员就不需要临时造轮子执行，还要长时间等待。 定时任务模块。很多任务其实是定时开始的，报表生成、特征生产，甚至是一些实时任务（说白了就是短时间内的计算），因此有定时任务模块去管理这些定时任务，将会非常方便。 模型管理模块。首先一个项目中可能存在大量模型，然后因为各个模型训练需要花费大量资源，还要结合AB试验，另外还有模型的更新和保存机制（防止模型加载失败导致的无模型可用），因此需要构建模型模块统一管理模型，这个不是每个部门都有，或者说每个模型都是各自管理，但是有一个比较好管理模型平台是非常高效的。 数据平台。额，简单地说就是写SQL的地方，但是也有类似ETL之类的内容，和特征生产模块很接近。 小结搜索作为一个完整地系统，难度甚至比搜索要困难，在更多的场景下，搜索系统只能针对短短几个字的query进行分析，从而在海量数据中找到用户需要的东西，而由于是用户主动输入的，所以用户的预期非常高，但因为是用户主动的行为，所以不会太过复杂，甚至可能会有各种各样的歧义，只有详细分析和挖掘才能得到，因此在我看来，搜索这个事情非常具有挑战性，即使这个东西已经发展多年，年龄比推荐系统更大，还是很多可挖掘的地方。","categories":[{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"}],"tags":[{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"}],"author":"CinKate"},{"title":"Git常用复习","slug":"git-learn","date":"2020-12-23T20:55:42.000Z","updated":"2020-12-23T12:57:43.570Z","comments":true,"path":"2020/12/24/git-learn/","link":"","permalink":"http://renxingkai.github.io/2020/12/24/git-learn/","excerpt":"","text":"版本控制的分类 本地版本控制，如RCS。最常见，最简单的版本控制方法。 集中版本控制，如SVN。所有的版本数据都保存在服务器上，协同开发者从服务器上同步更新或上传自己的修改。必须联网。。。 分布式版本控制，如Git。所有版本信息仓库全部同步在本地的每个用户中，可以在本地查看所有版本历史，可以在本地离线提交，有网了之后push到服务器。但是每个人都有全部代码，存在安全隐患。 Git原理 git add files 将本地文件添加到暂存区stage，git commit 将暂存区文件提交到本地的git仓库，git push将本地仓库文件提交到远程仓库，如下图左边，右边对应反向操作 Workspace：工作目录，平时存放代码的位置 Index/Stage：暂存区，用于临时存放你的改动，事实上只是一个文件，保存即将提交到文件列表信息 Repository：仓库区(或本地仓库)，就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本 Remote：远程仓库，托管代码的服务器，可以简单地认为是你项目组中的一台电脑用于远程数据交换 Git提交时忽略的文件在主目录下建立”.gitinore”文件，此文件以下规则常用#为注释*.txt #忽略所有.txt结尾的文件,这样的话，上传就不会被选中!lib.txt #但lib.txt除外/temp #仅忽略项目根目录下的TODO文件，不包括其他目录tempbuild/ #忽略build/目录下的所有文件doc/*.txt #会忽略 doc/notes.txt 但不包括 doc/server/arch.txt Git分支#列出所有本地分支git branch#列出所有远程分支git branch -r#新建一个分支git branch [branch-name]#虽然新建了但是默认还是在master分支下显示#新建一个分支并切换到该分支git checkout -b [branch-name]#合并指定分支到当前分支git merge [branch]#删除分支git branch -d [branch-name]#删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 多个分支如果并行执行，就会导致代码不会冲突，会同时存在多个版本！ web-api -A组开发 web-admin -B组开发 B会调用A web-app -C组开发 C会调用B和A的代码 如果冲突了要进行协商！ 如果同一个文件在合并分支时都被修改了，则会引起冲突：解决的办法是我们可以修改冲突文件后重新提交！选择要保留他的代码还是你的代码！ master主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分值代码稳定后可以合并到主分支master上来。","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"}],"author":"CinKate"},{"title":"Docker基础学习","slug":"docker-learning","date":"2020-12-01T11:07:19.000Z","updated":"2020-12-22T09:10:06.146Z","comments":true,"path":"2020/12/01/docker-learning/","link":"","permalink":"http://renxingkai.github.io/2020/12/01/docker-learning/","excerpt":"","text":"Docker的常用命令帮助命令docker version #显示docker的版本信息docker info #显示docker的系统信息，包括镜像和容器的数量docker 命令 --help #帮助命令 帮助文档的地址：https://docs.docker.com/engine/reference/commandline/docker/ 镜像命令docker images #查看所有本地主机上的镜像docker images #查看所有本地主机上的镜像[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 11 months ago 13.3kB#解释REPOSITORY #镜像的仓库TAG #镜像的标签IMAGE ID #镜像的IDCREATED #镜像的创建时间SIZE #镜像的大小#可选项Options: -a, --all #列出所有镜像 --digests Show digests -q, --quiet #仅显示镜像的ID docker search #搜索镜像[root@VM-0-15-centos ~]# docker search tensorflowNAME DESCRIPTION STARS OFFICIAL AUTOMATEDtensorflow/tensorflow Official Docker images for the machine learn… 1799 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ … 248 tensorflow/serving Official images for TensorFlow Serving (http… 102 rocm/tensorflow Tensorflow with ROCm backend support 58 xblaster/tensorflow-jupyter Dockerized Jupyter with tensorflow 54 [OK]floydhub/tensorflow tensorflow 26 [OK]bitnami/tensorflow-serving Bitnami Docker Image for TensorFlow Serving 14 [OK]opensciencegrid/tensorflow-gpu TensorFlow GPU set up for OSG 12 docker pull #下载镜像#docker pull mysql 镜像名[:tag][root@VM-0-15-centos ~]# docker pull mysqlUsing default tag: latest #不写tag，默认latestlatest: Pulling from library/mysql852e50cd189d: Pull complete #分层下载,docker 镜像的核心 联合文件系统29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete a880ba7c411f: Pull complete 984f656ec6ca: Pull complete 9f497bce458a: Pull complete b9940f97694b: Pull complete 2f069358dc96: Pull complete Digest: sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093 #签名Status: Downloaded newer image for mysql:latestdocker.io/library/mysql:latest #真实地址#指定版本docker pull mysql:5.7[root@VM-0-15-centos ~]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Already exists a43f41a44c48: Already exists 5cdd802543a3: Already exists b79b040de953: Already exists 938c64119969: Already exists 7689ec51a0d9: Already exists 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7 docker rmi #删除镜像#按docker id镜像删除[root@VM-0-15-centos ~]# docker rmi -f ae0658fdbad5 Untagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Deleted: sha256:a2cf831f4221764f4484ff0df961b54f1f949ed78220de1b24046843c55ac40fDeleted: sha256:0a7adcc95a91b1ec2beab283e0bfce5ccd6df590bd5a5e894954fcf27571e7f5Deleted: sha256:0fae465cbacf7c99aa90bc286689bc88a35d086f37fd931e03966d312d5dfb10Deleted: sha256:23af125b9e54a94c064bdfacc2414b1c8fba288aff48308e8117beb08b38cb19#递归删除所有的镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: mysql:latestUntagged: mysql@sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093Deleted: sha256:dd7265748b5dc3211208fb9aa232cef8d3fefd5d9a2a80d87407b8ea649e571cDeleted: sha256:aac9a624212bf416c3b41a62212caf12ed3c578d6b17b0f15be13a7dab56628dDeleted: sha256:1bf3ce09276e9e128108b166121e5d04abd16e7de7473b53b3018c6db0cf23ffDeleted: sha256:24c6444cea460c3cc2f4e0385e3e97819a0672a54a361921f95d4582583abd59Deleted: sha256:77585ebe3eaa035694084b3c5937fe82b8972aae1e6c6070fc4d7bc391d10928Deleted: sha256:1cfd539163ceb17f7bb85a0da968714fe9258b75dbf73f5ad45392a45cfd34b7Deleted: sha256:c37f414ac8d2b5e5d39f159a6dffd30b279c1268f30186cee5da721e451726eaDeleted: sha256:955b3c214bccf3ee2a7930768137fd7ed6a72677334be67a07c78a622abd318aDeleted: sha256:a2e35a0fdb20100365e2fb26c65357fcf926ac7990bf9074a51cbac5a8358d7eDeleted: sha256:8c3a028fc66f360ce6ce6c206786df68fac4c24257474cbe4f67eda0ac21efd6Deleted: sha256:0a6d37fabaceb4faa555e729a7d97cb6ee193cb97789a213907a3d3c156d7e35Deleted: sha256:579519c51de1afe1e29d284b1741af239a307975197cf6ce213a70068d923231Deleted: sha256:f5600c6330da7bb112776ba067a32a9c20842d6ecc8ee3289f1a713b644092f8Untagged: hello-world:latestUntagged: hello-world@sha256:e7c70bb24b462baa86c102610182e3efcb12a04854e8c582838d92970a09f323Deleted: sha256:bf756fb1ae65adf866bd8c456593cd24beb6a0a061dedf42b26a993176745f6b 容器命令有了镜像才能创建容器，linux，下载一个centos镜像来测试 [root@VM-0-15-centos ~]# docker pull centos 新建容器并启动 docker run [可选参数] image#参数说明--name=&quot;Name&quot; #容器名字-d #使用后台方式运行，类似nohup-it #使用交互方式运行，进入容器查看内容-p #指定容器端口 -p 8080:8080 -p 主机端口:容器端口 (常用) -p 容器端口 容器端口-P #随机指定端口 启动并进入容器 [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@1faa1a0b849e /]# [root@1faa1a0b849e /]# lsbin etc lib lost+found mnt proc run srv tmp vardev home lib64 media opt root sbin sys usr#退出容器[root@1faa1a0b849e /]# exitexit[root@VM-0-15-centos ~]# ls 列出所有运行中的容器 #列出当前在运行的容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES#列出所有运行过的容器[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 4 minutes ago Exited (0) 55 seconds ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov#列出最近创建的容器[root@VM-0-15-centos ~]# docker ps -n=1CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) About a minute ago frosty_cori#只显示容器的编号[root@VM-0-15-centos ~]# docker ps -aq1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36 退出容器 exit #直接容器停止并退出CTRL+P+Q #容器不停止并退出[root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@e5a4a59f28ff /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 21 seconds ago Up 20 seconds agitated_ritchie 删除容器 docker rm 容器id #删除指定的容器,不能删除正在运行的容器，如要删除，加-f#递归删除所有docker容器docker rm -f $(docker ps -aq) #删除所有容器docker ps -a -q|xargs docker rm[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 5 minutes ago Up 5 minutes agitated_ritchiec4fd9794c39a centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) 5 minutes ago dreamy_lumiere1faa1a0b849e centos &quot;/bin/bash&quot; 16 minutes ago Exited (0) 12 minutes ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov[root@VM-0-15-centos ~]# docker rm e5a4a59f28ffError response from daemon: You cannot remove a running container e5a4a59f28ffc678ab9ea5ed7a9af825ff053e1816cd94c90880b3d2a87997df. Stop the container before attempting removal or force remove[root@VM-0-15-centos ~]# docker rm -f $(docker ps -aq)e5a4a59f28ffc4fd9794c39a1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36[root@VM-0-15-centos ~]# docker ps -aq[root@VM-0-15-centos ~]# 启动和停止容器的操作 docker start 容器id #启动 docker restart 容器id #重启docker stop 容器id #停止当前运行的容器docker kill 容器id #强制停止当前容器 常用其他命令后台启动容器 docker run -d centos #后台启动 #docker ps 时候，发现centos会停止，因为容器后台运行，就必须要一个前台进程，docker发现没有应用，就会自动停止 查看日志 docker logs -f -t --tail 10 容器id #发现没有日志 查看容器中的进程信息 docker top 容器id [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@ba393184f315 /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 9 seconds ago Up 8 seconds wizardly_cerf[root@VM-0-15-centos ~]# docker top ba393184f315UID PID PPID C STIME TTY TIME CMDroot 25856 25840 0 09:46 pts/0 00:00:00 /bin/bash 查看镜像的元数据 docker inspect 容器id [root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 3 minutes ago Up 3 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker inspect ba393184f315[ &#123; &quot;Id&quot;: &quot;ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73&quot;, &quot;Created&quot;: &quot;2020-12-01T01:46:02.700180427Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 25856, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2020-12-01T01:46:02.955068318Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;, &quot;Image&quot;: &quot;sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73-json.log&quot;, &quot;Name&quot;: &quot;/wizardly_cerf&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: &#123; &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: &#123;&#125; &#125;, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: &#123;&#125;, &quot;RestartPolicy&quot;: &#123; &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 &#125;, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Capabilities&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] &#125;, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;Mounts&quot;: [], &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;ba393184f315&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: true, &quot;AttachStdout&quot;: true, &quot;AttachStderr&quot;: true, &quot;Tty&quot;: true, &quot;OpenStdin&quot;: true, &quot;StdinOnce&quot;: true, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;Image&quot;: &quot;centos&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123; &quot;org.label-schema.build-date&quot;: &quot;20200809&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot; &#125; &#125;, &quot;NetworkSettings&quot;: &#123; &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;913e925d94cac8aaa4ead2ed29197af7d07d4fbfd35625642019116a42cff156&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: &#123;&#125;, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/913e925d94ca&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08&quot;, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] 进入当前运行的容器 通常容器都是后台运行的，有时我们需要进入容器修改一些配置 docker exec -it 容器id bashShell [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos \"/bin/bash\" 11 minutes ago Up 11 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker exec -it ba393184f315 /bin/bash[root@ba393184f315 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 01:46 pts/0 00:00:00 /bin/bashroot 14 0 0 01:57 pts/1 00:00:00 /bin/bashroot 27 14 0 01:57 pts/1 00:00:00 ps -ef docker attach 容器id [root@VM-0-15-centos ~]# docker attach ba393184f315[root@ba393184f315 /]# docker exec 进入容器后，开启新的终端，可以在里面操作docker attach 进入容器正在执行的终端 从容器拷贝文件到主机 docker cp 容器id:容器内路径 目的的主机路径 [root@VM-0-15-centos home]# docker attach b67b64394534[root@b67b64394534 /]# cd /home [root@b67b64394534 home]# ls[root@b67b64394534 home]# touch rx.java[root@b67b64394534 home]# read escape sequence[root@VM-0-15-centos home]# [root@VM-0-15-centos home]# docker cp b67b64394534:/home/rx.java /home[root@VM-0-15-centos home]# lsrx.java rxk.java Test1-Docker部属nginx#下载Nginx[root@VM-0-15-centos ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx852e50cd189d: Pull complete 571d7e852307: Pull complete addb10abd9cb: Pull complete d20aa7ccdb77: Pull complete 8b03f1e11359: Pull complete Digest: sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest#查看目前运行镜像[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest bc9a0695f571 6 days ago 133MBcentos latest 0d120b6ccaa8 3 months ago 215MB#以后台方式运行镜像，并且将nginx的80端口映射到本地3344端口，同时给此镜像一个名字--name nginx01[root@VM-0-15-centos ~]# docker run -d --name nginx01 -p 3344:80 nginx5a3e8a8e16e5e1f46b8bbe6c23662aebd3c13dc5cacfd63fb24d0703c1235819[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 2 minutes ago Up 2 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland#请求本地3344端口，显示nginx，成功运行[root@VM-0-15-centos ~]# curl localhost:3344&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;#进入容器[root@VM-0-15-centos ~]# docker exec -it nginx01 /bin/bashroot@5a3e8a8e16e5:/# whereis nginxnginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx#关闭容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 33 minutes ago Up 33 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop nginx01nginx01[root@VM-0-15-centos ~]# docker stop centosError response from daemon: No such container: centos[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop b67b64394534b67b64394534 Test2-Docker部属tomcat#下载并启动tomcat (加了--rm 意味着用完就会删除)[root@VM-0-15-centos ~]# docker run -it --rm tomcat:9.0Unable to find image 'tomcat:9.0' locally9.0: Pulling from library/tomcat756975cb9c7e: Pull complete d77915b4e630: Pull complete 5f37a0a41b6b: Pull complete 96b2c1e36db5: Pull complete 27a2d52b526e: Pull complete a867dba77389: Pull complete 0939c055fb79: Pull complete 0b0694ce0ae2: Pull complete 81a5f8099e05: Pull complete c3d7917d545e: Pull complete#正常下载[root@VM-0-15-centos ~]# docker pull tomcat:9.09.0: Pulling from library/tomcatDigest: sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaStatus: Image is up to date for tomcat:9.0docker.io/library/tomcat:9.0#将tomcat 8080端口映射到外部3355端口[root@VM-0-15-centos ~]# docker run -d -p 3355:8080 --name tomcat01 tomcataf2d7437747112b51f83c556b1790503f41e2632bbbe965e00a8bff3d3268c92#进入tomcat[root@VM-0-15-centos ~]# docker exec -it tomcat01 /bin/bashroot@af2d74377471:/usr/local/tomcat# lsBUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.distCONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps work Test3-Docker部属ES+kibanaES暴露的端口较多，十分耗内存，ES的数据一般挂载在安全目录 # --net somenetwork网络配置docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:tag#下载并启动ESdocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.6.2#启动之后 服务器会较为卡顿#查看CPU和内存状态#docker stats #增加内存限制 -e环境配置修改docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms64m -Xmx512m\" elasticsearch:7.6.2CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDSdb5428acc88d elasticsearch02 0.77% 358.6MiB / 1.795GiB 19.51% 656B / 0B 6.41MB / 729kB 43^C[root@VM-0-15-centos ~]# curl localhost:9200&#123; \"name\" : \"db5428acc88d\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"MqrmYUWDTT2jYZJHxsVU0A\", \"version\" : &#123; \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 可视化启动portainer [root@VM-0-15-centos ~]# docker run -d -p 8088:9000 --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainerUnable to find image 'portainer/portainer:latest' locallylatest: Pulling from portainer/portainerd1e017099d17: Pull complete 717377b83d5c: Pull complete Digest: sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Status: Downloaded newer image for portainer/portainer:latestcb19cef5927877f6e10e15013b81fffdaefd5cb2e426f34819fc869c109ab7b3 commit镜像#先启动tomcat 并确保在运行[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 55 seconds ago Up 54 seconds 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#进入tomcat[root@VM-0-15-centos ~]# docker exec -it 675a7261e3b7 /bin/bash#将webapps中的东西复制上去root@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsroot@675a7261e3b7:/usr/local/tomcat/webapps# cd ..root@675a7261e3b7:/usr/local/tomcat# cp -r webapps.dist/* webappsroot@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsROOT docs examples host-manager managerroot@675a7261e3b7:/usr/local/tomcat/webapps# exit[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 7 minutes ago Up 7 minutes 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#commit一个镜像,-a是作者,-m是描述[root@VM-0-15-centos ~]# docker commit -a=\"cinkate\" -m=\"add webapps\" 675a7261e3b7 tomcat02:1.0sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74eb[root@VM-0-15-centos ~]# docker imgaesdocker: 'imgaes' is not a docker command.See 'docker --help'#查看是否commit成功[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat02 1.0 80b6126c75b9 8 seconds ago 654MBnginx latest bc9a0695f571 8 days ago 133MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB docker容器数据卷即使删除了容器，数据还是存在的，类似于数据持久化容器之间应该有一个数据共享的技术！Docker容器中产生的数据，同步到本地！ 这就是卷技术！目录的挂载，将容器的目录，挂载到Linux上面！ 总结：为了容器的持久化和同步操作~，容器间也是可以数据共享！ 使用数据卷 方式一：直接使用命令来挂载 #-v 主机目录，容器内目录#-p 主机端口，容器内端口[root@VM-0-15-centos ~]# docker run -it -v#将centos /home下的目录 挂载到 主机下/home/ceshi下[root@VM-0-15-centos home]# docker run -it -v /home/ceshi:/home centos /bin/bash[root@081342dcbd4f /]# #另一个terminal(本机的home目录)[root@VM-0-15-centos ~]# cd /home[root@VM-0-15-centos home]# lsceshi rx.java rxk.java#查看元信息，挂载成功 Mounts[root@VM-0-15-centos home]# docker inspect 081342dcbd4f[ &#123; \"Id\": \"081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee\", \"Created\": \"2020-12-03T13:39:28.974545204Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 1314, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T13:39:29.238082253Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566\", \"ResolvConfPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hostname\", \"HostsPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hosts\", \"LogPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee-json.log\", \"Name\": \"/clever_lovelace\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": [ \"/home/ceshi:/home\" ], \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/home/ceshi\", \"Destination\": \"/home\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" &#125; ], \"Config\": &#123; \"Hostname\": \"081342dcbd4f\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"centos\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"7cc48bb3849fdd8ca238a8ba1f09409e01e7c50856abbccf23d59caaf5997c8d\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/7cc48bb3849f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;]#容器的home目录下[root@081342dcbd4f home]# touch test.java[root@081342dcbd4f home]# lstest.java#主机挂载的ceshi目录下[root@VM-0-15-centos home]# cd ceshi/[root@VM-0-15-centos ceshi]# lstest.java 关闭容器之后，在主机中挂载的目录中创建，修改文件，都是可以同步到容器中的目录。 Test4–安装MySQL#获取镜像[root@VM-0-15-centos home]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7#运行容器，需要做数据挂载# -d后台启动# -p端口映射# -v 挂载配置文件 和 数据文件# -e 修改环境变量，修改密码# --name 重命名[root@VM-0-15-centos home]# docker run -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.72f81782daf7d49e73365da7def107120a8f79c71b9c5bb21a4fc082f2ebef664#启动成功后，使用本地数据库管理软件可以进行正常连接 假设将容器删除，挂载在本地的数据卷不会丢失，实现了容器的持久化技术。 具名和匿名挂载#匿名挂载-v不写本地路径，会匿名进行挂载docker run -d -P --name nginx01 -v /etc/nginx nginx#具名挂载docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx#通过-v 卷名:容器内路径#查看这个卷所有docker容器内的卷，没有指定目录的情况下：都是在/var/lib/docker/volumes/xxxx/_data通过具名挂载，可以方便的找到一个卷，大多数情况使用具名挂载 如何确定是具名还是匿名挂载？还是指定路径挂载？ -v 容器内路径 #匿名挂载-v 卷名:容器内路径 #具名挂载-v /宿主机路径:容器内路径 #指定路径挂载 拓展 #通过 -v 容器内路径:ro rw 可以改变读写权限#ro readonly#rw readwrite#一旦设定了容器权限，容器对我们挂载出来的内容就有限定了！#ro 说明这个路径只能通过宿主机来操作，容器内部无法操作！docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginxdocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx 初始Dockerfile 方式二：Dockerfile就是用来构建docker镜像的构建文件！通过这个脚本，可以生成镜像，脚本为一个个命令，每个命令都是一层。 #构建一个rxk/centos[root@VM-0-15-centos home]# cd docker-test-volume/[root@VM-0-15-centos docker-test-volume]# ls[root@VM-0-15-centos docker-test-volume]# pwd/home/docker-test-volume[root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# nano dockerfile1#文件中的内容[root@VM-0-15-centos docker-test-volume]# cat dockerfile1 FROM centosVOLUME [\"volume01\",\"volume02\"]CMD echo \"-----end-------\"CMD /bin/bash[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t /rxk/centos .invalid argument \"/rxk/centos\" for \"-t, --tag\" flag: invalid reference formatSee 'docker build --help'.[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t rxk/centos .Sending build context to Docker daemon 2.048kBStep 1/4 : FROM centos ---&gt; 0d120b6ccaa8Step 2/4 : VOLUME [\"volume01\",\"volume02\"] ---&gt; Running in 634e59a80996Removing intermediate container 634e59a80996 ---&gt; 02a6fde9ba35Step 3/4 : CMD echo \"-----end-------\" ---&gt; Running in 3fe4feb278dfRemoving intermediate container 3fe4feb278df ---&gt; d64085501c38Step 4/4 : CMD /bin/bash ---&gt; Running in 09d704b706feRemoving intermediate container 09d704b706fe ---&gt; 7735159af50cSuccessfully built 7735159af50cSuccessfully tagged rxk/centos:latest#查看当前镜像[root@VM-0-15-centos docker-test-volume]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZErxk/centos latest 7735159af50c 5 minutes ago 215MBtomcat02 1.0 80b6126c75b9 7 hours ago 654MBnginx latest bc9a0695f571 8 days ago 133MBmysql 5.7 ae0658fdbad5 12 days ago 449MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB#进入自己[root@VM-0-15-centos docker-test-volume]# docker run -it 7735159af50c /bin/bash#可以看到挂载的两个volume卷[root@d6463f570ce4 /]# ls -ltotal 56lrwxrwxrwx 1 root root 7 May 11 2019 bin -&gt; usr/bindrwxr-xr-x 5 root root 360 Dec 3 15:21 devdrwxr-xr-x 1 root root 4096 Dec 3 15:21 etcdrwxr-xr-x 2 root root 4096 May 11 2019 homelrwxrwxrwx 1 root root 7 May 11 2019 lib -&gt; usr/liblrwxrwxrwx 1 root root 9 May 11 2019 lib64 -&gt; usr/lib64drwx------ 2 root root 4096 Aug 9 21:40 lost+founddrwxr-xr-x 2 root root 4096 May 11 2019 mediadrwxr-xr-x 2 root root 4096 May 11 2019 mntdrwxr-xr-x 2 root root 4096 May 11 2019 optdr-xr-xr-x 91 root root 0 Dec 3 15:21 procdr-xr-x--- 2 root root 4096 Aug 9 21:40 rootdrwxr-xr-x 11 root root 4096 Aug 9 21:40 runlrwxrwxrwx 1 root root 8 May 11 2019 sbin -&gt; usr/sbindrwxr-xr-x 2 root root 4096 May 11 2019 srvdr-xr-xr-x 13 root root 0 Dec 3 15:21 sysdrwxrwxrwt 7 root root 4096 Aug 9 21:40 tmpdrwxr-xr-x 12 root root 4096 Aug 9 21:40 usrdrwxr-xr-x 20 root root 4096 Aug 9 21:40 vardrwxr-xr-x 2 root root 4096 Dec 3 15:21 volume01drwxr-xr-x 2 root root 4096 Dec 3 15:21 volume02#这个卷Volume一定和外部有一个同步的目录[root@d6463f570ce4 volume01]# touch container.txt[root@d6463f570ce4 volume01]# lscontainer.txt[root@d6463f570ce4 volume01]# [root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd6463f570ce4 7735159af50c \"/bin/bash\" 7 minutes ago Up 7 minutes zen_faraday[root@VM-0-15-centos docker-test-volume]# docker inspect d6463f570ce4[ &#123; \"Id\": \"d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881\", \"Created\": \"2020-12-03T15:21:34.913373526Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 15638, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T15:21:35.202804066Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24d\", \"ResolvConfPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hostname\", \"HostsPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hosts\", \"LogPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881-json.log\", \"Name\": \"/zen_faraday\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"volume\", \"Name\": \"4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a\", \"Source\": \"/var/lib/docker/volumes/4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a/_data\", \"Destination\": \"volume01\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125;, &#123; \"Type\": \"volume\", \"Name\": \"0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1\", \"Source\": \"/var/lib/docker/volumes/0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1/_data\", \"Destination\": \"volume02\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125; ], \"Config\": &#123; \"Hostname\": \"d6463f570ce4\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"7735159af50c\", \"Volumes\": &#123; \"volume01\": &#123;&#125;, \"volume02\": &#123;&#125; &#125;, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"cf151107d8fa56fd220695eed265b431618350222be583761b13354867477e39\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/cf151107d8fa\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;] 数据卷容器不同容器之间同步数据 #docker01已经启动了[root@VM-0-15-centos _data]# docker run -it --name docker02 --volumes-from docker01 rxk/centos#--volumes-from类似于 docker02继承docker01#即使docker01被删了，docker02中卷的文件也是存在的，类似硬链接 DockerFiledockerfile是用来构建docker镜像的文件！命令参数脚本！ 构建步骤： 1.编写一个dockerfile文件 2.docker build构建成为一个镜像 3.docker run运行镜像 4.docker push发布镜像(DockerHub、阿里云镜像仓库！) 很多官方镜像都是基础包，很多功能没有，通常会自己搭建自己的镜像。 DockerFile的构建过程基础知识 1.每个保留关键字(指令)都必须是大写字母 2.执行从上到下顺序执行 3.# 表示注释 4.每一个指令都会创建提交一个新的镜像层 并提交 DockerFile:构建文件，定义了一切的步骤，源代码。 DockerImages:通过DockerFile构建生成的镜像，最终发布和运行的产品 Docker容器：容器就是镜像运行起来提供服务的 DockerFile的指令FROM # 基础镜像，一切从这里开始构建MAINTAINER #镜像是谁写的，姓名+邮箱RUN #镜像构建的时候需要运行的命令ADD #步骤：构建基于tomcat的镜像，进行添加！ADD为添加内容WORKDIR #镜像的工作目录VOLUME #挂载的容器卷EXPOSE #指定暴露端口CMD #指定这个容器启动时候要运行的命令，只有最后一个会生效，可被替代ENTRYPOINT #指定这个容器启动的时候要运行的命令，可以追加命令ONBUILD #当构建一个被继承DockerFile， 这个时候就会运行ONBUILD命令。触发指令COPY #类似ADD命令 ，将文件拷贝到镜像中ENV #构建时候设置环境变量 构建自己的centos 构建自己的centos #1.编写dockerfile的文件FROM centosMAINTAINER kuangshen&lt;179049243@qq.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo \"----end----\"CMD /bin/bash#2.通过文件构建镜像# docker build -f dockerfile文件路径 -t 镜像名:[tag].#docker build -f my-dockerfile -t mycentos:0.1 .[root@VM-0-15-centos dockerfile]# docker build -f my-dockerfile -t mycentos:0.1 .Sending build context to Docker daemon 2.048kBStep 1/10 : FROM centos ---&gt; 0d120b6ccaa8Step 2/10 : MAINTAINER kuangshen&lt;179049243@qq.com&gt; ---&gt; Running in 3dd9d64bfbffRemoving intermediate container 3dd9d64bfbff ---&gt; e56a7b5f722bStep 3/10 : ENV MYPATH /usr/local ---&gt; Running in 7707c6ee0441Removing intermediate container 7707c6ee0441 ---&gt; b67dab755631Step 4/10 : WORKDIR $MYPATH ---&gt; Running in 1797c746ab6bRemoving intermediate container 1797c746ab6b ---&gt; 3ed5689097b4Step 5/10 : RUN yum -y install vim ---&gt; Running in 5791c022d132CentOS-8 - AppStream 969 kB/s | 6.2 MB 00:06 CentOS-8 - Base 950 kB/s | 2.3 MB 00:02 CentOS-8 - Extras 11 kB/s | 8.1 kB 00:00 Dependencies resolved.================================================================================ Package Arch Version Repository Size================================================================================Installing: vim-enhanced x86_64 2:8.0.1763-15.el8 AppStream 1.4 MInstalling dependencies: gpm-libs x86_64 1.20.7-15.el8 AppStream 39 k vim-common x86_64 2:8.0.1763-15.el8 AppStream 6.3 M vim-filesystem noarch 2:8.0.1763-15.el8 AppStream 48 k which x86_64 2.21-12.el8 BaseOS 49 kTransaction Summary================================================================================Install 5 PackagesTotal download size: 7.8 MInstalled size: 30 MDownloading Packages:(1/5): gpm-libs-1.20.7-15.el8.x86_64.rpm 327 kB/s | 39 kB 00:00 (2/5): vim-filesystem-8.0.1763-15.el8.noarch.rp 827 kB/s | 48 kB 00:00 (3/5): which-2.21-12.el8.x86_64.rpm 316 kB/s | 49 kB 00:00 (4/5): vim-enhanced-8.0.1763-15.el8.x86_64.rpm 1.2 MB/s | 1.4 MB 00:01 (5/5): vim-common-8.0.1763-15.el8.x86_64.rpm 983 kB/s | 6.3 MB 00:06 --------------------------------------------------------------------------------Total 974 kB/s | 7.8 MB 00:08 warning: /var/cache/dnf/AppStream-02e86d1c976ab532/packages/gpm-libs-1.20.7-15.el8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 8483c65d: NOKEYCentOS-8 - AppStream 1.6 MB/s | 1.6 kB 00:00 Importing GPG key 0x8483C65D: Userid : \"CentOS (CentOS Official Signing Key) &lt;security@centos.org&gt;\" Fingerprint: 99DB 70FA E1D7 CE22 7FB6 4882 05B5 55B3 8483 C65D From : /etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialKey imported successfullyRunning transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : which-2.21-12.el8.x86_64 1/5 Installing : vim-filesystem-2:8.0.1763-15.el8.noarch 2/5 Installing : vim-common-2:8.0.1763-15.el8.x86_64 3/5 Installing : gpm-libs-1.20.7-15.el8.x86_64 4/5 Running scriptlet: gpm-libs-1.20.7-15.el8.x86_64 4/5 Installing : vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-common-2:8.0.1763-15.el8.x86_64 5/5 Verifying : gpm-libs-1.20.7-15.el8.x86_64 1/5 Verifying : vim-common-2:8.0.1763-15.el8.x86_64 2/5 Verifying : vim-enhanced-2:8.0.1763-15.el8.x86_64 3/5 Verifying : vim-filesystem-2:8.0.1763-15.el8.noarch 4/5 Verifying : which-2.21-12.el8.x86_64 5/5 Installed: gpm-libs-1.20.7-15.el8.x86_64 vim-common-2:8.0.1763-15.el8.x86_64 vim-enhanced-2:8.0.1763-15.el8.x86_64 vim-filesystem-2:8.0.1763-15.el8.noarch which-2.21-12.el8.x86_64 Complete!Removing intermediate container 5791c022d132 ---&gt; 05dc9bde286eStep 6/10 : RUN yum -y install net-tools ---&gt; Running in 900fc6c0b10eLast metadata expiration check: 0:00:15 ago on Thu Dec 10 15:07:33 2020.Dependencies resolved.================================================================================ Package Architecture Version Repository Size================================================================================Installing: net-tools x86_64 2.0-0.52.20160912git.el8 BaseOS 322 kTransaction Summary================================================================================Install 1 PackageTotal download size: 322 kInstalled size: 942 kDownloading Packages:net-tools-2.0-0.52.20160912git.el8.x86_64.rpm 969 kB/s | 322 kB 00:00 --------------------------------------------------------------------------------Total 181 kB/s | 322 kB 00:01 Running transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Running scriptlet: net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Verifying : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Installed: net-tools-2.0-0.52.20160912git.el8.x86_64 Complete!Removing intermediate container 900fc6c0b10e ---&gt; 3f8796ecbdeaStep 7/10 : EXPOSE 80 ---&gt; Running in 37e6f3792548Removing intermediate container 37e6f3792548 ---&gt; 2d37cf00b36bStep 8/10 : CMD echo $MYPATH ---&gt; Running in 7e4ff55e4b24Removing intermediate container 7e4ff55e4b24 ---&gt; d74bce644e74Step 9/10 : CMD echo \"----end----\" ---&gt; Running in fc3301142b73Removing intermediate container fc3301142b73 ---&gt; 2c7e307a481cStep 10/10 : CMD /bin/bash ---&gt; Running in 4e5c6edd147cRemoving intermediate container 4e5c6edd147c ---&gt; 071c7fb7d0f9Successfully built 071c7fb7d0f9Successfully tagged mycentos:0.1#3.测试镜像[root@VM-0-15-centos dockerfile]# docker run -it mycentos:0.1[root@5b68abc14a5a local]# pwd/usr/local[root@5b68abc14a5a local]# vimsh: wq: command not foundshell returned 127Press ENTER or type command to continuesh: wq: command not foundshell returned 127Press ENTER or type command to continue 镜像构建历史查看 [root@VM-0-15-centos dockerfile]# docker history 071c7fb7d0f9IMAGE CREATED CREATED BY SIZE COMMENT071c7fb7d0f9 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"/bin… 0B 2c7e307a481c 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B d74bce644e74 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B 2d37cf00b36b 6 minutes ago /bin/sh -c #(nop) EXPOSE 80 0B 3f8796ecbdea 6 minutes ago /bin/sh -c yum -y install net-tools 23.1MB 05dc9bde286e 6 minutes ago /bin/sh -c yum -y install vim 57.7MB 3ed5689097b4 7 minutes ago /bin/sh -c #(nop) WORKDIR /usr/local 0B b67dab755631 7 minutes ago /bin/sh -c #(nop) ENV MYPATH=/usr/local 0B e56a7b5f722b 7 minutes ago /bin/sh -c #(nop) MAINTAINER kuangshen&lt;1790… 0B 0d120b6ccaa8 4 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) ADD file:538afc0c5c964ce0d… 215MB 实战：Dockerfile制作tomcat镜像1、准备镜像文件tomcat压缩包，jdk的压缩包！2、编写dockerfile文件。 Docker0网络详解#删除所有镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: rxk/centos:latestDeleted: sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24dDeleted: sha256:d64085501c38257f9828c1dc9717f26e20df9bcb8d1b958b7f49ea5bb0425ae0Deleted: sha256:02a6fde9ba357c11d75c6d828c8fee80e1e5e77c4aedb3a83c7e302fc1745d40Untagged: tomcat02:1.0Deleted: sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74ebDeleted: sha256:20e9e09986f8d937efa6f2ad283aa9a1b837cc5cc5d4d76e6105f83e38e8f3deUntagged: nginx:latestUntagged: nginx@sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Deleted: sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221cUntagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Untagged: tomcat:9.0Untagged: tomcat:latestUntagged: tomcat@sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaDeleted: sha256:e0bd8b34b4ea904874e55eae50e8987815030d140f9773a4b61759f4f85bf38dUntagged: portainer/portainer:latestUntagged: portainer/portainer@sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Deleted: sha256:62771b0b9b0973a3e8e95595534a1240d8cfd968d30ec82dc0393ce0a256c5f3Untagged: elasticsearch:7.6.2Untagged: elasticsearch@sha256:1b09dbd93085a1e7bca34830e77d2981521a7210e11f11eda997add1c12711faDeleted: sha256:f29a1ee41030e3963026369105f3bee76d75fdecbeca07932ac054126be7bff9Error response from daemon: conflict: unable to delete 071c7fb7d0f9 (cannot be forced) - image is being used by running container 5b68abc14a5aError response from daemon: conflict: unable to delete 3f8796ecbdea (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete d74bce644e74 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2d37cf00b36b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2c7e307a481c (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 05dc9bde286e (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 3ed5689097b4 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete e56a7b5f722b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete b67dab755631 (cannot be forced) - image has dependent child imagesError: No such image: 02a6fde9ba35Error: No such image: d64085501c38Error: No such image: e0bd8b34b4eaError response from daemon: conflict: unable to delete 0d120b6ccaa8 (cannot be forced) - image has dependent child images 三个网络 #启动一个tomcat[root@VM-0-15-centos ~]# docker run -d -P --name tomcat02 tomcat942539fdda0c018ffa3750661373e624c4f2226f5d9532323d5c42c1daea98db[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES942539fdda0c tomcat \"catalina.sh run\" 3 seconds ago Up 3 seconds 0.0.0.0:32768-&gt;8080/tcp tomcat02#查看容器的内部网络地址[root@VM-0-15-centos ~]# docker exec -it tomcat02 ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever112: eth0@if113: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever linux可以ping通docker容器内部 原理 1.我们每安装一个docker容器，docker就会给docker容器分配一个ip，我们只要安装了docker，就会有一个网卡，docker0，使用桥接模式，使用技术是veth-pair技术！ 再次测试ip addr: 2.再启动一个容器测试，又会多一对网卡，这样一对对的网卡，其实是veth-pair技术，就是一对的虚拟设备，他们都是成对出现，一段连着协议，一段彼此相连，veth-pair充当一个桥梁，链接各种虚拟网络设备 3.测试下tomcat02和tomcat03能否ping通，确实可以ping通 容器和容器之间是可以互相ping通的 [root@VM-0-15-centos ~]# docker exec -it tomcat03 ping 172.18.0.2PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.076 ms64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.054 ms64 bytes from 172.18.0.2: icmp_seq=3 ttl=64 time=0.053 ms^C--- 172.18.0.2 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.053/0.061/0.076/0.010 ms 结论：tomcat02和tomcat03共用一个路由器，docker0 所有容器不指定网络的情况下，都是由docker0路由的，docker会给我们容器分配一个默认的可用IP 当把docker容器删除之后，ip addr之后 容器ip就没了 [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbe137857cf4e tomcat \"catalina.sh run\" 29 minutes ago Up 29 minutes 0.0.0.0:32769-&gt;8080/tcp tomcat03942539fdda0c tomcat \"catalina.sh run\" 37 minutes ago Up 37 minutes 0.0.0.0:32768-&gt;8080/tcp tomcat02[root@VM-0-15-centos ~]# docker rm -f be137857cf4ebe137857cf4e[root@VM-0-15-centos ~]# docker rm -f 942539fdda0c942539fdda0c[root@VM-0-15-centos ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:83:af:57 brd ff:ff:ff:ff:ff:ff inet 172.17.0.15/20 brd 172.17.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe83:af57/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:18:37:f9:06 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:18ff:fe37:f906/64 scope link valid_lft forever preferred_lft forever Docker –link 容器互联使用服务名去ping通tomcat docker run -d -P tomcat03 --link tomcat02 tomcatdocker exec -it tomcat03 ping tomcat02#此时3能ping 通2，但是2 ping不通3 Docker自定义网络查看docker所有网络 [root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host local00b73cedd526 none null local 网络模式 bridge:桥接 bridge(默认) none:不配置网络 host:和宿主机共享网络 container:容器内网络连通(使用较少) 创建自己的自定义网络 –driver bridge 默认桥接 –subnet 192.168.0.0/16 子网地址 –gateway 192.168.0.1 网关地址 [root@VM-0-15-centos ~]# docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet[root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host localf5100a69756f mynet bridge local00b73cedd526 none null local[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123;&#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#启动两个容器[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-01 --network mynet tomcat8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-02 --network mynet tomcat01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a\": &#123; \"Name\": \"tomcat-net-02\", \"EndpointID\": \"25be3c90c1e1a4b74f5fc9e10a59c50029ca7ff3bf692d7de636738edb3457b3\", \"MacAddress\": \"02:42:c0:a8:00:03\", \"IPv4Address\": \"192.168.0.3/16\", \"IPv6Address\": \"\" &#125;, \"8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33\": &#123; \"Name\": \"tomcat-net-01\", \"EndpointID\": \"a83c09df1a4595735453dafdde352123dd024707400716fbab256c7e3c02ea4a\", \"MacAddress\": \"02:42:c0:a8:00:02\", \"IPv4Address\": \"192.168.0.2/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#再次测试，不再使用--link 也是可以连接通的，不管使用ip还是服务名[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping 192.168.0.3PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.085 ms64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.055 ms64 bytes from 192.168.0.3: icmp_seq=3 ttl=64 time=0.053 ms^C--- 192.168.0.3 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.053/0.064/0.085/0.016 ms[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping tomcat-net-02PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.050 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.052 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.053 ms^C--- tomcat-net-02 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 0.050/0.051/0.053/0.008 ms 我们自定义的网络docker 都已经帮我们维护好了对应的关系，推荐这样使用网络！ 好处：不同的集群使用不同的网络，保证集群是安全和健康 网络连通把一个容器连接到网络上 #将tomcat01连接到mynet网络下，一个容器两个ip docker network connect mynet tomcat01 Redis集群实战通过脚本创建六个redis配置 for port in $(seq 1 6); \\do \\mkdir -p /mydata/redis/node-$&#123;port&#125;/conftouch /mydata/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;/mydata/redis/node-$&#123;port&#125;/conf/redis.confport 6379bind 0.0.0.0cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.38.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379daemonize noappendonly yesEOFdone 启动redis docker run -p 6371:6379 -p 16371:16379 --name redis-1 -v /mydata/redis/node-1/data:/data -v /mydata/redis/node-1/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.11 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6372:6379 -p 16372:16379 --name redis-2 -v /mydata/redis/node-2/data:/data -v /mydata/redis/node-2/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.12 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6373:6379 -p 16373:16379 --name redis-3 -v /mydata/redis/node-3/data:/data -v /mydata/redis/node-3/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.13 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6374:6379 -p 16374:16379 --name redis-4 -v /mydata/redis/node-4/data:/data -v /mydata/redis/node-4/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.14 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6375:6379 -p 16375:16379 --name redis-5 -v /mydata/redis/node-5/data:/data -v /mydata/redis/node-5/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.15 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6376:6379 -p 16376:16379 --name redis-6 -v /mydata/redis/node-6/data:/data -v /mydata/redis/node-6/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.16 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.conf#正常启动之后[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe1535aac14ce redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 22 seconds ago Up 22 seconds 0.0.0.0:6376-&gt;6379/tcp, 0.0.0.0:16376-&gt;16379/tcp redis-6fcf18ac7033d redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 31 seconds ago Up 30 seconds 0.0.0.0:6375-&gt;6379/tcp, 0.0.0.0:16375-&gt;16379/tcp redis-5b17f66d41588 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 37 seconds ago Up 37 seconds 0.0.0.0:6374-&gt;6379/tcp, 0.0.0.0:16374-&gt;16379/tcp redis-4afb2f4e6484e redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 44 seconds ago Up 44 seconds 0.0.0.0:6373-&gt;6379/tcp, 0.0.0.0:16373-&gt;16379/tcp redis-3ef59db5b0cc0 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 54 seconds ago Up 53 seconds 0.0.0.0:6372-&gt;6379/tcp, 0.0.0.0:16372-&gt;16379/tcp redis-20eca360704d7 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" About a minute ago Up About a minute 0.0.0.0:6371-&gt;6379/tcp, 0.0.0.0:16371-&gt;16379/tcp redis-1#进入redis-1[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # #创建redis集群[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.38.0.15:6379 to 172.38.0.11:6379Adding replica 172.38.0.16:6379 to 172.38.0.12:6379Adding replica 172.38.0.14:6379 to 172.38.0.13:6379M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) masterM: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) masterM: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) masterS: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 replicates f70e64b741bad779e2b545b1429b47b2732a9a13S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84dCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 172.38.0.11:6379)M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 slots: (0 slots) slave replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 slots: (0 slots) slave replicates f70e64b741bad779e2b545b1429b47b2732a9a13M: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 slots: (0 slots) slave replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered./data # redis-cli -c127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:149cluster_stats_messages_pong_sent:140cluster_stats_messages_sent:289cluster_stats_messages_ping_received:135cluster_stats_messages_pong_received:149cluster_stats_messages_meet_received:5cluster_stats_messages_received:289127.0.0.1:6379&gt; cluster nodesde19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379@16379 master - 0 1608624786213 2 conn61-1092213b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379@16379 slave 43b45750adfaea90314c724ddf6b40bf 0 1608624785000 4 connected03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379@16379 slave f70e64b741bad779e2b545b1432a9a13 0 1608624785000 5 connected43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379@16379 master - 0 1608624785210 3 conn923-16383b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379@16379 slave de19aadcb9d25a77ca0e5a23f6ece84d 0 1608624785712 6 connectedf70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379@16379 myself,master - 0 1608624785000cted 0-5460127.0.0.1:6379&gt; set a b-&gt; Redirected to slot [15495] located at 172.38.0.13:6379OK172.38.0.13:6379&gt; get a^C#即使把redis-3 stop了，仍然可以通过其他slaver找到key a所对应的值b，主从配置成功/data # redis-cli -c127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.38.0.14:6379\"b\"172.38.0.14:6379&gt; 搭建redis集群完成","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"}],"author":"CinKate"},{"title":"《A Comparative Study of Word Embeddings for Reading Comprehension》论文阅读","slug":"WordEmbeddingsforReadingComprehension","date":"2019-05-11T19:43:34.000Z","updated":"2020-05-17T16:12:00.854Z","comments":true,"path":"2019/05/12/WordEmbeddingsforReadingComprehension/","link":"","permalink":"http://renxingkai.github.io/2019/05/12/WordEmbeddingsforReadingComprehension/","excerpt":"","text":"众所周知，预训练好的词向量有不同的维度，比如预训练好的GloVe词向量有从50-300维等的词向量表示，但这些不同维度的表示有什么区别，以及在什么时候该用什么维度的词向量（虽然各论文中大家大多用了300维的词向量），这些问题我也确实不太清除。这篇论文解答了我的这些困惑，写的还是很精彩的。 论文原链接 Let’s have a look: 这篇论文主要解决了两个问题点： - 在阅读理解任务中应该使用什么样的预训练词向量 - 测试阶段对于OOV词应怎样处理 所用的数据集和模型数据集： Who-Did-What(WDW) 完型填空类的数据集，从新闻故事中构建 Children’s Book Test(CBT) 从children’s book构建，此论文中仅用了CBT-NE，即数据集中答案为命名实体的数据 模型：Stanford AR、GA Reader 词向量对比： GloVe (50-300) word2vec (300) 实验和结果 词向量的对比 第一个结果：使用在合适的语料库上训练的词向量可以比随机初始化提高3-6％。然而，用于预训练的语料库和方法是重要的选择：例如，在CBT上训练的word2vec词向量比随机词向量执行效果更差。 另请注意，在每种情况下，GloVe词向量都优于在同一语料库中训练的word2vec嵌入。 但是由于词向量对训练参数很敏感，并不能说GloVe一定比word2vec好，但确实从各个论文中也可以看出，一般会优先选择GloVe。 第二个结果：对比GloVe不同维度对实验结果的影响(50-300)：随着词向量维度的增加，实验结果性能是下降的。但是即使使用了300维的GloVe词向量，实验结果仍然比word2vec词向量效果好。 第三个结果：在使用原始语料进行训练时，要先将停用词去掉，与停用词的共现提供关于特定单词的语义的很少有意义的信息，因此具有高百分比的语料库可能不会产生高质量的向量。训练词向量时，超参数调节很重要。 处理OOV词 第一个结果：一般对于OOV词的处理都是赋予一个固定大小不变的词向量（UNK）。这种方法忽略了这样一个事实，即分配为UNK的许多单词可能已经训练过VG中可用的词向量。实验中在测试时，任何新token将被分配其GloVe向量（如果存在）或UNK的向量。 第二个结果，不是为所有OOV词分配一个共同的UNK向量，而是为它们分配未经训练但唯一的随机向量可能更好。此方法在训练时访问测试集词表是没必要的。 总结作者已经证明，用于初始化单词向量的预训练词向量的选择对于阅读理解的神经网络模型的性能具有显著影响。在测试时处理OOV词的方法也是如此。 根据作者的实验，我们建议使用现成的GloVe词向量，并在测试时将预先训练的GloVe向量（如果可用）或随机但唯一的向量分配给OOV词。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"FastQA学习","slug":"fastqa","date":"2019-04-21T10:52:36.000Z","updated":"2020-05-17T16:11:59.241Z","comments":true,"path":"2019/04/21/fastqa/","link":"","permalink":"http://renxingkai.github.io/2019/04/21/fastqa/","excerpt":"","text":"现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 emedding-, encoding-, interaction-, answer-layer ），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层，比如BIDAF、R-Net等等，大量的工作都在问题和文章的交互query-aware表示上创新，类似人类做阅读理解问题的思路“重复多读文章”，“带着问题读文章”等等，普通的“阅读理解思路”也都被实现了，这篇论文作者发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans: 与 question 对应的 answer type 匹配，比如说问 when 就回答 time； 与重要的 question words 位置上临近； 添加问题单词是否出现在文章中这一“重要”特征；并没有使用复杂的question与context的交互，就取得了在SQuAD榜上与SOTA接近的结果，这篇论文之后，后来的研究者们在做MRC时也会将基础特征加入到embedding中进行共同训练，开源链接。 以下是阅读源码的一些总结： 1.Highway Network的使用Highway Network主要解决的问题是，网络深度加深，梯度信息回流受阻造成网络训练困难的问题。 当网络加深，训练的误差反而上升了，而加入了Highway Network之后，这个问题得到了缓解。一般来说，深度网络训练困难是由于梯度回流受阻的问题，可能浅层网络没有办法得到调整。Highway Network 受LSTM启发，增加了一个门函数，让网络的输出由两部分组成，分别是网络的直接输入以及输入变形后的部分。 网络中把此层放在embedding层后面 import tensorflow as tffrom keras import backend as Kfrom keras.engine.topology import Layerfrom keras.layers import Lambda, Wrapperclass Highway(Layer): def __init__(self, hidden_size, **kwargs): self.hidden_size = hidden_size super().__init__(**kwargs) def build(self, input_shape): self.projection = self.add_weight(name=&apos;projection&apos;, shape=(1, input_shape[-1], self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.W_h = self.add_weight(name=&apos;W_h&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_h = self.add_weight(name=&apos;b_h&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) self.W_t = self.add_weight(name=&apos;W_t&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_t = self.add_weight(name=&apos;b_t&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) def call(self, x): x = K.conv1d(x, self.projection) H = tf.nn.tanh(K.bias_add(K.conv1d(x, self.W_h), self.b_h)) T = tf.nn.sigmoid(K.bias_add(K.conv1d(x, self.W_t), self.b_t)) return T * x + (1 - T) * H def compute_output_shape(self, input_shape): batch, seq_len, d = input_shape return (batch, seq_len, self.hidden_size) 2.tf.sequence_mask的学习这个操作和one hot也很像，但是指定的不是index而是从前到后有多少个True，返回的是True和False。 sq_mask = tf.sequence_mask([1, 3, 2], 5)print(sess.run(sq_mask)) 输出： [[True, False, False, False, False],[True, True, True, False, False],[True, True, False, False, False]] 3.tf.expand_dims()学习TensorFlow中，想要维度增加一维，可以使用 tf.expand_dims(input, dim, name=None) 函数。当然，我们常用tf.reshape(input,shape=[])也可以达到相同效果，但是有些时候在构建图的过程中，placeholder没有被feed具体的值，这时就会包下面的错误：TypeError: Expected binary or unicode string, got 1 在这种情况下，我们就可以考虑使用expand_dims来将维度加1。比如我自己代码中遇到的情况，在对图像维度降到二维做特定操作后，要还原成四维[batch, height, width, channels]，前后各增加一维。如果用reshape，则因为上述原因报错 给出官方的例子： # &apos;t&apos; is a tensor of shape [2]shape(expand_dims(t, 0)) ==&gt; [1, 2]shape(expand_dims(t, 1)) ==&gt; [2, 1]shape(expand_dims(t, -1)) ==&gt; [2, 1]# &apos;t2&apos; is a tensor of shape [2, 3, 5]shape(expand_dims(t2, 0)) ==&gt; [1, 2, 3, 5]shape(expand_dims(t2, 2)) ==&gt; [2, 3, 1, 5]shape(expand_dims(t2, 3)) ==&gt; [2, 3, 5, 1] Args: input: A Tensor. dim: A Tensor. Must be one of the following types: int32, int64. 0-D (scalar). Specifies the dimension index at which to expand the shape of input. name: A name for the operation (optional).Returns: A Tensor. Has the same type as input. Contains the same data as input, but its shape has an additional dimension of size 1 added. 4.tf.tile()学习推荐博客tf.tile( input, #输入 multiples, #某一维度上复制的次数 name=None ) import tensorflow as tfa = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)a1 = tf.tile(a, [2, 3])a2 = tf.tile(a, [1, 2])with tf.Session() as sess: print(sess.run(a)) print(sess.run(a1)) print(sess.run(a2)) 输出： [[1. 2.] [3. 4.] [5. 6.]] [[1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.] [1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.]][[1. 2. 1. 2.] [3. 4. 3. 4.] [5. 6. 5. 6.]] 5.tf.equal()学习 equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，而是逐个元素进行判断，如果相等就是 True，不相等，就是 False。 由于是逐个元素判断，所以 x，y 的维度要一致。 例子： import tensorflow as tfa = [[1,2,3],[4,5,6]]b = [[1,0,3],[1,5,1]]with tf.Session() as sess: print(sess.run(tf.equal(a,b))) 输出： [[ True False True] [False True False]] 6.tf.reduce_any()学习在boolean张量的维度上计算元素的 “逻辑或” x = tf.constant([[True, True], [False, False]])with tf.Session() as sess: print(tf.reduce_any(x)) # True print(tf.reduce_any(x, 0)) # [True, True] print(tf.reduce_any(x, 1)) # [True, False] 7.tf.squeeze()学习该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果axis可以用来指定要删掉的为1的维度，此处要注意指定的维度必须确保其是1，否则会报错squeeze( input, axis=None, name=None, squeeze_dims=None) 例子：# &apos;t&apos; 是一个维度是[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t)) # [2, 3]， 默认删除所有为1的维度# &apos;t&apos; 是一个维度[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t, [2, 4])) # [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1 8.RepeatVector层RepeatVector层将输入重复n次keras.layers.core.RepeatVector(n) 参数 n：整数，重复的次数 输入shape形如（nb_samples, features）的2D张量 输出shape形如（nb_samples, n, features）的3D张量 例子 model = Sequential()model.add(Dense(32, input_dim=32))# now: model.output_shape == (None, 32)# note: `None` is the batch dimensionmodel.add(RepeatVector(3))# now: model.output_shape == (None, 3, 32) 9.tf.gather()学习类似于数组的索引，可以把向量中某些索引值提取出来，得到新的向量，适用于要提取的索引为不连续的情况。这个函数似乎只适合在一维的情况下使用。 import tensorflow as tf a = tf.Variable([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])index_a = tf.Variable([0,2]) b = tf.Variable([1,2,3,4,5,6,7,8,9,10])index_b = tf.Variable([2,4,6,8]) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather(a, index_a))) print(sess.run(tf.gather(b, index_b))) # [[ 1 2 3 4 5]# [11 12 13 14 15]] # [3 5 7 9] tf.gather_nd同上，但允许在多维上进行索引，例子只展示了一种很简单的用法，","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"}],"author":"CinKate"},{"title":"基于NLTK的TF-IDF关键词抽取","slug":"tfidfkeyextraction","date":"2019-04-10T14:46:21.000Z","updated":"2020-05-17T16:12:00.477Z","comments":true,"path":"2019/04/10/tfidfkeyextraction/","link":"","permalink":"http://renxingkai.github.io/2019/04/10/tfidfkeyextraction/","excerpt":"","text":"基于nltk总结了用TF-IDF提取关键词的方法，同时总结了文本标准化（预处理），SVD分解、基于TF-IDF、词频等的关键词抽取 SVD奇异值分解from scipy.sparse.linalg import svdsimport reimport nltkimport unicodedatadef low_rank_svd(matrix,singular_count=2): u,s,vt=svds(matrix,k=singular_count) return u,s,vt 删除换行,进行分句def parse_document(document): document=re.sub(&apos;\\n&apos;,&apos; &apos;,document) if isinstance(document,str): document=document elif isinstance(document,unicode): return unicodedata.normalize(&apos;NFKD&apos;,document).encode(&apos;ascii&apos;,&apos;ignore&apos;) else: raise ValueError(&apos;Document is not string or unicode!&apos;) document=document.strip() sentences=nltk.sent_tokenize(document) sentences=[sentence.strip() for sentence in sentences] return sentences 转移HTML标签from html.parser import HTMLParser html_parser=HTMLParser()def unescape_html(parser,text): return parser.unescape_html(text) 缩写词表CONTRACTION_MAP = &#123;&quot;ain&apos;t&quot;: &quot;is not&quot;,&quot;aren&apos;t&quot;: &quot;are not&quot;,&quot;can&apos;t&quot;: &quot;cannot&quot;,&quot;can&apos;t&apos;ve&quot;: &quot;cannot have&quot;,&quot;&apos;cause&quot;: &quot;because&quot;,&quot;could&apos;ve&quot;: &quot;could have&quot;,&quot;couldn&apos;t&quot;: &quot;could not&quot;,&quot;couldn&apos;t&apos;ve&quot;: &quot;could not have&quot;,&quot;didn&apos;t&quot;: &quot;did not&quot;,&quot;doesn&apos;t&quot;: &quot;does not&quot;,&quot;don&apos;t&quot;: &quot;do not&quot;,&quot;hadn&apos;t&quot;: &quot;had not&quot;,&quot;hadn&apos;t&apos;ve&quot;: &quot;had not have&quot;,&quot;hasn&apos;t&quot;: &quot;has not&quot;,&quot;haven&apos;t&quot;: &quot;have not&quot;,&quot;he&apos;d&quot;: &quot;he would&quot;,&quot;he&apos;d&apos;ve&quot;: &quot;he would have&quot;,&quot;he&apos;ll&quot;: &quot;he will&quot;,&quot;he&apos;ll&apos;ve&quot;: &quot;he he will have&quot;,&quot;he&apos;s&quot;: &quot;he is&quot;,&quot;how&apos;d&quot;: &quot;how did&quot;,&quot;how&apos;d&apos;y&quot;: &quot;how do you&quot;,&quot;how&apos;ll&quot;: &quot;how will&quot;,&quot;how&apos;s&quot;: &quot;how is&quot;,&quot;I&apos;d&quot;: &quot;I would&quot;,&quot;I&apos;d&apos;ve&quot;: &quot;I would have&quot;,&quot;I&apos;ll&quot;: &quot;I will&quot;,&quot;I&apos;ll&apos;ve&quot;: &quot;I will have&quot;,&quot;I&apos;m&quot;: &quot;I am&quot;,&quot;I&apos;ve&quot;: &quot;I have&quot;,&quot;i&apos;d&quot;: &quot;i would&quot;,&quot;i&apos;d&apos;ve&quot;: &quot;i would have&quot;,&quot;i&apos;ll&quot;: &quot;i will&quot;,&quot;i&apos;ll&apos;ve&quot;: &quot;i will have&quot;,&quot;i&apos;m&quot;: &quot;i am&quot;,&quot;i&apos;ve&quot;: &quot;i have&quot;,&quot;isn&apos;t&quot;: &quot;is not&quot;,&quot;it&apos;d&quot;: &quot;it would&quot;,&quot;it&apos;d&apos;ve&quot;: &quot;it would have&quot;,&quot;it&apos;ll&quot;: &quot;it will&quot;,&quot;it&apos;ll&apos;ve&quot;: &quot;it will have&quot;,&quot;it&apos;s&quot;: &quot;it is&quot;,&quot;let&apos;s&quot;: &quot;let us&quot;,&quot;ma&apos;am&quot;: &quot;madam&quot;,&quot;mayn&apos;t&quot;: &quot;may not&quot;,&quot;might&apos;ve&quot;: &quot;might have&quot;,&quot;mightn&apos;t&quot;: &quot;might not&quot;,&quot;mightn&apos;t&apos;ve&quot;: &quot;might not have&quot;,&quot;must&apos;ve&quot;: &quot;must have&quot;,&quot;mustn&apos;t&quot;: &quot;must not&quot;,&quot;mustn&apos;t&apos;ve&quot;: &quot;must not have&quot;,&quot;needn&apos;t&quot;: &quot;need not&quot;,&quot;needn&apos;t&apos;ve&quot;: &quot;need not have&quot;,&quot;o&apos;clock&quot;: &quot;of the clock&quot;,&quot;oughtn&apos;t&quot;: &quot;ought not&quot;,&quot;oughtn&apos;t&apos;ve&quot;: &quot;ought not have&quot;,&quot;shan&apos;t&quot;: &quot;shall not&quot;,&quot;sha&apos;n&apos;t&quot;: &quot;shall not&quot;,&quot;shan&apos;t&apos;ve&quot;: &quot;shall not have&quot;,&quot;she&apos;d&quot;: &quot;she would&quot;,&quot;she&apos;d&apos;ve&quot;: &quot;she would have&quot;,&quot;she&apos;ll&quot;: &quot;she will&quot;,&quot;she&apos;ll&apos;ve&quot;: &quot;she will have&quot;,&quot;she&apos;s&quot;: &quot;she is&quot;,&quot;should&apos;ve&quot;: &quot;should have&quot;,&quot;shouldn&apos;t&quot;: &quot;should not&quot;,&quot;shouldn&apos;t&apos;ve&quot;: &quot;should not have&quot;,&quot;so&apos;ve&quot;: &quot;so have&quot;,&quot;so&apos;s&quot;: &quot;so as&quot;,&quot;that&apos;d&quot;: &quot;that would&quot;,&quot;that&apos;d&apos;ve&quot;: &quot;that would have&quot;,&quot;that&apos;s&quot;: &quot;that is&quot;,&quot;there&apos;d&quot;: &quot;there would&quot;,&quot;there&apos;d&apos;ve&quot;: &quot;there would have&quot;,&quot;there&apos;s&quot;: &quot;there is&quot;,&quot;they&apos;d&quot;: &quot;they would&quot;,&quot;they&apos;d&apos;ve&quot;: &quot;they would have&quot;,&quot;they&apos;ll&quot;: &quot;they will&quot;,&quot;they&apos;ll&apos;ve&quot;: &quot;they will have&quot;,&quot;they&apos;re&quot;: &quot;they are&quot;,&quot;they&apos;ve&quot;: &quot;they have&quot;,&quot;to&apos;ve&quot;: &quot;to have&quot;,&quot;wasn&apos;t&quot;: &quot;was not&quot;,&quot;we&apos;d&quot;: &quot;we would&quot;,&quot;we&apos;d&apos;ve&quot;: &quot;we would have&quot;,&quot;we&apos;ll&quot;: &quot;we will&quot;,&quot;we&apos;ll&apos;ve&quot;: &quot;we will have&quot;,&quot;we&apos;re&quot;: &quot;we are&quot;,&quot;we&apos;ve&quot;: &quot;we have&quot;,&quot;weren&apos;t&quot;: &quot;were not&quot;,&quot;what&apos;ll&quot;: &quot;what will&quot;,&quot;what&apos;ll&apos;ve&quot;: &quot;what will have&quot;,&quot;what&apos;re&quot;: &quot;what are&quot;,&quot;what&apos;s&quot;: &quot;what is&quot;,&quot;what&apos;ve&quot;: &quot;what have&quot;,&quot;when&apos;s&quot;: &quot;when is&quot;,&quot;when&apos;ve&quot;: &quot;when have&quot;,&quot;where&apos;d&quot;: &quot;where did&quot;,&quot;where&apos;s&quot;: &quot;where is&quot;,&quot;where&apos;ve&quot;: &quot;where have&quot;,&quot;who&apos;ll&quot;: &quot;who will&quot;,&quot;who&apos;ll&apos;ve&quot;: &quot;who will have&quot;,&quot;who&apos;s&quot;: &quot;who is&quot;,&quot;who&apos;ve&quot;: &quot;who have&quot;,&quot;why&apos;s&quot;: &quot;why is&quot;,&quot;why&apos;ve&quot;: &quot;why have&quot;,&quot;will&apos;ve&quot;: &quot;will have&quot;,&quot;won&apos;t&quot;: &quot;will not&quot;,&quot;won&apos;t&apos;ve&quot;: &quot;will not have&quot;,&quot;would&apos;ve&quot;: &quot;would have&quot;,&quot;wouldn&apos;t&quot;: &quot;would not&quot;,&quot;wouldn&apos;t&apos;ve&quot;: &quot;would not have&quot;,&quot;y&apos;all&quot;: &quot;you all&quot;,&quot;y&apos;all&apos;d&quot;: &quot;you all would&quot;,&quot;y&apos;all&apos;d&apos;ve&quot;: &quot;you all would have&quot;,&quot;y&apos;all&apos;re&quot;: &quot;you all are&quot;,&quot;y&apos;all&apos;ve&quot;: &quot;you all have&quot;,&quot;you&apos;d&quot;: &quot;you would&quot;,&quot;you&apos;d&apos;ve&quot;: &quot;you would have&quot;,&quot;you&apos;ll&quot;: &quot;you will&quot;,&quot;you&apos;ll&apos;ve&quot;: &quot;you will have&quot;,&quot;you&apos;re&quot;: &quot;you are&quot;,&quot;you&apos;ve&quot;: &quot;you have&quot;&#125; 文本标准化import stringfrom nltk.stem import WordNetLemmatizerstopword_list = nltk.corpus.stopwords.words(&apos;english&apos;)wnl = WordNetLemmatizer()html_parser = HTMLParser() 文本分词def tokenize_text(text): tokens = nltk.word_tokenize(text) tokens = [token.strip() for token in tokens] return tokens 扩展缩写def expand_contractions(text, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match if contraction_mapping.get(match) else contraction_mapping.get(match.lower())) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_text = contractions_pattern.sub(expand_match, text) expanded_text = re.sub(&quot;&apos;&quot;, &quot;&quot;, expanded_text) return expanded_text 标记文本词性from pattern.en import tagfrom nltk.corpus import wordnet as wn# 标记文本词性def pos_tag_text(text): def penn_to_wn_tags(pos_tag): if pos_tag.startswith(&apos;J&apos;): return wn.ADJ elif pos_tag.startswith(&apos;V&apos;): return wn.VERB elif pos_tag.startswith(&apos;N&apos;): return wn.NOUN elif pos_tag.startswith(&apos;R&apos;): return wn.ADV else: return None tagged_text = tag(text) tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text] return tagged_lower_text 基于词性标签提取主干词def lemmatize_text(text): pos_tagged_text = pos_tag_text(text) lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text] lemmatized_text = &apos; &apos;.join(lemmatized_tokens) return lemmatized_text 删除特殊字符def remove_special_characters(text): tokens = tokenize_text(text) pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = filter(None, [pattern.sub(&apos; &apos;, token) for token in tokens]) filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 删除停用词def remove_stopwords(text): tokens = tokenize_text(text) filtered_tokens = [token for token in tokens if token not in stopword_list] filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 转移HTML标签def unescape_html(parser, text): return parser.unescape(text) 标准化文本(合并执行上面流程)def normalize_corpus(corpus, lemmatize=True, tokenize=False): normalized_corpus = [] for text in corpus: text = html_parser.unescape(text) text = expand_contractions(text, CONTRACTION_MAP) if lemmatize: text = lemmatize_text(text) else: text = text.lower() text = remove_special_characters(text) text = remove_stopwords(text) if tokenize: text = tokenize_text(text) normalized_corpus.append(text) else: normalized_corpus.append(text) return normalized_corpus 文本特征提取 基于词项次数的二值特征 基于词袋模型的频率特征 TF-IDF权重特征 构建特征矩阵binary、frequency、tfidffrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizerdef build_feature_matrix(documents,feature_type=&apos;frequency&apos;): feature_type=feature_type.lower().strip() if feature_type==&apos;binary&apos;: vectorizer=CountVectorizer(binary=True,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;frequency&apos;: vectorizer=CountVectorizer(binary=False,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;tfidf&apos;: vectorizer=TfidfVectorizer(min_df=1,ngram_range=(1,1)) else: raise Exception(&quot;Wrong feature type entered. Possible values: &apos;binary&apos;, &apos;frequency&apos;, &apos;tfidf&apos;&quot;) feature_matrix=vectorizer.fit_transform(documents).astype(float) return vectorizer,feature_matrix 关键短语提取词项搭配from nltk.corpus import gutenbergimport nltkfrom operator import itemgetteralice = gutenberg.sents(fileids=&apos;carroll-alice.txt&apos;)alice = [&apos; &apos;.join(ts) for ts in alice]norm_alice = normalize_corpus(alice, lemmatize=False) 将语料压缩成1个大的文本串def flatten_corpus(corpus): return &apos; &apos;.join([document.strip() for document in corpus]) 计算n元分词（比较巧妙）def compute_ngrams(sequence,n):# print([sequence[index:] for index in range(n)])# print(list(zip(*[sequence[index:] for index in range(n)]))) #解压时仅按最小元素数组数量进行解压 return zip(*[sequence[index:] for index in range(n)]) 获取n元分词def get_top_ngram(corpus,ngram_val=1,limit=5): corpus=flatten_corpus(corpus) tokens=nltk.word_tokenize(corpus) ngrams=compute_ngrams(tokens,ngram_val) #获取单词频率 ngrams_freq_dist=nltk.FreqDist(ngrams) #排序频率 sorted_ngrams_fd=sorted(ngrams_freq_dist.items(),key=itemgetter(1),reverse=True) sorted_ngrams=sorted_ngrams_fd[0:limit] sorted_ngrams=[(&apos; &apos;.join(text),freq) for text,freq in sorted_ngrams] return sorted_ngrams 输出频率前10的二元分词get_top_ngram(corpus=norm_alice,ngram_val=2,limit=10) 输出结果[(&apos;said alice&apos;, 123), (&apos;mock turtle&apos;, 56), (&apos;march hare&apos;, 31), (&apos;said king&apos;, 29), (&apos;thought alice&apos;, 26), (&apos;white rabbit&apos;, 22), (&apos;said hatter&apos;, 22), (&apos;said mock&apos;, 20), (&apos;said caterpillar&apos;, 18), (&apos;said gryphon&apos;, 18)] 频率前10的三元分词get_top_ngram(corpus=norm_alice,ngram_val=3,limit=10) 输出结果 [(&apos;said mock turtle&apos;, 20), (&apos;said march hare&apos;, 9), (&apos;poor little thing&apos;, 6), (&apos;little golden key&apos;, 5), (&apos;certainly said alice&apos;, 5), (&apos;white kid gloves&apos;, 5), (&apos;march hare said&apos;, 5), (&apos;mock turtle said&apos;, 5), (&apos;know said alice&apos;, 4), (&apos;might well say&apos;, 4)] 频率前10的一元分词get_top_ngram(corpus=norm_alice,ngram_val=1,limit=10) 输出结果[(&apos;said&apos;, 462), (&apos;alice&apos;, 398), (&apos;little&apos;, 128), (&apos;one&apos;, 104), (&apos;know&apos;, 88), (&apos;like&apos;, 85), (&apos;would&apos;, 83), (&apos;went&apos;, 83), (&apos;could&apos;, 77), (&apos;queen&apos;, 75)] 使用nltk的搭配查找器二元词项from nltk.collocations import BigramCollocationFinderfrom nltk.collocations import BigramAssocMeasuresfinder=BigramCollocationFinder.from_documents([item.split() for item in norm_alice])bigram_measures=BigramAssocMeasures()#使用原始频率进行查找finder.nbest(bigram_measures.raw_freq,10) 输出结果 [(&apos;said&apos;, &apos;alice&apos;), (&apos;mock&apos;, &apos;turtle&apos;), (&apos;march&apos;, &apos;hare&apos;), (&apos;said&apos;, &apos;king&apos;), (&apos;thought&apos;, &apos;alice&apos;), (&apos;said&apos;, &apos;hatter&apos;), (&apos;white&apos;, &apos;rabbit&apos;), (&apos;said&apos;, &apos;mock&apos;), (&apos;said&apos;, &apos;caterpillar&apos;), (&apos;said&apos;, &apos;gryphon&apos;)] 二元使用点互信息PMI进行查找搭配finder.nbest(bigram_measures.pmi,10) 三元词组from nltk.collocations import TrigramAssocMeasuresfrom nltk.collocations import TrigramCollocationFinderfinder=TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])trigram_measures=TrigramAssocMeasures()#三元组频率finder.nbest(trigram_measures.raw_freq,10) 输出结果[(&apos;said&apos;, &apos;mock&apos;, &apos;turtle&apos;), (&apos;said&apos;, &apos;march&apos;, &apos;hare&apos;), (&apos;poor&apos;, &apos;little&apos;, &apos;thing&apos;), (&apos;little&apos;, &apos;golden&apos;, &apos;key&apos;), (&apos;march&apos;, &apos;hare&apos;, &apos;said&apos;), (&apos;mock&apos;, &apos;turtle&apos;, &apos;said&apos;), (&apos;white&apos;, &apos;kid&apos;, &apos;gloves&apos;), (&apos;beau&apos;, &apos;ootiful&apos;, &apos;soo&apos;), (&apos;certainly&apos;, &apos;said&apos;, &apos;alice&apos;), (&apos;might&apos;, &apos;well&apos;, &apos;say&apos;)] 三元使用点互信息PMI进行查找搭配finder.nbest(trigram_measures.pmi,10) 输出结果 [(&apos;accustomed&apos;, &apos;usurpation&apos;, &apos;conquest&apos;), (&apos;adjourn&apos;, &apos;immediate&apos;, &apos;adoption&apos;), (&apos;adoption&apos;, &apos;energetic&apos;, &apos;remedies&apos;), (&apos;ancient&apos;, &apos;modern&apos;, &apos;seaography&apos;), (&apos;apple&apos;, &apos;roast&apos;, &apos;turkey&apos;), (&apos;arithmetic&apos;, &apos;ambition&apos;, &apos;distraction&apos;), (&apos;brother&apos;, &apos;latin&apos;, &apos;grammar&apos;), (&apos;canvas&apos;, &apos;bag&apos;, &apos;tied&apos;), (&apos;cherry&apos;, &apos;tart&apos;, &apos;custard&apos;), (&apos;circle&apos;, &apos;exact&apos;, &apos;shape&apos;)] 基于权重标签的短语提取 使用浅层分析提取所有名词短语词块 计算每个词块的TF-IDF权重并返回最大加权短语 toy_text = &quot;&quot;&quot;Elephants are large mammals of the family Elephantidae and the order Proboscidea. Two species are traditionally recognised, the African elephant and the Asian elephant. Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male African elephants are the largest extant terrestrial animals. All elephants have a long trunk used for many purposes, particularly breathing, lifting water and grasping objects. Their incisors grow into tusks, which can serve as weapons and as tools for moving objects and digging. Elephants&apos; large ear flaps help to control their body temperature. Their pillar-like legs can carry their great weight. African elephants have larger ears and concave backs while Asian elephants have smaller ears and convex or level backs. &quot;&quot;&quot; import numpy as npimport itertoolsfrom gensim import corpora, models 基本上，我们有一个已定义的语法模式来分块或提取名词短语。我们在同一模式中定义一个分块器，对于文档中的每个句子，首先用它的POS标签来标注它(因此，不应该对文本进行规范化)，然后构建一个具有名词短语的浅层分析树作为词块和其他全部基于POS标签的单词作为缝隙，缝隙是不属于任何词块的部分。完成此操作后，我们使用tree2conl1tags函数来生成(w, t，c)三元组，它们是的单词、POS标签和IOB格式的词块标签。删除所有词块带有’O ‘标签的标签，因为它们基本上是不属于任何词块的单词或词项。最后，从这些有效的词块中，组合分块的词项，并从每个词块分组中生成短语。 提取文档中的名词短语 v adj adv#提取文档中的名词短语 v adj advdef get_chunks(sentences, grammar = r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;): all_chunks = [] chunker = nltk.chunk.regexp.RegexpParser(grammar) for sentence in sentences: tagged_sents = nltk.pos_tag_sents( [nltk.word_tokenize(sentence)]) chunks = [chunker.parse(tagged_sent) for tagged_sent in tagged_sents] wtc_sents = [nltk.chunk.tree2conlltags(chunk) for chunk in chunks] flattened_chunks = list( itertools.chain.from_iterable( wtc_sent for wtc_sent in wtc_sents) )# print(flattened_chunks)# print(flattened_chunks) valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk in itertools.groupby(flattened_chunks,lambda chunk: chunk != &apos;O&apos;)]# print(&apos;---&apos;*20)# print(valid_chunks_tagged) valid_chunks = [&apos; &apos;.join(word.lower() for word, tag, chunk in wtc_group if word.lower() not in stopword_list) for status, wtc_group in valid_chunks_tagged if status] all_chunks.append(valid_chunks) return all_chunks sentences = parse_document(toy_text) valid_chunks = get_chunks(sentences) 获取TF-IDF关键短语权重def get_tfidf_weighted_keyphrases(sentences, grammar=r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;, top_n=10): valid_chunks = get_chunks(sentences, grammar=grammar) dictionary = corpora.Dictionary(valid_chunks) corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks] tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] weighted_phrases = &#123;dictionary.get(id): round(value,3) for doc in corpus_tfidf for id, value in doc&#125; weighted_phrases = sorted(weighted_phrases.items(), key=itemgetter(1), reverse=True) return weighted_phrases[:top_n] 前两个关键短语get_tfidf_weighted_keyphrases(sentences, top_n=2) 输出结果 [(&apos;elephants large mammals family elephantidae order proboscidea .&apos;, 1.0), (&apos;two species traditionally recognised , african elephant asian elephant .&apos;, 1.0)] 其他语料实验get_tfidf_weighted_keyphrases(alice, top_n=10) 输出结果 [(&quot;[ alice &apos; adventures wonderland lewis carroll 1865 ]&quot;, 1.0), (&apos;chapter .&apos;, 1.0), (&apos;rabbit - hole&apos;, 1.0), (&quot;alice beginning get tired sitting sister bank , nothing : twice peeped book sister reading , pictures conversations , &apos; use book , &apos; thought alice &apos; without pictures conversation ? &apos;&quot;, 1.0), (&apos;considering mind ( well could , hot day made feel sleepy stupid ) , whether pleasure making daisy - chain would worth trouble getting picking daisies , suddenly white rabbit pink eyes ran close .&apos;, 1.0), (&quot;nothing remarkable ; alice think much way hear rabbit say , &apos; oh dear !&quot;, 1.0), (&apos;oh dear !&apos;, 1.0), (&quot;shall late ! &apos;&quot;, 1.0), (&apos;( thought afterwards , occurred ought wondered , time seemed quite natural ) ; rabbit actually took watch waistcoat - pocket , looked , hurried , alice started feet , flashed across mind never seen rabbit either waistcoat - pocket , watch take , burning curiosity , ran across field , fortunately time see pop large rabbit - hole hedge .&apos;, 1.0), (&apos;another moment went alice , never considering world get .&apos;, 1.0)]","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"}],"author":"CinKate"},{"title":"Word2Vec相关(用TFIDF加权词向量)","slug":"word-tfidf","date":"2019-04-05T10:34:06.000Z","updated":"2020-05-17T16:12:00.563Z","comments":true,"path":"2019/04/05/word-tfidf/","link":"","permalink":"http://renxingkai.github.io/2019/04/05/word-tfidf/","excerpt":"","text":"今天是快乐的清明节，而博主还在实验室敲代码，23333这次记录下Word2Vec相关的姿势~ Word2Vec模型直接用开源的gensism库进行词向量训练： import gensimimport nltkimport numpy as np#自制语料CORPUS = [&apos;the sky is blue&apos;,&apos;sky is blue and sky is beautiful&apos;,&apos;the beautiful sky is so blue&apos;,&apos;i love blue cheese&apos;]new_doc = [&apos;loving this blue sky today&apos;] 对语料进行分词 #tokenize corpusTOKENIZED_CORPUS=[nltk.word_tokenize(sentence) for sentence in CORPUS]tokenized_new_doc=[nltk.word_tokenize(sentence) for sentence in new_doc]print(TOKENIZED_CORPUS)print(tokenized_new_doc) 输出 [[&apos;the&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;], [&apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;, &apos;and&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;beautiful&apos;], [&apos;the&apos;, &apos;beautiful&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;so&apos;, &apos;blue&apos;], [&apos;i&apos;, &apos;love&apos;, &apos;blue&apos;, &apos;cheese&apos;]] [[&apos;loving&apos;, &apos;this&apos;, &apos;blue&apos;, &apos;sky&apos;, &apos;today&apos;]] 构建词向量 model=gensim.models.Word2Vec(TOKENIZED_CORPUS,size=10,window=10,min_count=2,sample=1e-3) 平均词向量来表示文档 #num_features表示的文本单词大小def average_word_vectors(words,model,vocabulary,num_features): feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) nwords=0 for word in words: if word in vocabulary: nwords=nwords+1 feature_vector=np.add(feature_vector,model[word]) if nwords: feature_vector=np.divide(feature_vector,nwords) return feature_vectordef averaged_word_vectorizer(corpus,model,num_features): #get the all vocabulary vocabulary=set(model.wv.index2word) features=[average_word_vectors(tokenized_sentence,model,vocabulary,num_features) for tokenized_sentence in corpus] return np.array(features) avg_word_vec_features=averaged_word_vectorizer(TOKENIZED_CORPUS,model=model,num_features=10)print(avg_word_vec_features) 输出array([[-0.00710545, -0.01549264, 0.02188712, -0.00322829, 0.00586532, -0.00687592, 0.00339291, -0.01177494, 0.00265543, -0.00539964], [-0.0157312 , -0.01630003, 0.00551589, 0.00166568, 0.02385859, 0.0085727 , 0.02538068, -0.02266891, 0.02231819, -0.02521743], [-0.0070758 , -0.00578274, 0.01280785, -0.00960104, 0.00821758, -0.00023592, 0.01009926, -0.00624976, 0.00913788, -0.01323305], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.00845861, -0.0247597 ]]) nd_avg_word_vec_features=averaged_word_vectorizer(corpus=tokenized_new_doc,model=model,num_features=10)print(nd_avg_word_vec_features) 输出array([[-0.00968785, -0.02889012, 0.02670473, -0.01596956, 0.00815679, -0.00325876, 0.02226594, -0.01347479, 0.01384218, -0.01042995]]) # TF-IDF加权平均词向量如果直接求平均效果不好的话，或者过于简单的话，可以对词求TFIDF，然后乘以相应的权重 def tfidf_wtd_avg_word_vectors(words,tfidf_vector,tfidf_vocabulary,model,num_features): word_tfidfs=[tfidf_vector[0,tfidf_vocabulary.get(word)] if tfidf_vocabulary.get(word) else 0 for word in words] word_tfidf_map=&#123;word:tfidf_val for word,tfidf_val in zip(words,word_tfidfs)&#125; feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) vocabulary=set(model.wv.index2word) wts=0 for word in words: if word in vocabulary: word_vector=model[word] weighted_word_vector=word_tfidf_map[word]*word_vector wts=wts+word_tfidf_map[word] feature_vector=np.add(feature_vector,weighted_word_vector) if wts: feature_vector=np.divide(feature_vector,wts) return feature_vectordef tfidf_weighted_averaged_word_vectorizer(corpus,tfidf_vectors,tfidf_vocabulary,model,num_features): docs_tfidfs=[(doc,doc_tfidf) for doc,doc_tfidf in zip(corpus,tfidf_vectors)] features=[tfidf_wtd_avg_word_vectors(tokenized_sentence,tfidf,tfidf_vocabulary,model,num_features) for tokenized_sentence,tfidf in docs_tfidfs] return np.array(features) TFIDF预处理from sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerimport pandas as pddef tfidf_transformer(bow_matrix): transformer = TfidfTransformer(norm=&apos;l2&apos;, smooth_idf=True, use_idf=True) tfidf_matrix = transformer.fit_transform(bow_matrix) return transformer, tfidf_matrixdef tfidf_extractor(corpus, ngram_range=(1,1)): vectorizer = TfidfVectorizer(min_df=1, norm=&apos;l2&apos;, smooth_idf=True, use_idf=True, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef bow_extractor(corpus, ngram_range=(1,1)): #min_df为1说明文档中词频最小为1也会被考虑 #ngram_range可以设置(1,3)将建立包括所有unigram、bigram、trigram的向量空间 vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef display_features(features, feature_names): df = pd.DataFrame(data=features, columns=feature_names) print(df) bow_vectorizer, bow_features = bow_extractor(CORPUS)feature_names = bow_vectorizer.get_feature_names()tfidf_trans, tfidf_features = tfidf_transformer(bow_features)tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)display_features(np.round(tdidf_features.todense(), 2), feature_names)nd_tfidf = tfidf_vectorizer.transform(new_doc)display_features(np.round(nd_tfidf.todense(), 2), feature_names) TFIDF加权词向量corpus_tfidf=tfidf_featuresvocab=tfidf_vectorizer.vocabulary_ wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,tfidf_vectors=corpus_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(wt_tfidf_word_vec_features) 输出array([[-0.00728862, -0.01345045, 0.02334223, -0.00258989, 0.00500905, -0.00913428, 0.00057808, -0.01095917, -0.00025702, -0.00165257], [-0.02009719, -0.01936696, 0.0056747 , 0.00887485, 0.02952368, 0.00819392, 0.02715274, -0.0298718 , 0.02297843, -0.0237992 ], [-0.00721121, -0.00258696, 0.01239834, -0.01018197, 0.00795635, -0.00085167, 0.00906817, -0.00469667, 0.00799437, -0.01167674], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.0084586 , -0.0247597 ]]) nd_wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,tfidf_vectors=nd_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(nd_wt_tfidf_word_vec_features) 输出 array([[-0.01223734, -0.02956665, 0.02708268, -0.01397412, 0.01101045, -0.00361711, 0.02421493, -0.01619775, 0.01438254, -0.00899163]])","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"《基于BiDAF多文档重排序的阅读理解模型》论文阅读","slug":"rbidaf","date":"2019-04-02T18:58:21.000Z","updated":"2020-05-17T16:12:00.222Z","comments":true,"path":"2019/04/03/rbidaf/","link":"","permalink":"http://renxingkai.github.io/2019/04/03/rbidaf/","excerpt":"","text":"0 引言目前的机器学习方法主要有两类：抽取式和生成式，抽取式通过给定问题以及相关的文章进行训练,让机器具备阅读的能力，并对提出的新问题,在相关文章中抽取出相应的答案。另一种是生成式,从理论_上来说不受知识的局限,对于问题自动生成答案,但是生成式有时产生的答案答非所问，句式不通,不能很好地体现出人类的思维逻辑以及自然表述的特点。 在本文中，提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。该模型是一-种抽取式的机器阅读理解模型,在BiDAF模型四层网络框架的基础，上添加了ParaRanking层，针对ParaRank-ing,本文提出了多特征融合的ParaRanking 算法，此外本文还在答案预测层,提出了基于先验知识的多答案交叉验证算法,进而对答案进行综合预测。 1 机器阅读理解和相关工作自斯坦福机器阅读理解数据集SQuAD问世以来,经过谷歌、微软、百度、科大讯飞、腾讯、斯坦福大学等在内的众多研究机构的不懈努力,形成了“词向量化-语义编码-语义交互-答案预测”这样一-套四层机器阅读理解模型体系。该体系的主要思想是:首先将自然文本表示为可计算的向量,其次融合问题向量与支撑文档向量来学习到语义交互信息,最后根据交互信息预测答案的位置或逐一输出最大概率的字词来生成答案。 词向量化层(Word-Embedder) 的作用是使用词向量技术将分词后的自然文本转化为稠密、连续且可计算的低维向量。 文本编码层(Encoder)的作用是进行语义加工，向量化层输出的结果是一串独立的词向量序列,而编码层根据这些词向量捕捉词与词的前后语义关系,并把这种语义关系融入词向量中,生成一串互相关联的文本编码序列。 语义交互层（Interaction-Layer)是整个模型体系中最重要的一环。在进入这一层之前，问题与给定支撑文档在大多数情况下是分别独立进行向量转化与语义编码的。当然，在有些模型中，问题词向量序列也会被提前融合到文档向量中。当前大部分研究工作集中在语义交互层的设计上，在这一层，将最终得到混合两者语义的交互向量。此外，交互向量也时常与未交互前的问题编码向量直接拼接，以强调问题语义的重要性。 答案预测层（Answer-Layer)负责根据语义交互向量产出 最终的答案。目前，答案预测模型主要是生成模型与边界模型（边界模型常用的有Pointer Network指出答案所在的开始、结束位置）。答案预测层将答案均视作一串词序列，生成模型逐个预测该序列中每个词应使用给定文档中哪个词进行填充，每次预测均基于之前预测完成的结果。边界模型则相当于一个简化的生成模型，其预先假定问题都可以使用给定文档中的一个连续短语或句子进行回答，因此只需预测答案的起点与终点词的位置即可。目前，边界模型的预测效率与结果均好于生成模型。 以下作者对比了近年来在SQuAD榜上的部分阅读理解模型： Match-LSTM(2016提出， 发表于ICLR 17)Match-LSTM是首个应用于SQuAD数据的端到端机器阅读理解模型，并成功超越原有使用人工特征进行答案抽取的基线模型。该模型的特点是：（1）在文本编码层使用单向LSTM进行语义建模；（2）在语义交互层对支撑文档中的每个词计算该词在问题编码向量上的注意力分配向量，将这一注意力分配向量与问题编码向量点乘获得文档词–问题交互向量，并再拼接上文档词编码向量，最后用一个新的单向LSTM网络对拼接后的向量进行二次语义编码；（3)用反向LSTM重复（1)、（2)操作，并将正反向二次语义编码向量拼接。 BIDAF(2016提出， 发表于ICLR 17)BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。 R-Net(发表于ACL 17)R-Net是对Match-LSTM匹配模型的改进。这一模型最大的特点是采用了双语义交互层设计。在一级语义交互层，R-Net仿照Match-LSTM实现将问题信息融入到每个文档词中去；而在二级语义交互层，R-Net则使用相同办法将已经获得的文档词–问题语义编码向量再度与问题编码向量二次融合，进一步加强语义匹配。 QANet(发表于ICLR 18)QANet则是一种在BIDAF模型基础上为追求效率而设计的模型。该模型非常创新地在文本编码层使用CNN与Multi-Head Self-Attention机制实现语义编码，由于CNN可以捕捉局部特征、Self-Attention能够捕捉全局特征，因此完全可以用它们替代传统的LSTM网络。此外，由于CNN的建模效率显著高于LSTM网络，该模型以在更大规模的数据集上进行深度学习——泛化能力得到了进一步提升。这一模型可以在SQuAD数据集上达到训练速度提高3〜13倍！推理速度提高4~9倍，且获得与先前基于LSTM网络媲美的精度。 V-net(百度公司发表于ACL 18)V-net是一种新的多文档校验深度神经网络建模方法，该模型通过注意力使不同候选文档抽取的答案能够互相印证，从而预测出更好的答案。 2 数据探索和数据处理百度数据集与其他数据集很大的区别在于，每篇文章中包含了很多个段落，而SQuAD数据集的支撑文档直接是一个最相关段落，微软数据集MS MARCO则是若干篇只有一个段落的文章。因此，在百度机器阅读理解任务中，需要在主流四层体系的基础上，增加一个段落定位层。 在DuReader原文中提到，使用recall指标增加的段落定位层，并使用recall指标进行段落选择，可以使模型的效果至少10% 3 RBiDAF模型设计与实现本文提出的基于BiDAF模型的RBiDAF模型，主要是在BiDAF模型的基础上添加了ParaRanking，在该层提出了ParaRanking算法，从而对候选段落进行排序（ParaRanking)操作，进而筛选出含答案概率更高的候选段落。 此外在答案预测层，提出了基于先验知识的多答案交叉验证（MACVerify)算法，从而对答案进行综合预测。 3.1 ParaRanking算法DuReader数据集中，每一个问题对应多个段落,尤其是在Search数据集中，问题和段落的比接近1:57,所以应该尽量检索出含有答案的段落,从而减小候选段落集的数据规模。在这里本文提出了多特征融合的ParaRanking算法,图8是ParaRanking算法的大体架构,主要包括段落过滤、段落重组、语义匹配、最大覆盖度、特征加权以及多文档投票。 3.1.1 段落过滤 本文利用特征工程根据问题类型对不相关段落进行过滤,例如,实体类型的问题中,问题中的关键词是“联系方式”、“热线”，那么本文利用正则表达式将不含电话号码的段落进行过滤，最终本文设计了23条规则对段落进行初步过滤。 3.1.2段落重组 DuReader数据集中的段落长度极度不平衡,有些段落的长度很短,这种情况会造成段落的上下文信息缺失，不利于模型的Match操作。而且本文通过观察训练集中答案的分布，发现有些答案是跨段落的，尤其是描述类的问题，所以如果仅仅以某-一个原始段落作为预测的输人，那么将无法解决答案跨段落的问题,因此本文将原始的段落进行重组，重组后长度控制在长度splice_ L之内。 3.1.3语义匹配 问题(question)与段落(paragraph)间的匹配不仅要考虑问题和段落之间的显式关系,还要考虑两者之间的隐式关系，即两者之间的语义关系。例如，question:北京2017年的商业住房的均价是多少?paragraph:据我所知是四万元一平。上例question和paragraph之间的最大覆盖度虽然为0,但是两者之间具有极大的语义相关性,并且“四万元一平”极有可能是答案。所以为了克服字词匹配上的弊端，本文选择利用深度神经网络计算question和para-graph之间的语义相关性。 由于ARC-II保留了词序信息,更具一般性，所以本文采用ARC-II文本匹配模型对question以及paragraph之间的语义相关度进行计算,在第一层中,首先把卷积窗口设定为k1,然后对句子Squestion和句子Sprangraph中所有组合的二维矩阵进行卷积,每一个二维矩阵输出一个值(文中把这个称作一维卷积,因为实际上是把组合中所有词语的vector排成一行进行的卷积计算),构成Layer-2,然后进行2X2的MaxPooling。后续的卷积层均是传统的二维卷积操作，与第一层卷积层后的简单MaxPooling方式不同,后续的卷积层的Pooling是一种动态Pooling方法。输出固定维度的向量,接着输人MLP层,最终得到文本相似度分数ps。 3.1.4 最大覆盖度 本文沿用了基线模型的最大覆盖度算法,DuReader的基线模型采用问题和段落的最大词级别的覆盖度算法对段落进行排序,然后对每一个篇章挑选top-1作为模型的输入,本文将问题与段落的最大覆盖度作为ParaRanking的一个重要特征值定义为pc,其中不同于基线模型中最大覆盖度算法的是,这里分别选择了词和字两个粒度进行最大覆盖度计算,两者相加作为最终pc的值。 3.1.5 特征加权 首先通过分析DuReader的训练集可知,在描述类问题的答案中存在大量列表类型的答案，所以本文针对描述类问题识别出段落中的列表信息,并根据这一特征对段落的ParaRanking值进行加权，定义权值为B。经过语义匹配、最大覆盖度计算以及特征加权可以得到问题和段落i的最终匹配得分，如式下式所示。 3.1.6 多文档投票 本文两次用到多文档投票，一次在ParaRanking操作中，一次在答案预测中，前后两次所用到的方法有些不同。使用多文档投票是基于某一问题的正确答案在多个段落中会多次出现这一假设。首先 定义候选段落集合为Dp，对于段落i属于Dp，那么每一个段落的投票得分如下式所示。 所以最终得分段落，的最终得分为: 其中，f函数是指数平滑函数，最终经过ParaRanking 算法 ，每一个段落,i(属于Dp)会生成一个分score，随后根据score选择输人模型的段落集合Df，并且Df数量远远小于Dp。 RBiDAF模型架构 5 总结与展望本文提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。首先对DuReader数据集进行分析并对数据进行清洗,从而提取出有利于模型训练的特征;然后本文对RBiDAF机器阅读理解模型进行相关设计和实现,该模型的创新点在于在BiDAF模型四层网络框架的基础上添加了ParaRanking层,在该层,本文提出了基于多特征融合的ParaRanking算法。此外本文还在答案预测层,提出了基于先验知识的MACVerify算法,利用该算法对答案进行综合预测。最后经过实验和分析,RBiDAF模型能够产生有效的答案。在未来的工作中，首先将尝试实验多种词嵌入方法,很多学者证实选择合适的词嵌人方法对该任务会产生很大的影响;其次尝试采用机器翻译模型与对抗式生成模型(GAN)增强训练语料;最后在文本交互层融合双向注意力(Bi-Attention)与多轮匹配机制(Multi-Matching),从而可以在多文档场景下取得更好的效果。 论文地址","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"英文文本预处理代码","slug":"textpreprocess","date":"2019-03-29T21:50:08.000Z","updated":"2020-05-17T16:12:00.814Z","comments":true,"path":"2019/03/30/textpreprocess/","link":"","permalink":"http://renxingkai.github.io/2019/03/30/textpreprocess/","excerpt":"","text":"贴一段在做Kaggle QIQC时别人开源的kernel英语文本预处理代码，在做英文nlp任务时还是很有用的~ import osimport reimport gcimport stringimport unicodedataimport operatorimport numpy as npimport pandas as pdfrom tqdm import tqdmtqdm.pandas()&quot;&quot;&quot;utils&quot;&quot;&quot;def load_data(datapath): print(&quot;loading data ......&quot;) df_train = pd.read_csv(os.path.join(datapath, &quot;train.csv&quot;)) df_test = pd.read_csv(os.path.join(datapath, &quot;test.csv&quot;)) print(&quot;train data with shape : &quot;, df_train.shape) print(&quot;test data with shape : &quot;, df_test.shape) return df_train, df_test&quot;&quot;&quot;nlp&quot;&quot;&quot;def clean_misspell(text): &quot;&quot;&quot; misspell list (quora vs. glove) &quot;&quot;&quot; misspell_to_sub = &#123; &apos;Terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;BIMARU&apos;: &apos;Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh&apos;, &apos;Hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;Hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;Babchenko&apos;: &apos;Arkady Arkadyevich Babchenko faked death&apos;, &apos;Boshniaks&apos;: &apos;Bosniaks&apos;, &apos;Dravidanadu&apos;: &apos;Dravida Nadu&apos;, &apos;mysoginists&apos;: &apos;misogynists&apos;, &apos;MGTOWS&apos;: &apos;Men Going Their Own Way&apos;, &apos;mongloid&apos;: &apos;Mongoloid&apos;, &apos;unsincere&apos;: &apos;insincere&apos;, &apos;meninism&apos;: &apos;male feminism&apos;, &apos;jewplicate&apos;: &apos;jewish replicate&apos;, &apos;unoin&apos;: &apos;Union&apos;, &apos;daesh&apos;: &apos;Islamic State of Iraq and the Levant&apos;, &apos;Kalergi&apos;: &apos;Coudenhove-Kalergi&apos;, &apos;Bhakts&apos;: &apos;Bhakt&apos;, &apos;bhakts&apos;: &apos;Bhakt&apos;, &apos;Tambrahms&apos;: &apos;Tamil Brahmin&apos;, &apos;Pahul&apos;: &apos;Amrit Sanskar&apos;, &apos;SJW&apos;: &apos;social justice warrior&apos;, &apos;SJWs&apos;: &apos;social justice warrior&apos;, &apos; incel&apos;: &apos; involuntary celibates&apos;, &apos; incels&apos;: &apos; involuntary celibates&apos;, &apos;emiratis&apos;: &apos;Emiratis&apos;, &apos;weatern&apos;: &apos;western&apos;, &apos;westernise&apos;: &apos;westernize&apos;, &apos;Pizzagate&apos;: &apos;Pizzagate conspiracy theory&apos;, &apos;naïve&apos;: &apos;naive&apos;, &apos;Skripal&apos;: &apos;Sergei Skripal&apos;, &apos;Remainers&apos;: &apos;British remainer&apos;, &apos;remainers&apos;: &apos;British remainer&apos;, &apos;bremainer&apos;: &apos;British remainer&apos;, &apos;antibrahmin&apos;: &apos;anti Brahminism&apos;, &apos;HYPSM&apos;: &apos; Harvard, Yale, Princeton, Stanford, MIT&apos;, &apos;HYPS&apos;: &apos; Harvard, Yale, Princeton, Stanford&apos;, &apos;kompromat&apos;: &apos;compromising material&apos;, &apos;Tharki&apos;: &apos;pervert&apos;, &apos;tharki&apos;: &apos;pervert&apos;, &apos;mastuburate&apos;: &apos;masturbate&apos;, &apos;Zoë&apos;: &apos;Zoe&apos;, &apos;indans&apos;: &apos;Indian&apos;, &apos; xender&apos;: &apos; gender&apos;, &apos;Naxali &apos;: &apos;Naxalite &apos;, &apos;Naxalities&apos;: &apos;Naxalites&apos;, &apos;Bathla&apos;: &apos;Namit Bathla&apos;, &apos;Mewani&apos;: &apos;Indian politician Jignesh Mevani&apos;, &apos;clichéd&apos;: &apos;cliche&apos;, &apos;cliché&apos;: &apos;cliche&apos;, &apos;clichés&apos;: &apos;cliche&apos;, &apos;Wjy&apos;: &apos;Why&apos;, &apos;Fadnavis&apos;: &apos;Indian politician Devendra Fadnavis&apos;, &apos;Awadesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Awdhesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Khalistanis&apos;: &apos;Sikh separatist movement&apos;, &apos;madheshi&apos;: &apos;Madheshi&apos;, &apos;BNBR&apos;: &apos;Be Nice, Be Respectful&apos;, &apos;Bolsonaro&apos;: &apos;Jair Bolsonaro&apos;, &apos;XXXTentacion&apos;: &apos;Tentacion&apos;, &apos;Padmavat&apos;: &apos;Indian Movie Padmaavat&apos;, &apos;Žižek&apos;: &apos;Slovenian philosopher Slavoj Žižek&apos;, &apos;Adityanath&apos;: &apos;Indian monk Yogi Adityanath&apos;, &apos;Brexit&apos;: &apos;British Exit&apos;, &apos;Brexiter&apos;: &apos;British Exit supporter&apos;, &apos;Brexiters&apos;: &apos;British Exit supporters&apos;, &apos;Brexiteer&apos;: &apos;British Exit supporter&apos;, &apos;Brexiteers&apos;: &apos;British Exit supporters&apos;, &apos;Brexiting&apos;: &apos;British Exit&apos;, &apos;Brexitosis&apos;: &apos;British Exit disorder&apos;, &apos;brexit&apos;: &apos;British Exit&apos;, &apos;brexiters&apos;: &apos;British Exit supporters&apos;, &apos;jallikattu&apos;: &apos;Jallikattu&apos;, &apos;fortnite&apos;: &apos;Fortnite &apos;, &apos;Swachh&apos;: &apos;Swachh Bharat mission campaign &apos;, &apos;Quorans&apos;: &apos;Quoran&apos;, &apos;Qoura &apos;: &apos;Quora &apos;, &apos;quoras&apos;: &apos;Quora&apos;, &apos;Quroa&apos;: &apos;Quora&apos;, &apos;QUORA&apos;: &apos;Quora&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, # extra in sample &apos;Doklam&apos;: &apos;Tibet&apos;, &apos;Drumpf &apos;: &apos;Donald Trump fool &apos;, &apos;Drumpfs&apos;: &apos;Donald Trump fools&apos;, &apos;Strzok&apos;: &apos;Hillary Clinton scandal&apos;, &apos;rohingya&apos;: &apos;Rohingya &apos;, &apos;wumao &apos;: &apos;cheap Chinese stuff&apos;, &apos;wumaos&apos;: &apos;cheap Chinese stuff&apos;, &apos;Sanghis&apos;: &apos;Sanghi&apos;, &apos;Tamilans&apos;: &apos;Tamils&apos;, &apos;biharis&apos;: &apos;Biharis&apos;, &apos;Rejuvalex&apos;: &apos;hair growth formula&apos;, &apos;Feku&apos;: &apos;The Man of India &apos;, &apos;deplorables&apos;: &apos;deplorable&apos;, &apos;muhajirs&apos;: &apos;Muslim immigrant&apos;, &apos;Gujratis&apos;: &apos;Gujarati&apos;, &apos;Chutiya&apos;: &apos;Tibet people &apos;, &apos;Chutiyas&apos;: &apos;Tibet people &apos;, &apos;thighing&apos;: &apos;masturbate&apos;, &apos;卐&apos;: &apos;Nazi Germany&apos;, &apos;Pribumi&apos;: &apos;Native Indonesian&apos;, &apos;Gurmehar&apos;: &apos;Gurmehar Kaur Indian student activist&apos;, &apos;Novichok&apos;: &apos;Soviet Union agents&apos;, &apos;Khazari&apos;: &apos;Khazars&apos;, &apos;Demonetization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;demonitisation&apos;: &apos;demonetization&apos;, &apos;demonitization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;cryptocurrencies&apos;: &apos;cryptocurrency&apos;, &apos;Hindians&apos;: &apos;North Indian who hate British&apos;, &apos;vaxxer&apos;: &apos;vocal nationalist &apos;, &apos;remoaner&apos;: &apos;remainer &apos;, &apos;bremoaner&apos;: &apos;British remainer &apos;, &apos;Jewism&apos;: &apos;Judaism&apos;, &apos;Eroupian&apos;: &apos;European&apos;, &apos;WMAF&apos;: &apos;White male married Asian female&apos;, &apos;moeslim&apos;: &apos;Muslim&apos;, &apos;cishet&apos;: &apos;cisgender and heterosexual person&apos;, &apos;Eurocentric&apos;: &apos;Eurocentrism &apos;, &apos;Jewdar&apos;: &apos;Jew dar&apos;, &apos;Asifa&apos;: &apos;abduction, rape, murder case &apos;, &apos;marathis&apos;: &apos;Marathi&apos;, &apos;Trumpanzees&apos;: &apos;Trump chimpanzee fool&apos;, &apos;Crimean&apos;: &apos;Crimea people &apos;, &apos;atrracted&apos;: &apos;attract&apos;, &apos;LGBT&apos;: &apos;lesbian, gay, bisexual, transgender&apos;, &apos;Boshniak&apos;: &apos;Bosniaks &apos;, &apos;Myeshia&apos;: &apos;widow of Green Beret killed in Niger&apos;, &apos;demcoratic&apos;: &apos;Democratic&apos;, &apos;raaping&apos;: &apos;rape&apos;, &apos;Dönmeh&apos;: &apos;Islam&apos;, &apos;feminazism&apos;: &apos;feminism nazi&apos;, &apos;langague&apos;: &apos;language&apos;, &apos;Hongkongese&apos;: &apos;HongKong people&apos;, &apos;hongkongese&apos;: &apos;HongKong people&apos;, &apos;Kashmirians&apos;: &apos;Kashmirian&apos;, &apos;Chodu&apos;: &apos;fucker&apos;, &apos;penish&apos;: &apos;penis&apos;, &apos;micropenis&apos;: &apos;tiny penis&apos;, &apos;Madridiots&apos;: &apos;Real Madrid idiot supporters&apos;, &apos;Ambedkarite&apos;: &apos;Dalit Buddhist movement &apos;, &apos;ReleaseTheMemo&apos;: &apos;cry for the right and Trump supporters&apos;, &apos;harrase&apos;: &apos;harass&apos;, &apos;Barracoon&apos;: &apos;Black slave&apos;, &apos;Castrater&apos;: &apos;castration&apos;, &apos;castrater&apos;: &apos;castration&apos;, &apos;Rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;Turkified&apos;: &apos;Turkification&apos;, &apos;turkified&apos;: &apos;Turkification&apos;, &apos;Dumbassistan&apos;: &apos;dumb ass Pakistan&apos;, &apos;facetards&apos;: &apos;Facebook retards&apos;, &apos;rapefugees&apos;: &apos;rapist refugee&apos;, &apos;superficious&apos;: &apos;superficial&apos;, # extra from kagglers &apos;colour&apos;: &apos;color&apos;, &apos;centre&apos;: &apos;center&apos;, &apos;favourite&apos;: &apos;favorite&apos;, &apos;travelling&apos;: &apos;traveling&apos;, &apos;counselling&apos;: &apos;counseling&apos;, &apos;theatre&apos;: &apos;theater&apos;, &apos;cancelled&apos;: &apos;canceled&apos;, &apos;labour&apos;: &apos;labor&apos;, &apos;organisation&apos;: &apos;organization&apos;, &apos;wwii&apos;: &apos;world war 2&apos;, &apos;citicise&apos;: &apos;criticize&apos;, &apos;youtu &apos;: &apos;youtube &apos;, &apos;sallary&apos;: &apos;salary&apos;, &apos;Whta&apos;: &apos;What&apos;, &apos;narcisist&apos;: &apos;narcissist&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, &apos;howdo&apos;: &apos;how do&apos;, &apos;whatare&apos;: &apos;what are&apos;, &apos;howcan&apos;: &apos;how can&apos;, &apos;howmuch&apos;: &apos;how much&apos;, &apos;howmany&apos;: &apos;how many&apos;, &apos;whydo&apos;: &apos;why do&apos;, &apos;doI&apos;: &apos;do I&apos;, &apos;theBest&apos;: &apos;the best&apos;, &apos;howdoes&apos;: &apos;how does&apos;, &apos;mastrubation&apos;: &apos;masturbation&apos;, &apos;mastrubate&apos;: &apos;masturbate&apos;, &apos;mastrubating&apos;: &apos;masturbating&apos;, &apos;pennis&apos;: &apos;penis&apos;, &apos;Etherium&apos;: &apos;Ethereum&apos;, &apos;bigdata&apos;: &apos;big data&apos;, &apos;2k17&apos;: &apos;2017&apos;, &apos;2k18&apos;: &apos;2018&apos;, &apos;qouta&apos;: &apos;quota&apos;, &apos;exboyfriend&apos;: &apos;ex boyfriend&apos;, &apos;airhostess&apos;: &apos;air hostess&apos;, &apos;whst&apos;: &apos;what&apos;, &apos;watsapp&apos;: &apos;whatsapp&apos;, # extra &apos;bodyshame&apos;: &apos;body shaming&apos;, &apos;bodyshoppers&apos;: &apos;body shopping&apos;, &apos;bodycams&apos;: &apos;body cams&apos;, &apos;Cananybody&apos;: &apos;Can any body&apos;, &apos;deadbody&apos;: &apos;dead body&apos;, &apos;deaddict&apos;: &apos;de addict&apos;, &apos;Northindian&apos;: &apos;North Indian &apos;, &apos;northindian&apos;: &apos;north Indian &apos;, &apos;northkorea&apos;: &apos;North Korea&apos;, &apos;Whykorean&apos;: &apos;Why Korean&apos;, &apos;koreaboo&apos;: &apos;Korea boo &apos;, &apos;Brexshit&apos;: &apos;British Exit bullshit&apos;, &apos;shithole&apos;: &apos; shithole &apos;, &apos;shitpost&apos;: &apos;shit post&apos;, &apos;shitslam&apos;: &apos;shit Islam&apos;, &apos;shitlords&apos;: &apos;shit lords&apos;, &apos;Fck&apos;: &apos;Fuck&apos;, &apos;fck&apos;: &apos;fuck&apos;, &apos;Clickbait&apos;: &apos;click bait &apos;, &apos;clickbait&apos;: &apos;click bait &apos;, &apos;mailbait&apos;: &apos;mail bait&apos;, &apos;healhtcare&apos;: &apos;healthcare&apos;, &apos;trollbots&apos;: &apos;troll bots&apos;, &apos;trollled&apos;: &apos;trolled&apos;, &apos;trollimg&apos;: &apos;trolling&apos;, &apos;cybertrolling&apos;: &apos;cyber trolling&apos;, &apos;sickular&apos;: &apos;India sick secular &apos;, &apos;suckimg&apos;: &apos;sucking&apos;, &apos;Idiotism&apos;: &apos;idiotism&apos;, &apos;Niggerism&apos;: &apos;Nigger&apos;, &apos;Niggeriah&apos;: &apos;Nigger&apos; &#125; misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_to_sub.keys())) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = misspell_to_sub.get(match.group(0)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return misspell_re.sub(_replace, text)def spacing_misspell(text): &quot;&quot;&quot; &apos;deadbody&apos; -&gt; &apos;dead body&apos; &quot;&quot;&quot; misspell_list = [ &apos;(F|f)uck&apos;, &apos;Trump&apos;, &apos;\\W(A|a)nti&apos;, &apos;(W|w)hy&apos;, &apos;(W|w)hat&apos;, &apos;How&apos;, &apos;care\\W&apos;, &apos;\\Wover&apos;, &apos;gender&apos;, &apos;people&apos;, ] misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_list)) return misspell_re.sub(r&quot; \\1 &quot;, text)def clean_latex(text): &quot;&quot;&quot; convert r&quot;[math]\\vec&#123;x&#125; + \\vec&#123;y&#125;&quot; to English &quot;&quot;&quot; # edge case text = re.sub(r&apos;\\[math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\[\\/math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\\\&apos;, &apos; LaTex &apos;, text) pattern_to_sub = &#123; r&apos;\\\\mathrm&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\mathbb&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\boxed&apos;: &apos; LaTex equation &apos;, r&apos;\\\\begin&apos;: &apos; LaTex equation &apos;, r&apos;\\\\end&apos;: &apos; LaTex equation &apos;, r&apos;\\\\left&apos;: &apos; LaTex equation &apos;, r&apos;\\\\right&apos;: &apos; LaTex equation &apos;, r&apos;\\\\(over|under)brace&apos;: &apos; LaTex equation &apos;, r&apos;\\\\text&apos;: &apos; LaTex equation &apos;, r&apos;\\\\vec&apos;: &apos; vector &apos;, r&apos;\\\\var&apos;: &apos; variable &apos;, r&apos;\\\\theta&apos;: &apos; theta &apos;, r&apos;\\\\mu&apos;: &apos; average &apos;, r&apos;\\\\min&apos;: &apos; minimum &apos;, r&apos;\\\\max&apos;: &apos; maximum &apos;, r&apos;\\\\sum&apos;: &apos; + &apos;, r&apos;\\\\times&apos;: &apos; * &apos;, r&apos;\\\\cdot&apos;: &apos; * &apos;, r&apos;\\\\hat&apos;: &apos; ^ &apos;, r&apos;\\\\frac&apos;: &apos; / &apos;, r&apos;\\\\div&apos;: &apos; / &apos;, r&apos;\\\\sin&apos;: &apos; Sine &apos;, r&apos;\\\\cos&apos;: &apos; Cosine &apos;, r&apos;\\\\tan&apos;: &apos; Tangent &apos;, r&apos;\\\\infty&apos;: &apos; infinity &apos;, r&apos;\\\\int&apos;: &apos; integer &apos;, r&apos;\\\\in&apos;: &apos; in &apos;, &#125; # post process for look up pattern_dict = &#123;k.strip(&apos;\\\\&apos;): v for k, v in pattern_to_sub.items()&#125; # init re patterns = pattern_to_sub.keys() pattern_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(patterns)) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = pattern_dict.get(match.group(0).strip(&apos;\\\\&apos;)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return pattern_re.sub(_replace, text)def normalize_unicode(text): &quot;&quot;&quot; unicode string normalization &quot;&quot;&quot; return unicodedata.normalize(&apos;NFKD&apos;, text)def remove_newline(text): &quot;&quot;&quot; remove \\n and \\t &quot;&quot;&quot; text = re.sub(&apos;\\n&apos;, &apos; &apos;, text) text = re.sub(&apos;\\t&apos;, &apos; &apos;, text) text = re.sub(&apos;\\b&apos;, &apos; &apos;, text) text = re.sub(&apos;\\r&apos;, &apos; &apos;, text) return textdef decontracted(text): &quot;&quot;&quot; de-contract the contraction &quot;&quot;&quot; # specific text = re.sub(r&quot;(W|w)on(\\&apos;|\\’)t&quot;, &quot;will not&quot;, text) text = re.sub(r&quot;(C|c)an(\\&apos;|\\’)t&quot;, &quot;can not&quot;, text) text = re.sub(r&quot;(Y|y)(\\&apos;|\\’)all&quot;, &quot;you all&quot;, text) text = re.sub(r&quot;(Y|y)a(\\&apos;|\\’)ll&quot;, &quot;you all&quot;, text) # general text = re.sub(r&quot;(I|i)(\\&apos;|\\’)m&quot;, &quot;i am&quot;, text) text = re.sub(r&quot;(A|a)in(\\&apos;|\\’)t&quot;, &quot;is not&quot;, text) text = re.sub(r&quot;n(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)re&quot;, &quot; are&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)s&quot;, &quot; is&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)d&quot;, &quot; would&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ll&quot;, &quot; will&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ve&quot;, &quot; have&quot;, text) return textdef spacing_punctuation(text): &quot;&quot;&quot; add space before and after punctuation and symbols &quot;&quot;&quot; regular_punct = list(string.punctuation) extra_punct = [ &apos;,&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;:&apos;, &apos;)&apos;, &apos;(&apos;, &apos;-&apos;, &apos;!&apos;, &apos;?&apos;, &apos;|&apos;, &apos;;&apos;, &quot;&apos;&quot;, &apos;$&apos;, &apos;&amp;&apos;, &apos;/&apos;, &apos;[&apos;, &apos;]&apos;, &apos;&gt;&apos;, &apos;%&apos;, &apos;=&apos;, &apos;#&apos;, &apos;*&apos;, &apos;+&apos;, &apos;\\\\&apos;, &apos;•&apos;, &apos;~&apos;, &apos;@&apos;, &apos;£&apos;, &apos;·&apos;, &apos;_&apos;, &apos;&#123;&apos;, &apos;&#125;&apos;, &apos;©&apos;, &apos;^&apos;, &apos;®&apos;, &apos;`&apos;, &apos;&lt;&apos;, &apos;→&apos;, &apos;°&apos;, &apos;€&apos;, &apos;™&apos;, &apos;›&apos;, &apos;♥&apos;, &apos;←&apos;, &apos;×&apos;, &apos;§&apos;, &apos;″&apos;, &apos;′&apos;, &apos;Â&apos;, &apos;█&apos;, &apos;½&apos;, &apos;à&apos;, &apos;…&apos;, &apos;“&apos;, &apos;★&apos;, &apos;”&apos;, &apos;–&apos;, &apos;●&apos;, &apos;â&apos;, &apos;►&apos;, &apos;−&apos;, &apos;¢&apos;, &apos;²&apos;, &apos;¬&apos;, &apos;░&apos;, &apos;¶&apos;, &apos;↑&apos;, &apos;±&apos;, &apos;¿&apos;, &apos;▾&apos;, &apos;═&apos;, &apos;¦&apos;, &apos;║&apos;, &apos;―&apos;, &apos;¥&apos;, &apos;▓&apos;, &apos;—&apos;, &apos;‹&apos;, &apos;─&apos;, &apos;▒&apos;, &apos;：&apos;, &apos;¼&apos;, &apos;⊕&apos;, &apos;▼&apos;, &apos;▪&apos;, &apos;†&apos;, &apos;■&apos;, &apos;’&apos;, &apos;▀&apos;, &apos;¨&apos;, &apos;▄&apos;, &apos;♫&apos;, &apos;☆&apos;, &apos;é&apos;, &apos;¯&apos;, &apos;♦&apos;, &apos;¤&apos;, &apos;▲&apos;, &apos;è&apos;, &apos;¸&apos;, &apos;¾&apos;, &apos;Ã&apos;, &apos;⋅&apos;, &apos;‘&apos;, &apos;∞&apos;, &apos;∙&apos;, &apos;）&apos;, &apos;↓&apos;, &apos;、&apos;, &apos;│&apos;, &apos;（&apos;, &apos;»&apos;, &apos;，&apos;, &apos;♪&apos;, &apos;╩&apos;, &apos;╚&apos;, &apos;³&apos;, &apos;・&apos;, &apos;╦&apos;, &apos;╣&apos;, &apos;╔&apos;, &apos;╗&apos;, &apos;▬&apos;, &apos;❤&apos;, &apos;ï&apos;, &apos;Ø&apos;, &apos;¹&apos;, &apos;≤&apos;, &apos;‡&apos;, &apos;√&apos;, &apos;«&apos;, &apos;»&apos;, &apos;´&apos;, &apos;º&apos;, &apos;¾&apos;, &apos;¡&apos;, &apos;§&apos;, &apos;£&apos;, &apos;₤&apos;] all_punct = &apos;&apos;.join(sorted(list(set(regular_punct + extra_punct)))) re_tok = re.compile(f&apos;([&#123;all_punct&#125;])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_digit(text): &quot;&quot;&quot; add space before and after digits &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_number(text): &quot;&quot;&quot; add space before and after numbers &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9]&#123;1,&#125;)&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def remove_number(text): &quot;&quot;&quot; numbers are not toxic &quot;&quot;&quot; return re.sub(&apos;\\d+&apos;, &apos; &apos;, text)def remove_space(text): &quot;&quot;&quot; remove extra spaces and ending space if any &quot;&quot;&quot; text = re.sub(&apos;\\s+&apos;, &apos; &apos;, text) text = re.sub(&apos;\\s+$&apos;, &apos;&apos;, text) return text&quot;&quot;&quot;tokenizer&quot;&quot;&quot;def preprocess(text, remove_num=True): &quot;&quot;&quot; preprocess text into clean text for tokenization NOTE: 1. glove supports uppper case words 2. glove supports digit 3. glove supports punctuation 5. glove supports domains e.g. www.apple.com 6. glove supports misspelled words e.g. FUCKKK &quot;&quot;&quot; # # 1. normalize # text = normalize_unicode(text) # # 2. remove new line # text = remove_newline(text) # 3. de-contract text = decontracted(text) # 4. clean misspell text = clean_misspell(text) # 5. space misspell text = spacing_misspell(text) # 6. clean_latex text = clean_latex(text) # 7. space text = spacing_punctuation(text) # 8. handle number if remove_num: text = remove_number(text) else: text = spacing_digit(text) # 9. remove space text = remove_space(text) return text 调用preprocess(text) 就好，返回处理完后的文本","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"}],"author":"CinKate"},{"title":"nltk---词性标注","slug":"nltk-postag","date":"2019-03-29T09:06:25.000Z","updated":"2020-05-17T16:11:59.994Z","comments":true,"path":"2019/03/29/nltk-postag/","link":"","permalink":"http://renxingkai.github.io/2019/03/29/nltk-postag/","excerpt":"","text":"1.POS标签器推荐使用nltk推荐的pos_tag()函数，基于Penn Treebank，以下代码展示了使用nltk获取句子POS标签的方法： sentence = &apos;The brown fox is quick and he is jumping over the lazy dog&apos;# recommended tagger based on PTBimport nltktokens = nltk.word_tokenize(sentence)tagged_sent = nltk.pos_tag(tokens, tagset=&apos;universal&apos;)print (tagged_sent) 输出： [(&apos;The&apos;, &apos;DET&apos;), (&apos;brown&apos;, &apos;ADJ&apos;), (&apos;fox&apos;, &apos;NOUN&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;quick&apos;, &apos;ADJ&apos;), (&apos;and&apos;, &apos;CONJ&apos;), (&apos;he&apos;, &apos;PRON&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;jumping&apos;, &apos;VERB&apos;), (&apos;over&apos;, &apos;ADP&apos;), (&apos;the&apos;, &apos;DET&apos;), (&apos;lazy&apos;, &apos;ADJ&apos;), (&apos;dog&apos;, &apos;NOUN&apos;)] 2.建立自己的POS标签器准备数据：# preparing the datafrom nltk.corpus import treebankdata = treebank.tagged_sents()train_data = data[:3500]test_data = data[3500:]print (train_data[0]) 输出：[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;61&apos;, &apos;CD&apos;), (&apos;years&apos;, &apos;NNS&apos;), (&apos;old&apos;, &apos;JJ&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;will&apos;, &apos;MD&apos;), (&apos;join&apos;, &apos;VB&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;board&apos;, &apos;NN&apos;), (&apos;as&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;DT&apos;), (&apos;nonexecutive&apos;, &apos;JJ&apos;), (&apos;director&apos;, &apos;NN&apos;), (&apos;Nov.&apos;, &apos;NNP&apos;), (&apos;29&apos;, &apos;CD&apos;), (&apos;.&apos;, &apos;.&apos;)] 2.1DefaultTagger默认标签器首先我们试下从SequentialBackoffTagger基类继承的DefaultTagger，并为每个单词分配相同的用户输入POS标签。 # default taggerfrom nltk.tag import DefaultTaggerdt = DefaultTagger(&apos;NN&apos;)print(dt.evaluate(test_data))print(dt.tag(tokens)) 输出： 0.1454158195372253[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;jumping&apos;, &apos;NN&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 默认获得了14%的准确率，由于给标签器输入的都是相同的标签（‘NN’），因此输出标签获得的都是名词。 2.2RegexpTagger正则表达式标签器# regex taggerfrom nltk.tag import RegexpTagger# define regex tag patternspatterns = [ (r&apos;.*ing$&apos;, &apos;VBG&apos;), # gerunds (r&apos;.*ed$&apos;, &apos;VBD&apos;), # simple past (r&apos;.*es$&apos;, &apos;VBZ&apos;), # 3rd singular present (r&apos;.*ould$&apos;, &apos;MD&apos;), # modals (r&apos;.*\\&apos;s$&apos;, &apos;NN$&apos;), # possessive nouns (r&apos;.*s$&apos;, &apos;NNS&apos;), # plural nouns (r&apos;^-?[0-9]+(.[0-9]+)?$&apos;, &apos;CD&apos;), # cardinal numbers (r&apos;.*&apos;, &apos;NN&apos;) # nouns (default) ... ]rt = RegexpTagger(patterns)print(rt.evaluate(test_data))print(rt.tag(tokens)) 输出： 0.24039113176493368[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率提高到了24%，还是有效果的~ 2.3一、二、三元标签器## N gram taggersfrom nltk.tag import UnigramTaggerfrom nltk.tag import BigramTaggerfrom nltk.tag import TrigramTaggerut = UnigramTagger(train_data)bt = BigramTagger(train_data)tt = TrigramTagger(train_data)print(ut.evaluate(test_data))print(ut.tag(tokens))print(bt.evaluate(test_data))print(bt.tag(tokens))print (tt.evaluate(test_data))print(tt.tag(tokens)) 输出： 0.8607803272340013[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.13466937748087907[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.08064672281924679[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)] 发现一元的准确率最高，达到了86%,二、三元准确率低的原因可能是在训练数据中观察到的二元词组和三元词组不一定会在测试数据中以相同的方式出现。 2.4包含标签列表的组合标签器及使用backoff标签器本质上，我们将创建一个标签器链，对于每一个标签器，吐过他不能标记输入的标识，则标签器的下一步将会回退到backoff标签器： def combined_tagger(train_data, taggers, backoff=None): for tagger in taggers: backoff = tagger(train_data, backoff=backoff) return backoff#backoff to regtaggerct = combined_tagger(train_data=train_data, taggers=[UnigramTagger, BigramTagger, TrigramTagger], backoff=rt)print(ct.evaluate(test_data)) print(ct.tag(tokens)) 输出： 0.9094781682641108[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率到了90% 2.5ClassifierBasedPOSTagger标签器(有监督分类算法)使用ClassifierBasedPOSTagger类中的classifier_builder参数中的有监督机器学习算法来训练标签器。 from nltk.classify import NaiveBayesClassifier, MaxentClassifierfrom nltk.tag.sequential import ClassifierBasedPOSTaggernbt = ClassifierBasedPOSTagger(train=train_data, classifier_builder=NaiveBayesClassifier.train)print(nbt.evaluate(test_data))print(nbt.tag(tokens)) 输出： 0.9306806079969019[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;JJ&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;JJ&apos;), (&apos;dog&apos;, &apos;VBG&apos;)] 有监督准确率达到了0.93 2.6Just For Fun!(MaxentClassifier)# try this out for fun!met = ClassifierBasedPOSTagger(train=train_data, classifier_builder=MaxentClassifier.train)print(met.evaluate(test_data)) print(met.tag(tokens)) 输出： ==&gt; Training (100 iterations) Iteration Log Likelihood Accuracy --------------------------------------- 1 -3.82864 0.007 2 -0.76176 0.957 Final nan 0.9840.9269048310581857[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] MaxentClassifier准确率达到了0.92","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"}],"author":"CinKate"},{"title":"nltk---分词与文本预处理","slug":"nltk-tokenize","date":"2019-03-28T15:08:16.000Z","updated":"2020-05-17T16:12:00.073Z","comments":true,"path":"2019/03/28/nltk-tokenize/","link":"","permalink":"http://renxingkai.github.io/2019/03/28/nltk-tokenize/","excerpt":"","text":"参考《text-analytics-with-python》中的第三章中的处理和理解文本对nltk等常用nlp包进行总结，以供之后复习与使用~ 1.tokenize(切分词(句子))首先，标识(token)是具有一定的句法语义且独立的最小文本成分， 1.1句子切分句子切分基本技术包括在句子之间寻找特定的分割符，例如句号(‘.’)，换行符(‘\\n’)或者分号(‘;’)等。在nltk中，主要关注以下句子切分器: nltk.sent_tokenize(默认句子切分器) nltk.tokenize.PunktSentenceTokenizer() nltk.tokenize.RegexpTokenizer()以下直接上代码： import nltkfrom nltk.corpus import gutenbergfrom pprint import pprint#载入语料alice=gutenberg.raw(fileids=&apos;carroll-alice.txt&apos;)sample_text = &apos;We will discuss briefly about the basic syntax,\\ structure and design philosophies. \\ There is a defined hierarchical syntax for Python code which you should remember \\ when writing code! Python is a really powerful programming language!&apos;# Total characters in Alice in Wonderlandprint(len(alice))# First 100 characters in the corpusprint(alice[0:100]) 输出： 144395[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]CHAPTER I. Down the Rabbit-HoleAlice was 1.1.1默认分词器–nltk.sent_tokenize#默认分词器default_st = nltk.sent_tokenizealice_sentences = default_st(text=alice)sample_sentences = default_st(text=sample_text)print(&apos;Total sentences in sample_text:&apos;, len(sample_sentences))print(&apos;Sample text sentences :-&apos;)pprint(sample_sentences)print(&apos;\\nTotal sentences in alice:&apos;, len(alice_sentences))print(&apos;First 5 sentences in alice:-&apos;)pprint(alice_sentences[0:5]) 输出： Total sentences in sample_text: 3Sample text sentences :-[&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;]Total sentences in alice: 1625First 5 sentences in alice:-[&quot;[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.&quot;, &apos;Down the Rabbit-Hole\\n&apos; &apos;\\n&apos; &apos;Alice was beginning to get very tired of sitting by her sister on the\\n&apos; &apos;bank, and of having nothing to do: once or twice she had peeped into the\\n&apos; &apos;book her sister was reading, but it had no pictures or conversations in\\n&apos; &quot;it, &apos;and what is the use of a book,&apos; thought Alice &apos;without pictures or\\n&quot; &quot;conversation?&apos;&quot;, &apos;So she was considering in her own mind (as well as she could, for the\\n&apos; &apos;hot day made her feel very sleepy and stupid), whether the pleasure\\n&apos; &apos;of making a daisy-chain would be worth the trouble of getting up and\\n&apos; &apos;picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n&apos; &apos;close by her.&apos;, &apos;There was nothing so VERY remarkable in that; nor did Alice think it so\\n&apos; &quot;VERY much out of the way to hear the Rabbit say to itself, &apos;Oh dear!&quot;, &apos;Oh dear!&apos;] 德语切分 ## 其他语言句子切分器from nltk.corpus import europarl_raw#德语german_text = europarl_raw.german.raw(fileids=&apos;ep-00-01-17.de&apos;)# 语料中的词数print(len(german_text))# 前100字符print(german_text[0:100]) 输出： 157171 Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit 使用默认分词器切分德语# 默认句子切分器german_sentences_def = default_st(text=german_text, language=&apos;german&apos;)# loading german text tokenizer into a PunktSentenceTokenizer instance german_tokenizer = nltk.data.load(resource_url=&apos;tokenizers/punkt/german.pickle&apos;)german_sentences = german_tokenizer.tokenize(german_text)# verify the type of german_tokenizer# should be PunktSentenceTokenizerprint(type(german_tokenizer))# check if results of both tokenizers match# should be Trueprint(german_sentences_def == german_sentences)# print first 5 sentences of the corpusfor sent in german_sentences[0:5]: print(sent) 输出: &lt;class &apos;nltk.tokenize.punkt.PunktSentenceTokenizer&apos;&gt;True Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der gefürchtete &quot; Millenium-Bug &quot; nicht eingetreten .Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken . 1.1.2使用PunktSentenceTokenizer## using PunktSentenceTokenizer for sentence tokenizationpunkt_st = nltk.tokenize.PunktSentenceTokenizer()sample_sentences = punkt_st.tokenize(sample_text)pprint(sample_sentences) 输出: [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.1.3使用RegexpTokenizer#使用正则表达式做句子切分## using RegexpTokenizer for sentence tokenizationSENTENCE_TOKENS_PATTERN = r&apos;(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;![A-Z]\\.)(?&lt;=\\.|\\?|\\!)\\s&apos;regex_st = nltk.tokenize.RegexpTokenizer( pattern=SENTENCE_TOKENS_PATTERN, gaps=True)sample_sentences = regex_st.tokenize(sample_text)pprint(sample_sentences) 输出： [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos; There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.2词语切分1.2.1默认分词器nltk.word_tokenize## 分词sentence = &quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;# default word tokenizerdefault_wt = nltk.word_tokenizewords = default_wt(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.2Treebank分词器# treebank word tokenizertreebank_wt = nltk.TreebankWordTokenizer()words = treebank_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.3正则分词器RegexpTokenizer# 正则切分TOKEN_PATTERN = r&apos;\\w+&apos; regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 设置正则模式: GAP_PATTERN = r&apos;\\s+&apos; regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 分词索引word_indices = list(regex_wt.span_tokenize(sentence))print(word_indices)print([sentence[start:end] for start, end in word_indices]) 输出：[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)][&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.4WordPunctTokenizer分词器# derived regex tokenizers(派生类执行分词)wordpunkt_wt = nltk.WordPunctTokenizer()words = wordpunkt_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.5WhitespaceTokenizer分词器#WhitespaceTokenizer基于诸如缩进符、换行符及空格的空白字符将句子分割成单词whitespace_wt = nltk.WhitespaceTokenizer()words = whitespace_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 2.文本规范化文本规范化定义为这样的一一个过程，它包含一系列步骤， 依次是转换、清洗以及将文本数据标准化成可供NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常称为文本清洗或转换。 本节将讨论在文本规范化过程中使用的各种技术。在探索各种技术之前，请使用以下代码段来加载基本的依存关系以及将使用的语料库: import nltkimport reimport stringfrom pprint import pprintcorpus = [&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for $199&quot;, &quot;@@You&apos;ll (learn) a **lot** in the book. Python is an amazing language!@@&quot;] 2.2文本清洗可以使用nltk中的clean_html()函数，或者BeautifulSoup库来解析HTML数据，还可以使用自定义的逻辑，包括正则表达式、xpath和lxml库来解析XML数据。 2.3文本切分def tokenize_text(text): sentences = nltk.sent_tokenize(text) word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] return word_tokens token_list = [tokenize_text(text) for text in corpus]print(token_list) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;$&apos;, &apos;199&apos;]], [[&apos;@&apos;, &apos;@&apos;, &apos;You&apos;, &quot;&apos;ll&quot;, &apos;(&apos;, &apos;learn&apos;, &apos;)&apos;, &apos;a&apos;, &apos;**lot**&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;], [&apos;@&apos;, &apos;@&apos;]]] 2.4删除特殊字符在分词后删除特殊字符def remove_characters_after_tokenization(tokens): pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = [pattern.sub(&apos;&apos;, token) for token in tokens] return filtered_tokens filtered_list_1 = [[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens] for sentence_tokens in token_list]pprint(filtered_list_1) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &apos;nt&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &apos;nt&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &apos;s&apos;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;&apos;, &apos;199&apos;]], [[&apos;&apos;, &apos;&apos;, &apos;You&apos;, &apos;ll&apos;, &apos;&apos;, &apos;learn&apos;, &apos;&apos;, &apos;a&apos;, &apos;lot&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;&apos;], [&apos;&apos;, &apos;&apos;]]] 在分词前删除特殊字符def remove_characters_before_tokenization(sentence, keep_apostrophes=False): sentence = sentence.strip() if keep_apostrophes: PATTERN = r&apos;[?|$|&amp;|*|%|@|(|)|~]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) else: PATTERN = r&apos;[^a-zA-Z0-9 ]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) return filtered_sentence filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus] print(filtered_list_2)cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]print(cleaned_corpus) 输出： [&apos;The brown fox wasnt that quick and he couldnt win the race&apos;, &apos;Hey thats a great deal I just bought a phone for 199&apos;, &apos;Youll learn a lot in the book Python is an amazing language&apos;][&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for 199&quot;, &quot;You&apos;ll learn a lot in the book. Python is an amazing language!&quot;] 2.5扩展缩写词将is’nt 还原为is not等等… from contractions import contractions_dictdef expand_contractions(sentence, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match)\\ if contraction_mapping.get(match)\\ else contraction_mapping.get(match.lower()) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_sentence = contractions_pattern.sub(expand_match, sentence) return expanded_sentence expanded_corpus = [expand_contractions(sentence, contractions_dict) for sentence in cleaned_corpus] print(expanded_corpus) 输出： [&apos;The brown fox was not that quick and he could not win the race&apos;, &apos;Hey that is a great deal! I just bought a phone for 199&apos;, &apos;You will learn a lot in the book. Python is an amazing language!&apos;] 2.6大小写转换# case conversion print(corpus[0].lower())print(corpus[0].upper()) 输出：the brown fox wasn&apos;t that quick and he couldn&apos;t win the raceTHE BROWN FOX WASN&apos;T THAT QUICK AND HE COULDN&apos;T WIN THE RACE 2.7删除停用词# removing stopwordsdef remove_stopwords(tokens): stopword_list = nltk.corpus.stopwords.words(&apos;english&apos;) filtered_tokens = [token for token in tokens if token not in stopword_list] return filtered_tokens expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus] filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]print(filtered_list_3) 输出: [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;quick&apos;, &apos;could&apos;, &apos;win&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;bought&apos;, &apos;phone&apos;, &apos;199&apos;]], [[&apos;You&apos;, &apos;learn&apos;, &apos;lot&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;]]] 2.8词语校正删除重复的字符# removing repeated characterssample_sentence = &apos;My schooool is realllllyyy amaaazingggg&apos;sample_sentence_tokens = tokenize_text(sample_sentence)[0]from nltk.corpus import wordnetdef remove_repeated_characters(tokens): repeat_pattern = re.compile(r&apos;(\\w*)(\\w)\\2(\\w*)&apos;) match_substitution = r&apos;\\1\\2\\3&apos; def replace(old_word): if wordnet.synsets(old_word): return old_word new_word = repeat_pattern.sub(match_substitution, old_word) return replace(new_word) if new_word != old_word else new_word correct_tokens = [replace(word) for word in tokens] return correct_tokensprint(remove_repeated_characters(sample_sentence_tokens)) 输出:[&apos;My&apos;, &apos;school&apos;, &apos;is&apos;, &apos;really&apos;, &apos;amazing&apos;] 2.9词干提取2.9.1Port词干提取器# porter stemmerfrom nltk.stem import PorterStemmerps = PorterStemmer()print(ps.stem(&apos;jumping&apos;), ps.stem(&apos;jumps&apos;), ps.stem(&apos;jumped&apos;))print(ps.stem(&apos;lying&apos;))print(ps.stem(&apos;strange&apos;)) 输出： jump jump jumpliestrang 2.9.2LancasterStemmer词干提取器# lancaster stemmerfrom nltk.stem import LancasterStemmerls = LancasterStemmer()print(ls.stem(&apos;jumping&apos;), ls.stem(&apos;jumps&apos;), ls.stem(&apos;jumped&apos;))print (ls.stem(&apos;lying&apos;))print (ls.stem(&apos;strange&apos;)) 输出： jump jump jumplyingstrange 2.9.3RegexpStemmer正则词干提取器# regex stemmerfrom nltk.stem import RegexpStemmerrs = RegexpStemmer(&apos;ing$|s$|ed$&apos;, min=4)print( rs.stem(&apos;jumping&apos;), rs.stem(&apos;jumps&apos;), rs.stem(&apos;jumped&apos;))print (rs.stem(&apos;lying&apos;))print (rs.stem(&apos;strange&apos;)) 输出： jump jump jumplystrange 2.9.4SnowballStemmer词干提取器# snowball stemmerfrom nltk.stem import SnowballStemmerss = SnowballStemmer(&quot;german&quot;)print (&apos;Supported Languages:&apos;, SnowballStemmer.languages)# autobahnen -&gt; cars# autobahn -&gt; carss.stem(&apos;autobahnen&apos;)# springen -&gt; jumping# spring -&gt; jumpss.stem(&apos;springen&apos;) 输出： Supported Languages: (&apos;arabic&apos;, &apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;, &apos;hungarian&apos;, &apos;italian&apos;, &apos;norwegian&apos;, &apos;porter&apos;, &apos;portuguese&apos;, &apos;romanian&apos;, &apos;russian&apos;, &apos;spanish&apos;, &apos;swedish&apos;)Out[14]:&apos;spring&apos; 2.10词形还原# lemmatizationfrom nltk.stem import WordNetLemmatizerwnl = WordNetLemmatizer()# lemmatize nounsprint( wnl.lemmatize(&apos;cars&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;men&apos;, &apos;n&apos;))# lemmatize verbsprint (wnl.lemmatize(&apos;running&apos;, &apos;v&apos;))print (wnl.lemmatize(&apos;ate&apos;, &apos;v&apos;))# lemmatize adjectivesprint (wnl.lemmatize(&apos;saddest&apos;, &apos;a&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;a&apos;))# ineffective lemmatizationprint (wnl.lemmatize(&apos;ate&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;v&apos;)) 输出： carmenruneatsadfancyatefancier","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"}],"author":"CinKate"},{"title":"《活着》读后感","slug":"huozheduhougan","date":"2019-03-25T15:01:09.000Z","updated":"2020-05-17T16:11:59.457Z","comments":true,"path":"2019/03/25/huozheduhougan/","link":"","permalink":"http://renxingkai.github.io/2019/03/25/huozheduhougan/","excerpt":"","text":"之前在本科的时候只看过《活着》这部电影（葛优、巩俐主演），电影里已经很惨了，当时虽然看了把两遍电影，但是最大的感受是：虽然这部电影名为“活着”，可却不停地有人离开，也对福贵的悲痛人生感到痛惜，看着自己的亲人，一个个，一个个离开自己，世上也孤零零仅剩自己一个人，那种悲伤之情真的难以承受。 最近，在微信读书上看原著，可能由于年龄的增加，经历的事多了，以及作者的绝妙文笔，感觉读起来的画面感，不亚于看一部精彩的电影，甚至了超过了表演形式。 原著中，福贵的生平更惨，电影中他的孙子还能陪他一起在世上，原著中却真的只有他一个人走到了最后，亲手埋下了自己的儿子、女儿、妻子、女婿、孙子……从最初的悲恸不已，到后来的“心情平淡”，或许生活的残酷已经将这个男人摧残的遍体鳞伤，但他仍然坚强地活着，攒了两年钱却买了一头已然年暮的老牛，因为他知道活着的不易，他珍惜活着，他珍惜每一天。 可惜生活能留给他的，也仅剩了活着。“少年去游荡，中年想掘藏，老年做和尚。”经历了最惨痛的事情，才能看淡生活吧。 真的希望，活着的人能好好活着，每天清晨迎接初日，傍晚送走晚霞，日复一日，如此安好~ 二零一九年三月二十五日 于岳麓山下","categories":[{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"}],"tags":[{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"}],"author":"CinKate"},{"title":"Keras踩坑总结","slug":"kerestricks","date":"2019-03-23T17:04:49.000Z","updated":"2020-05-17T16:11:59.830Z","comments":true,"path":"2019/03/24/kerestricks/","link":"","permalink":"http://renxingkai.github.io/2019/03/24/kerestricks/","excerpt":"","text":"转载自：链接 Keras 是一个用 Python 编写的高级神经网络 API，它能够以 TensorFlow, CNTK, 或者 Theano 作为后端运行。Keras 的开发重点是支持快速的实验。能够以最小的时间把你的想法转换为实验结果，是做好研究的关键。本人是keras的忠实粉丝，可能是因为它实在是太简单易用了，不用多少代码就可以将自己的想法完全实现，但是在使用的过程中还是遇到了不少坑，本文做了一个归纳，供大家参考。 Keras 兼容的 Python 版本: Python 2.7-3.6。 详细教程请参阅Keras官方中文文档 1、Keras输出的loss，val这些值如何保存到文本中去：Keras中的fit函数会返回一个History对象，它的History.history属性会把之前的那些值全保存在里面，如果有验证集的话，也包含了验证集的这些指标变化情况，具体写法： hist=model.fit(train_set_x,train_set_y,batch_size=256,shuffle=True,nb_epoch=nb_epoch,validation_split=0.1)with open(&apos;log_sgd_big_32.txt&apos;,&apos;w&apos;) as f: f.write(str(hist.history)) 我觉得保存之前的loss，val这些值还是比较重要的，在之后的调参过程中有时候还是需要之前loss的结果作为参考的，特别是你自己添加了一些自己的loss的情况下，但是这样的写法会使整个文本的取名比较乱，所以其实可以考虑使用Aetros的插件，Aetros网址，这是一个基于Keras的一个管理工具，可以可视化你的网络结构，中间卷积结果的可视化，以及保存你以往跑的所有结果，还是很方便的，就是有些不稳定，有时候会崩。。。 history对象包含两个重要属性：epoch：训练的轮数history：它是一个字典，包含val_loss,val_acc,loss,acc四个key。 2、关于训练集，验证集和测试集：其实一开始我也没搞清楚这个问题，拿着测试集当验证集用，其实验证集是从训练集中抽取出来用于调参的，而测试集是和训练集无交集的，用于测试所选参数用于该模型的效果的，这个还是不要弄错了。。。在Keras中，验证集的划分只要在fit函数里设置validation_split的值就好了，这个对应了取训练集中百分之几的数据出来当做验证集。但由于shuffle是在validation _split之后执行的，所以如果一开始训练集没有shuffle的话，有可能使验证集全是负样本。测试集的使用只要在evaluate函数里设置就好了。 print model.evaluate（test_set_x，test_set_y ,batch_size=256） 这里注意evaluate和fit函数的默认batch_size都是32，自己记得修改。 总结： 验证集是在fit的时候通过validation_split参数自己从训练集中划分出来的； 测试集需要专门的使用evaluate去进行评价。 3、关于优化方法使用的问题之学习率调整开始总会纠结哪个优化方法好用，但是最好的办法就是试，无数次尝试后不难发现，Sgd的这种学习率非自适应的优化方法，调整学习率和初始化的方法会使它的结果有很大不同，但是由于收敛确实不快，总感觉不是很方便，我觉得之前一直使用Sgd的原因一方面是因为优化方法不多，其次是用Sgd都能有这么好的结果，说明你网络该有多好啊。其他的Adam，Adade，RMSprop结果都差不多，Nadam因为是adam的动量添加的版本，在收敛效果上会更出色。所以如果对结果不满意的话，就把这些方法换着来一遍吧。 （1）方法一：通过LearningRateScheduler实现学习率调整有很多初学者人会好奇怎么使sgd的学习率动态的变化，其实Keras里有个反馈函数叫LearningRateScheduler，具体使用如下： #使学习率指数下降def step_decay(epoch): initial_lrate = 0.01 drop = 0.5 epochs_drop = 10.0 lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) return lratelrate = LearningRateScheduler(step_decay)sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)model.fit(train_set_x, train_set_y, validation_split=0.1, nb_epoch=200, batch_size=256, callbacks=[lrate]) （2）方式二：最直接的调整学习率方式当然也可以直接在sgd声明函数中修改参数来直接修改学习率，学习率变化如下图： sgd = SGD(lr=learning_rate, decay=learning_rate/nb_epoch, momentum=0.9, nesterov=True) 具体可以参考这篇文章Using Learning Rate Schedules for Deep Learning Models in Python with Keras 除此之外，还有一种学利率调整方式，即 （3）方法三：通过ReduceLROnPlateau调整学习率keras.callbacks.ReduceLROnPlateau(monitor=&apos;val_loss&apos;, factor=0.1, patience=10, verbose=0, mode=&apos;auto&apos;, epsilon=0.0001, cooldown=0, min_lr=0) 当评价指标不在提升时，减少学习率。当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在patience个epoch中看不到模型性能提升，则减少学习率 参数 monitor：被监测的量factor：每次减少学习率的因子，学习率将以lr = lr*factor的形式被减少patience：当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值触发学习率减少。在max模式下，当检测值不再上升则触发学习率减少。epsilon：阈值，用来确定是否进入检测值的“平原区”cooldown：学习率减少后，会经过cooldown个epoch才重新进行正常操作min_lr：学习率的下限 代码示例如下： from keras.callbacks import ReduceLROnPlateaureduce_lr = ReduceLROnPlateau(monitor=&apos;val_loss&apos;, patience=10, mode=&apos;auto&apos;)model.fit(train_x, train_y, batch_size=32, epochs=5, validation_split=0.1, callbacks=[reduce_lr]) 4、如何用 Keras 处理超过内存的数据集？你可以使用 model.train_on_batch(x，y) 和 model.test_on_batch(x，y) 进行批量训练与测试。请参阅 模型文档。 或者，你可以编写一个生成批处理训练数据的生成器，然后使用 model.fit_generator(data_generator，steps_per_epoch，epochs) 方法。 5、Batchnormalization层的放置问题：BN层是真的吊，简直神器，除了会使网络搭建的时间和每个epoch的时间延长一点之外，但是关于这个问题我看到了无数的说法，对于卷积和池化层的放法，又说放中间的，也有说池化层后面的，对于dropout层，有说放在它后面的，也有说放在它前面的，对于这个问题我的说法还是试！虽然麻烦。。。但是DL本来不就是一个偏工程性的学科吗。。。还有一点是需要注意的，就是BN层的参数问题，我一开始也没有注意到，仔细看BN层的参数： keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, axis=-1, momentum=0.9, weights=None, beta_init=&apos;zero&apos;, gamma_init=&apos;one&apos;) 参数mode：整数，指定规范化的模式，取0或10：按特征规范化，输入的各个特征图将独立被规范化。规范化的轴由参数axis指定。注意，如果输入是形如（samples，channels，rows，cols）的4D图像张量，则应设置规范化的轴为1，即沿着通道轴规范化。输入格式是‘tf’同理。1：按样本规范化，该模式默认输入为2D 我们大都使用的都是mode=0也就是按特征规范化，对于放置在卷积和池化之间或之后的4D张量，需要设置axis=1，而Dense层之后的BN层则直接使用默认值就好了。 6、在验证集的误差不再下降时，如何中断训练？你可以使用 EarlyStopping 回调： from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor=&apos;val_loss&apos;, patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) 总结：关于callbacks参数的妙用 （1）查询每隔epoch之后的loss和acc （2）通过LearningRateScheduler实现衰减学习率或自定义衰减学习率 （3）通过EarlyStopping实现中断训练 （4）我们还可以自己定义回调函数，所为回调函数其实就是在训练完每一个epoch之后我们希望实现的操作。 7.如何「冻结」网络层？「冻结」一个层意味着将其排除在训练之外，即其权重将永远不会更新。这在微调模型或使用固定的词向量进行文本输入中很有用。有两种方式实现： 方式一：在构造层的时候传递一个bool类型trainable参数，如下： frozen_layer = Dense(32, trainable=False) 您可以将 trainable 参数（布尔值）传递给一个层的构造器，以将该层设置为不可训练的： 方式二：通过层对象的trainable属性去设置，如下： x = Input(shape=(32,))layer = Dense(32) #构造一个层layer.trainable = False #设置层的trainable属性y = layer(x) 注意：可以在实例化之后将网络层的 trainable 属性设置为 True 或 False。为了使之生效，在修改 trainable 属性之后，需要在模型上调用 compile()。及重新编译模型。 8.如何从 Sequential 模型中移除一个层？你可以通过调用模型的 .pop() 来删除 Sequential 模型中最后添加的层： model = Sequential()model.add(Dense(32, activation=&apos;relu&apos;, input_dim=784))model.add(Dense(32, activation=&apos;relu&apos;))print(len(model.layers)) # &quot;2&quot;model.pop()print(len(model.layers)) # &quot;1&quot;","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"BIDAF代码阅读","slug":"bidaf","date":"2019-03-21T11:31:10.000Z","updated":"2020-05-17T16:11:59.217Z","comments":true,"path":"2019/03/21/bidaf/","link":"","permalink":"http://renxingkai.github.io/2019/03/21/bidaf/","excerpt":"","text":"1.assert的用法 ： 主要用于检查条件，不符合就终止程序a=-1#报错assert a&gt;0,&quot;a超出范围&quot;#正常运行assert a&lt;0 2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicodeimport codecs, sys# 用codecs提供的open方法来指定打开的文件的语言编码，它会在读 取的时候自动转换为内部unicode bfile = codecs.open( &quot; dddd.txt &quot; , &apos; r &apos; , &quot; big5 &quot; )# bfile = open(&quot;dddd.txt&quot;, &apos;r&apos;) ss = bfile.read()bfile.close()# 输出，这个时候看到的就是转换后的结果。如果使用语言内建的open函数 来打开文件，这里看到的必定是乱码 以下是prepro.py文件的代码阅读与分析：import spacyimport jsonfrom tqdm import tqdmfrom collections import Counterimport randomimport codecsimport numpy as npimport osimport tensorflow as tf#加载模型nlp=spacy.blank(&apos;en&apos;)#对句子进行分词def word_tokenize(sent): doc=nlp(sent) return [token.text for token in doc]#常用的word2idx#此处输出spans形式：[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)]#意为取出该词当前所在的位置，并且结束长度+当前长度#两者之差即为该单词长度def convert_idx(text,tokens): current=0 spans=[] for token in tokens: current=text.find(token,current) if current&lt;0: print(&apos;Token &#123;&#125; cannot be found!&apos;.format(token)) raise Exception() #[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)] #取出该词当前所在的位置，并且结束长度+当前长度 #两者之差即为该单词长度 spans.append((current,current+len(token))) current+=len(token) return spans#预处理文件def process_file(filename,data_type=None,word_counter=None,char_counter=None): print(&quot;Generating &#123;&#125; examples...&quot;.format(data_type)) examples = [] eval_examples = &#123;&#125; total=0 with open(filename,&apos;r&apos;) as fh: source=json.load(fh) print(len(source[&apos;data&apos;])) #遍历每篇文章dev有48篇文章 for article in tqdm(source[&quot;data&quot;]): #遍历每篇文章的段落 for para in article[&apos;paragraphs&apos;]: #替换段落中的&apos;&apos;和`` context = para[&quot;context&quot;].replace(&quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #并对段落进行分词,分词中还是带了标点和特殊符号，需要后面进行处理 context_tokens=word_tokenize(context) #[&apos;The&apos;, &apos;connection&apos;, &apos;between&apos;, &apos;macroscopic&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;and&apos;, &apos;microscopic&apos;, &apos;conservative&apos;, &apos;forces&apos;, &apos;is&apos;, &apos;described&apos;, &apos;by&apos;, &apos;detailed&apos;, &apos;treatment&apos;, &apos;with&apos;, &apos;statistical&apos;, &apos;mechanics&apos;, &apos;.&apos;, &apos;In&apos;, &apos;macroscopic&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;act&apos;, &apos;to&apos;, &apos;change&apos;, &apos;the&apos;, &apos;internal&apos;, &apos;energies&apos;, &apos;of&apos;, &apos;the&apos;, &apos;system&apos;, &apos;,&apos;, &apos;and&apos;, &apos;are&apos;, &apos;often&apos;, &apos;associated&apos;, &apos;with&apos;, &apos;the&apos;, &apos;transfer&apos;, &apos;of&apos;, &apos;heat&apos;, &apos;.&apos;, &apos;According&apos;, &apos;to&apos;, &apos;the&apos;, &apos;Second&apos;, &apos;law&apos;, &apos;of&apos;, &apos;thermodynamics&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;necessarily&apos;, &apos;result&apos;, &apos;in&apos;, &apos;energy&apos;, &apos;transformations&apos;, &apos;within&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;from&apos;, &apos;ordered&apos;, &apos;to&apos;, &apos;more&apos;, &apos;random&apos;, &apos;conditions&apos;, &apos;as&apos;, &apos;entropy&apos;, &apos;increases&apos;, &apos;.&apos;] #获取每个单词的字符表示 context_chars = [list(token) for token in context_tokens] #word2idx 每个词开始的位置和结束的位置 spans = convert_idx(context, context_tokens) for token in context_tokens: #这儿加的是每个qas的长度？？ word_counter[token] += len(para[&quot;qas&quot;]) for char in token: #每个单词的字符这儿也加的是每个qas的长度 #Counter(&#123;&apos;e&apos;: 28293, &apos;a&apos;: 19610, &apos;n&apos;: 17317, &apos;t&apos;: 17071, &apos;r&apos;: 15443, &apos;o&apos;: 15358, &apos;i&apos;: # 14669, &apos;s&apos;: 14081, &apos;h&apos;: 11839, &apos;l&apos;: 9031, &apos;d&apos;: 8982, &apos;c&apos;: 6540, &apos;u&apos;: 5885, # &apos;w&apos;: 5806, &apos;f&apos;: 4516, &apos;g&apos;: 4463, &apos;p&apos;: 4372, &apos;m&apos;: 4165, &apos;,&apos;: 3116, &apos;y&apos;: 2842, &apos;b&apos;: 2321, &apos;v&apos;: 2152, # &apos;.&apos;: 2057, &apos;B&apos;: 2052, &apos;S&apos;: 1832, &apos;1&apos;: 1776, &apos;k&apos;: 1553, &apos;0&apos;: 1168, &apos;C&apos;: 1107, &apos;F&apos;: 963, &apos;T&apos;: 876, # &apos;2&apos;: 856, &apos;P&apos;: 836, &apos;I&apos;: 819, &apos;5&apos;: 798, &apos;N&apos;: 766, &apos;L&apos;: 741, &apos;X&apos;: 714, &apos;M&apos;: 672, &apos;4&apos;: 662, &apos;3&apos;: 636, # &apos;A&apos;: 619, &apos;9&apos;: 584, &quot;&apos;&quot;: 552, &apos;-&apos;: 523, &apos;7&apos;: 488, &apos;D&apos;: 470, &apos;–&apos;: 415, &apos;(&apos;: 412, &apos;)&apos;: 412, &apos;8&apos;: 380, # &apos;6&apos;: 371, &apos;V&apos;: 352, &apos;O&apos;: 272, &apos;J&apos;: 268, &apos;j&apos;: 249, &apos;q&apos;: 235, &apos;&quot;&apos;: 222, &apos;G&apos;: 221, &apos;x&apos;: 220, &apos;E&apos;: 177, # &apos;R&apos;: 173, &apos;W&apos;: 168, &apos;K&apos;: 159, &apos;H&apos;: 117, &apos;U&apos;: 108, &apos;z&apos;: 107, &apos;½&apos;: 81, &apos;:&apos;: 81, &apos;;&apos;: 63, &apos;$&apos;: 49, &apos;#&apos;: 30, # &apos;é&apos;: 26, &apos;/&apos;: 21, &apos;Q&apos;: 15&#125;) char_counter[char] += len(para[&quot;qas&quot;]) #遍历qas for qa in para[&quot;qas&quot;]: total += 1 #替换问题&apos;&apos; `` ques = qa[&quot;question&quot;].replace( &quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #对问题进行分词 ques_tokens = word_tokenize(ques) #取出问题中的字符 ques_chars = [list(token) for token in ques_tokens] #遍历问题每个词 for token in ques_tokens: #此处真的正确 word_counter[token] += 1 for char in token: char_counter[char] += 1 y1s, y2s = [], [] answer_texts = [] #遍历答案文本 for answer in qa[&quot;answers&quot;]: #答案文本 answer_text = answer[&quot;text&quot;] #开始位置 answer_start = answer[&apos;answer_start&apos;] answer_end = answer_start + len(answer_text) answer_texts.append(answer_text) answer_span = [] #加入答案span answer_span for idx, span in enumerate(spans): if not (answer_end &lt;= span[0] or answer_start &gt;= span[1]): answer_span.append(idx) y1, y2 = answer_span[0], answer_span[-1] y1s.append(y1) y2s.append(y2) example = &#123;&quot;context_tokens&quot;: context_tokens, &quot;context_chars&quot;: context_chars, &quot;ques_tokens&quot;: ques_tokens, &quot;ques_chars&quot;: ques_chars, &quot;y1s&quot;: y1s, &quot;y2s&quot;: y2s, &quot;id&quot;: total&#125; examples.append(example) #未分词结果 eval_examples[str(total)] = &#123; &quot;context&quot;: context, &quot;spans&quot;: spans, &quot;answers&quot;: answer_texts, &quot;uuid&quot;: qa[&quot;id&quot;]&#125; random.shuffle(examples) print(&quot;&#123;&#125; questions in total&quot;.format(len(examples))) return examples, eval_examples#获取词向量def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None): print(&quot;Generating &#123;&#125; embedding...&quot;.format(data_type)) embedding_dict=&#123;&#125; #过滤掉低频词，仅取出频率较高的词 filtered_elements=[k for k,v in counter.items() if v&gt;limit] #判断词向量文件是否为空 if emb_file is not None: assert size is not None#如果size为空直接退出程序 assert vec_size is not None#如果vec_size为空直接退出程序 #读取词向量 with codecs.open(emb_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as fh: # 依次遍历词向量每一行 for line in tqdm(fh, total=size): #分开词和向量 array = line.split() #取出开头的单词 word=&quot;&quot;.join(array[0:-vec_size]) #取出单词对应的词向量 vector=list(map(float,array[-vec_size:])) #词向量的单词在counter单词中，并且 在文本中的单词数目&gt;limit if word in counter and counter[word]&gt;limit: embedding_dict[word]=vector print(&quot;&#123;&#125; / &#123;&#125; tokens have corresponding &#123;&#125; embedding vector&quot;.format( len(embedding_dict), len(filtered_elements), data_type)) #如果词向量文件为空 else: assert vec_size is not None #遍历所有过滤的词 for token in filtered_elements: #对每个单词进行随机初始化向量 embedding_dict[token]=[np.random.normal(scale=0.01) for _ in range(vec_size)] print(&quot;&#123;&#125; tokens have corresponding embedding vector&quot;.format( len(filtered_elements))) #处理OOV词 NULL = &quot;--NULL--&quot; OOV = &quot;--OOV--&quot; #从下标2索引开始，过滤掉NULL和OOV 创建token2_idx_dict token2idx_dict=&#123;token:idx for idx,token in enumerate(embedding_dict.keys(),2)&#125; if token2idx_dict is None else token2idx_dict #NULL OOV 设置token2idx token2idx_dict[NULL] = 0 token2idx_dict[OOV] = 1 #NULL OOV设置embedding_dict embedding_dict[NULL] = [0. for _ in range(vec_size)] embedding_dict[OOV] = [0. for _ in range(vec_size)] #id2embedding 单词id对应的词向量 id2emb_dict=&#123;idx:embedding_dict[token] for token,idx in token2idx_dict.items() &#125; #获取词向量矩阵 emb_mat=[id2emb_dict[idx] for idx in range(id2emb_dict)] #仅返回 词向量矩阵，token2idx_dict return emb_mat, token2idx_dict#构建文本特征question paragraph answer and so ondef build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False): #文章长度 para_limit=config.test_para_limit if is_test else config.para_limit #问题长度 ques_limit = config.test_ques_limit if is_test else config.ques_limit #字符限制长度 char_limit = config.char_limit #过滤文章和问题长度函数 def filter_func(example, is_test=False): return len(example[&quot;context_tokens&quot;]) &gt; para_limit or len(example[&quot;ques_tokens&quot;]) &gt; ques_limit print(&quot;Processing &#123;&#125; examples...&quot;.format(data_type)) writer = tf.python_io.TFRecordWriter(out_file) total = 0 total_ = 0 meta = &#123;&#125; #处理文章 for example in tqdm(examples): total_+=1 #过滤长度大于限制值的文章 if filter_func(example, is_test): continue total += 1 #段落ids context_idxs = np.zeros([para_limit], dtype=np.int32) #段落id char对应的矩阵 context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32) ##问题ids ques_idxs = np.zeros([ques_limit], dtype=np.int32) ##问题id char对应的矩阵 ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32) #段落长度 y1 = np.zeros([para_limit], dtype=np.float32) y2 = np.zeros([para_limit], dtype=np.float32) #获取单词 def _get_word(word): for each in (word, word.lower(), word.capitalize(), word.upper()): if each in word2idx_dict: #返回每个单词对应的id return word2idx_dict[each] return 1 #获取字符 def _get_char(char): if char in char2idx_dict: # 返回每个字符对应的id return char2idx_dict[char] return 1 #为每个文章内容获取对应的ids context_tokens为已经分好词的文章 for i, token in enumerate(example[&quot;context_tokens&quot;]): context_idxs[i] = _get_word(token) # 为每个问题内容获取对应的ids ques_tokens为已经分好词的问题 for i, token in enumerate(example[&quot;ques_tokens&quot;]): ques_idxs[i] = _get_word(token) # 为每个文章内容获取对应的chars for i, token in enumerate(example[&quot;context_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break #赋值char 不够的用0填充 context_char_idxs[i, j] = _get_char(char) # 为每个问题内容获取对应的chars for i, token in enumerate(example[&quot;ques_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break # 赋值char 不够的用0填充 ques_char_idxs[i, j] = _get_char(char) #开始，结束位置 start, end = example[&quot;y1s&quot;][-1], example[&quot;y2s&quot;][-1] y1[start], y2[end] = 1.0, 1.0 #构建tensorflow 记录 record = tf.train.Example(features=tf.train.Features(feature=&#123; &quot;context_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])), &quot;ques_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])), &quot;context_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])), &quot;ques_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])), &quot;y1&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])), &quot;y2&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])), &quot;id&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[example[&quot;id&quot;]])) &#125;)) writer.write(record.SerializeToString()) print(&quot;Build &#123;&#125; / &#123;&#125; instances of features in total&quot;.format(total, total_)) meta[&quot;total&quot;] = total writer.close() return meta#保存文件def save(filename, obj, message=None): if message is not None: print(&quot;Saving &#123;&#125;...&quot;.format(message)) with open(filename, &quot;w&quot;) as fh: json.dump(obj, fh)# 预处理文件def prepro(config): #单词，字符计数器 word_counter, char_counter = Counter(), Counter() #处理训练集 train_examples, train_eval = process_file( config.train_file, &quot;train&quot;, word_counter, char_counter) #处理验证集 dev_examples, dev_eval = process_file( config.dev_file, &quot;dev&quot;, word_counter, char_counter) #处理测试集 test_examples, test_eval = process_file( config.test_file, &quot;test&quot;, word_counter, char_counter) #词向量文件 word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file #字符向量文件 char_emb_file = config.glove_char_file if config.pretrained_char else None #字符向量大小 char_emb_size = config.glove_char_size if config.pretrained_char else None #字符向量维度 char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim #word2idx字典 word2idx_dict = None #如果存在word2idx字典 则直接导入 if os.path.isfile(config.word2idx_file): with open(config.word2idx_file, &quot;r&quot;) as fh: word2idx_dict = json.load(fh) #构建词向量矩阵 word_emb_mat, word2idx_dict = get_embedding(word_counter, &quot;word&quot;, emb_file=word_emb_file, size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict) #构建字符向量矩阵 char2idx_dict = None # 如果存在char2idx字典 则直接导入 if os.path.isfile(config.char2idx_file): with open(config.char2idx_file, &quot;r&quot;) as fh: char2idx_dict = json.load(fh) # 构建字符向量矩阵 char_emb_mat, char2idx_dict = get_embedding( char_counter, &quot;char&quot;, emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim, token2idx_dict=char2idx_dict) #对训练集、验证集、测试集构建特征 build_features(config, train_examples, &quot;train&quot;, config.train_record_file, word2idx_dict, char2idx_dict) dev_meta = build_features(config, dev_examples, &quot;dev&quot;, config.dev_record_file, word2idx_dict, char2idx_dict) test_meta = build_features(config, test_examples, &quot;test&quot;, config.test_record_file, word2idx_dict, char2idx_dict, is_test=True) #对预处理的文件进行保存 save(config.word_emb_file, word_emb_mat, message=&quot;word embedding&quot;) save(config.char_emb_file, char_emb_mat, message=&quot;char embedding&quot;) save(config.train_eval_file, train_eval, message=&quot;train eval&quot;) save(config.dev_eval_file, dev_eval, message=&quot;dev eval&quot;) save(config.test_eval_file, test_eval, message=&quot;test eval&quot;) save(config.dev_meta, dev_meta, message=&quot;dev meta&quot;) save(config.word2idx_file, word2idx_dict, message=&quot;word2idx&quot;) save(config.char2idx_file, char2idx_dict, message=&quot;char2idx&quot;) save(config.test_meta, test_meta, message=&quot;test meta&quot;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"神经网络调参的一些tips","slug":"nntuningparameter","date":"2019-03-19T20:10:50.000Z","updated":"2020-05-17T16:12:00.003Z","comments":true,"path":"2019/03/20/nntuningparameter/","link":"","permalink":"http://renxingkai.github.io/2019/03/20/nntuningparameter/","excerpt":"","text":"参考建议调整超参数思想为控制变量法，并且按照学习率、批处理大小和隐藏层设计的顺序进行。以下是一些建议： 使用一个规模比较小的数据集一般都会把数据集分为训练集、测试集、验证集三份，训练集和验证集也被称为开发数据集，有的数据集不设验证集，是因为数据量小，通常可以用训练集调整超参数。 如果有验证集，验证集的数据不宜过多，因为数据越多，越需要多次迭代才能看到超参数的效果，所需要的时间就越长，在寻找一组最佳参数阶段，需要比较不同参数下损失变化的曲线和精度的值。 调整学习率控制其他超参数不变，改变学习率，比如从0.0001开始，顺序选择0.001、0.01、0.005、0.1和0.5，然后比较在不同的学习率下损失函数的曲线增长或减小的幅度，我们可以找到一个区间，也就是在这个区间内，损失函数的波形是稳定下降的，不会发生振荡，则取这个区间最小的值就可以。 调整batch_size控制其他超参数不变，改变批处理大小，可以依次选择20、50、100、200，然后比较在不同的批处理大小下能使准确率变化最陡的值，准确率变化越陡，证明参数学习收敛越快。 调整隐藏层设计控制其他超参数不变，改变隐藏层的层数或每层神经元的多少，选择能获得最高准确率的值。 超参数的调整主要还是需要自己做大量的实验，得出较好的“经验”，这样调起来会更得心应手一些~","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"}],"author":"CinKate"},{"title":"第一篇博文","slug":"firstpage","date":"2019-03-19T14:30:40.000Z","updated":"2020-05-17T16:11:59.814Z","comments":true,"path":"2019/03/19/firstpage/","link":"","permalink":"http://renxingkai.github.io/2019/03/19/firstpage/","excerpt":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~","text":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~ 分享一首自己很喜欢的词： 扬州慢·淮左名都宋代：姜夔淳熙丙申至日，予过维扬。夜雪初霁，荠麦弥望。入其城，则四顾萧条，寒水自碧，暮色渐起，戍角悲吟。予怀怆然，感慨今昔，因自度此曲。千岩老人以为有“黍离”之悲也。 淮左名都，竹西佳处，解鞍少驻初程。过春风十里。尽荠麦青青。自胡马窥江去后，废池乔木，犹厌言兵。渐黄昏，清角吹寒。都在空城。杜郎俊赏，算而今、重到须惊。纵豆蔻词工，青楼梦好，难赋深情。二十四桥仍在，波心荡、冷月无声。念桥边红药，年年知为谁生。 接下来的日子，锻炼自己的耐力，Always learn from the dalao~坐看天边云卷云舒~","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}],"author":"CinKate"}],"categories":[{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"},{"name":"语义搜索","slug":"语义搜索","permalink":"http://renxingkai.github.io/categories/语义搜索/"},{"name":"预训练语言模型","slug":"预训练语言模型","permalink":"http://renxingkai.github.io/categories/预训练语言模型/"},{"name":"shell","slug":"shell","permalink":"http://renxingkai.github.io/categories/shell/"},{"name":"C++","slug":"C","permalink":"http://renxingkai.github.io/categories/C/"},{"name":"搜索NLP","slug":"搜索NLP","permalink":"http://renxingkai.github.io/categories/搜索NLP/"},{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"},{"name":"比赛","slug":"比赛","permalink":"http://renxingkai.github.io/categories/比赛/"},{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"},{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"},{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"},{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"},{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"},{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"},{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"排序 - 推荐系统","slug":"排序-推荐系统","permalink":"http://renxingkai.github.io/tags/排序-推荐系统/"},{"name":"搜索NLP","slug":"搜索NLP","permalink":"http://renxingkai.github.io/tags/搜索NLP/"},{"name":"序列标注","slug":"序列标注","permalink":"http://renxingkai.github.io/tags/序列标注/"},{"name":"BERT","slug":"BERT","permalink":"http://renxingkai.github.io/tags/BERT/"},{"name":"搜索相关性","slug":"搜索相关性","permalink":"http://renxingkai.github.io/tags/搜索相关性/"},{"name":"shell","slug":"shell","permalink":"http://renxingkai.github.io/tags/shell/"},{"name":"linux","slug":"linux","permalink":"http://renxingkai.github.io/tags/linux/"},{"name":"程序内存模型","slug":"程序内存模型","permalink":"http://renxingkai.github.io/tags/程序内存模型/"},{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"},{"name":"法研杯2021MRC","slug":"法研杯2021MRC","permalink":"http://renxingkai.github.io/tags/法研杯2021MRC/"},{"name":"MRC NQ","slug":"MRC-NQ","permalink":"http://renxingkai.github.io/tags/MRC-NQ/"},{"name":"MTL MRC","slug":"MTL-MRC","permalink":"http://renxingkai.github.io/tags/MTL-MRC/"},{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"},{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"},{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"},{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"},{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"},{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"},{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"},{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"},{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"},{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"},{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}]}