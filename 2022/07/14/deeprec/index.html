<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="推荐系统排序模型-从LR到XXXX"><meta name="keywords" content="排序 - 推荐系统"><meta name="author" content="CinKate"><meta name="copyright" content="CinKate"><title>推荐系统排序模型-从LR到XXXX | CinKate's Blogs</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b77222dd6b9929f160b8a04fc8705337";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '3.9.0'
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-LR-逻辑回归"><span class="toc-number">1.</span> <span class="toc-text">1. LR-逻辑回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-POLY2-特征交叉的开始"><span class="toc-number">2.</span> <span class="toc-text">2.POLY2-特征交叉的开始</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-FM-隐向量特征交叉"><span class="toc-number">3.</span> <span class="toc-text">3.FM-隐向量特征交叉</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-DeppFM"><span class="toc-number">4.</span> <span class="toc-text">4.DeppFM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Sparse-Feature"><span class="toc-number">4.0.1.</span> <span class="toc-text">4.1 Sparse Feature</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Dense-Embeddings"><span class="toc-number">4.0.2.</span> <span class="toc-text">4.2 Dense Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-FM-Layer"><span class="toc-number">4.0.3.</span> <span class="toc-text">4.3 FM Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Hidden-Layer"><span class="toc-number">4.0.4.</span> <span class="toc-text">4.4 Hidden Layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-Output-Units"><span class="toc-number">4.0.5.</span> <span class="toc-text">4.5 Output Units</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考链接"><span class="toc-number">4.1.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://imgsrc.baidu.com/forum/w%3D580/sign=f8de0e9b3e87e9504217f3642039531b/1c3bd133c895d143e395e57b77f082025baf0726.jpg"></div><div class="author-info__name text-center">CinKate</div><div class="author-info__description text-center">长笛一声人倚楼~</div><div class="follow-button"><a href="https://github.com/renxingkai">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">48</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">31</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">17</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CinKate's Blogs</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">推荐系统排序模型-从LR到XXXX</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-07-13</time><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.3k</span><span class="post-meta__separator">|</span><span>Reading time: 9 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>平时会用到不少的排序模型，但是一直没有系统化总结，今天还是认真总结下，如有错误，求大佬们不吝指出。</p>
<h1 id="1-LR-逻辑回归"><a href="#1-LR-逻辑回归" class="headerlink" title="1. LR-逻辑回归"></a>1. LR-逻辑回归</h1><p>逻辑回归通常对输入特征如用户年龄、性别、item属性、描述等进行变换，然后输入到模型中，通常训练目标为是否点击。在推理阶段，将同样的特征输入到模型中，模型预测出点击概率，最终经过排序得到推荐item的列表。</p>
<p>逻辑回归核心为sigmoid函数，wx输入到sigmoid函数中，通常使用梯度下降算法来更新参数w。</p>
<p>逻辑回归的优点：</p>
<ul>
<li>强数学含义支撑。LR属于广义线性模型的一种。</li>
<li>可解释性强。从公式层面来看，LR数学形式就是不同特征之间的加权之和，最终过一个sigmoid函数，将输出值限制在0到1之间。根据不同特征的权重，可以明显观察到哪些特征重要。</li>
<li>工程实现容易。</li>
</ul>
<p>逻辑回归的缺点：</p>
<ul>
<li>表示能力不足。</li>
<li>无法进行特征交叉，学习高阶特征。</li>
</ul>
<h1 id="2-POLY2-特征交叉的开始"><a href="#2-POLY2-特征交叉的开始" class="headerlink" title="2.POLY2-特征交叉的开始"></a>2.POLY2-特征交叉的开始</h1><p>LR存在无法自动进行特征交叉的问题，最容易想到的是人工构造交叉组合特征。公式如下：</p>
<p><code>$ POLY2(w,x)=\sum_{j_1=i}^{n-1} \sum_{j_2=j_1+1}^{n} w_{h(j_1,j_2)}x_{j_1}x_{j_2}$</code></p>
<p>可以看到，该方法对所有特征均进行交叉，并对特征组合赋予权重<br><code>$ w_{h(j_1,j_2)} $</code>，一定程度解决了特征交叉问题，但是本质上还是对于不同特征加权求和的线性模型。</p>
<p>POLY2缺点：</p>
<ul>
<li>交叉特征容易出现极度稀疏问题。使用one-hot编码类别特征之后，容易出现稀疏特征问题。</li>
<li>权重参数由n上升到n2，增加训练复杂度。</li>
</ul>
<h1 id="3-FM-隐向量特征交叉"><a href="#3-FM-隐向量特征交叉" class="headerlink" title="3.FM-隐向量特征交叉"></a>3.FM-隐向量特征交叉</h1><p>FM的主要优点是解决稀疏数据下的特征组合问题。<br>原始的FM公式为：</p>
<p><code>$ FM=w_0+\sum_{i=1}^n w_ix_i+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{ij}x_ix_j$</code></p>
<p>前两项其实就是一阶加权特征，计算复杂度为O(n)，第三项中的权重<code>$w_{ij}$</code>，这儿使用到了矩阵分解，分解为 <code>$ W=V^TV $</code>, vi、vj分别为xi、xj的隐向量<br><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20220712231715.png" alt></p>
<p>于是，原始公式变为了：</p>
<p><code>$ FM=w_0+\sum_{i=1}^n w_ix_i+\sum_{i=1}^{n} \sum_{j=i+1}^{n} &lt;v_i,v_j&gt;x_ix_j$</code></p>
<p>我们假设隐向量的长度为k ，那么交叉项的参数量变为 kn 个。此时时间复杂度仍为O(kn^2)，通过以下方式可以简化为O(kn)，如下图：<br><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20220712232204.png" alt></p>
<p>附上核心代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class FeaturesLinear(torch.nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, field_dims, output_dim=1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)</span><br><span class="line">        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))</span><br><span class="line">        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param x: Long tensor of size ``(batch_size, num_fields)``</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        x = x + x.new_tensor(self.offsets).unsqueeze(0)</span><br><span class="line">        return torch.sum(self.fc(x), dim=1) + self.bias</span><br><span class="line"></span><br><span class="line">class FeaturesEmbedding(torch.nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, field_dims, embed_dim):</span><br><span class="line">        super().__init__()</span><br><span class="line">        #sum(field_dims) 是为了把所有的field结合在一起</span><br><span class="line">        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)</span><br><span class="line">        # 这里为什么取[:-1]? 这样0-9992 的embedding表中</span><br><span class="line">        # 0-6040是对应user，6041-9992对应item</span><br><span class="line">        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)</span><br><span class="line">        torch.nn.init.xavier_uniform_(self.embedding.weight.data)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param x: Long tensor of size ``(batch_size, num_fields)``</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        x = x + x.new_tensor(self.offsets).unsqueeze(0)</span><br><span class="line">        return self.embedding(x) # output size (batch_size, num_fields, embed_dim)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">class FactorizationMachine(torch.nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, reduce_sum=True):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.reduce_sum = reduce_sum</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        square_of_sum = torch.sum(x, dim=1) ** 2</span><br><span class="line">        sum_of_square = torch.sum(x ** 2, dim=1)</span><br><span class="line">        ix = square_of_sum - sum_of_square</span><br><span class="line">        if self.reduce_sum:</span><br><span class="line">            ix = torch.sum(ix, dim=1, keepdim=True)</span><br><span class="line">        return 0.5 * ix</span><br><span class="line">class FactorizationMachineModel(torch.nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    A pytorch implementation of Factorization Machine.</span><br><span class="line"></span><br><span class="line">    Reference:</span><br><span class="line">        S Rendle, Factorization Machines, 2010.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, field_dims, embed_dim):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = FeaturesEmbedding(field_dims, embed_dim)</span><br><span class="line">        self.linear = FeaturesLinear(field_dims)</span><br><span class="line">        self.fm = FactorizationMachine(reduce_sum=True)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param x: Long tensor of size ``(batch_size, num_fields)``</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        x = self.linear(x) + self.fm(self.embedding(x))</span><br><span class="line">        return torch.sigmoid(x.squeeze(1))</span><br></pre></td></tr></table></figure>
<h1 id="4-DeppFM"><a href="#4-DeppFM" class="headerlink" title="4.DeppFM"></a>4.DeppFM</h1><p>顾名思义，DeepFM是Deep与FM结合的产物，</p>
<p><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20220713221459.png" alt></p>
<h3 id="4-1-Sparse-Feature"><a href="#4-1-Sparse-Feature" class="headerlink" title="4.1 Sparse Feature"></a>4.1 Sparse Feature</h3><p>Sparse Feature是指离散型变量。比如现在我有数据：xx公司每个员工的姓名、年龄、岗位、收入的表格，那么年龄和岗位就属于离散型变量，而收入则称为连续型变量。这从字面意思也能够理解。</p>
<p>Sparse Feature框里表示的是将每个特征经过one-hot编码后拼接在一起的稀疏长向量，黄色的点表示某对象在该特征的取值中属于该位置的值。</p>
<h3 id="4-2-Dense-Embeddings"><a href="#4-2-Dense-Embeddings" class="headerlink" title="4.2 Dense Embeddings"></a>4.2 Dense Embeddings</h3><p>该层为嵌入层，用于对高维稀疏的 01 向量做嵌入，得到低维稠密的向量 e (每个01向量对应自己的嵌入层，不同向量的嵌入过程相互独立，如上图所示）。然后将每个稠密向量横向拼接，在拼接上原始的数值特征，然后作为 Deep 与 FM 的输入。</p>
<p>最终输入模型的值如下图，Sparse Feature经过embedding之后，与归一化后的连续特征拼接，一起输入模型<br><img src="https://rxk-1300064984.cos.ap-nanjing.myqcloud.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20220713222246.png" alt></p>
<h3 id="4-3-FM-Layer"><a href="#4-3-FM-Layer" class="headerlink" title="4.3 FM Layer"></a>4.3 FM Layer</h3><p>线性部分 (黑色线段) 是给与每个特征一个权重，然后进行加权和；交叉部分 (红色线段) 是对特征进行两两相乘，然后赋予权重加权求和。然后将两部分结果累加在一起即为 FM Layer 的输出。</p>
<h3 id="4-4-Hidden-Layer"><a href="#4-4-Hidden-Layer" class="headerlink" title="4.4 Hidden Layer"></a>4.4 Hidden Layer</h3><p>Deep 部分的输入 为所有稠密向量的横向拼接，然后经过多层线性映射+非线性转换得到 Hidden Layer 的输出，一般会映射到1维，因为需要与 FM 的结果进行累加。</p>
<h3 id="4-5-Output-Units"><a href="#4-5-Output-Units" class="headerlink" title="4.5 Output Units"></a>4.5 Output Units</h3><p><code>$ DeepFM=sigmoid(y_{FM}+y_{DNN})$</code></p>
<p>输出层为 FM Layer 的结果与 Hidden Layer 结果的累加，低阶与高阶特征交互的融合，然后经过 sigmoid 非线性转换，得到预测的概率输出。</p>
<p>优点：</p>
<ul>
<li>两部分联合训练，无需加入人工特征，更易部署；</li>
<li>结构简单，复杂度低，两部分共享输入，共享信息，可更精确的训练学习。</li>
</ul>
<p>缺点：</p>
<ul>
<li>将类别特征对应的稠密向量拼接作为输入，然后对元素进行两两交叉。这样导致模型无法意识到域的概念，FM 与 Deep 两部分都不会考虑到域，属于同一个域的元素应该对应同样的计算。</li>
</ul>
<p>最后上核心代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class DeepFM(nn.Module):</span><br><span class="line">    def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8, </span><br><span class="line">                 hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]): </span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        cate_fea_nuniqs: 类别特征的唯一值个数列表，也就是每个类别特征的vocab_size所组成的列表</span><br><span class="line">        nume_fea_size: 数值特征的个数，该模型会考虑到输入全为类别型，即没有数值特征的情况 </span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.cate_fea_size = len(cate_fea_nuniqs)</span><br><span class="line">        self.nume_fea_size = nume_fea_size</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;FM部分&quot;&quot;&quot;</span><br><span class="line">        # 一阶</span><br><span class="line">        if self.nume_fea_size != 0:</span><br><span class="line">            self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1)  # 数值特征的一阶表示</span><br><span class="line">        self.fm_1st_order_sparse_emb = nn.ModuleList([</span><br><span class="line">            nn.Embedding(voc_size, 1) for voc_size in cate_fea_nuniqs])  # 类别特征的一阶表示</span><br><span class="line">        </span><br><span class="line">        # 二阶</span><br><span class="line">        self.fm_2nd_order_sparse_emb = nn.ModuleList([</span><br><span class="line">            nn.Embedding(voc_size, emb_size) for voc_size in cate_fea_nuniqs])  # 类别特征的二阶表示</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;DNN部分&quot;&quot;&quot;</span><br><span class="line">        self.all_dims = [self.cate_fea_size * emb_size] + hid_dims</span><br><span class="line">        self.dense_linear = nn.Linear(self.nume_fea_size, self.cate_fea_size * emb_size)  # 数值特征的维度变换到FM输出维度一致</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        # for DNN </span><br><span class="line">        for i in range(1, len(self.all_dims)):</span><br><span class="line">            setattr(self, &apos;linear_&apos;+str(i), nn.Linear(self.all_dims[i-1], self.all_dims[i]))</span><br><span class="line">            setattr(self, &apos;batchNorm_&apos; + str(i), nn.BatchNorm1d(self.all_dims[i]))</span><br><span class="line">            setattr(self, &apos;activation_&apos; + str(i), nn.ReLU())</span><br><span class="line">            setattr(self, &apos;dropout_&apos;+str(i), nn.Dropout(dropout[i-1]))</span><br><span class="line">        # for output </span><br><span class="line">        self.dnn_linear = nn.Linear(hid_dims[-1], num_classes)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        </span><br><span class="line">    def forward(self, X_sparse, X_dense=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        X_sparse: 类别型特征输入  [bs, cate_fea_size]</span><br><span class="line">        X_dense: 数值型特征输入（可能没有）  [bs, dense_fea_size]</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;FM 一阶部分&quot;&quot;&quot;</span><br><span class="line">        fm_1st_sparse_res = [emb(X_sparse[:, i].unsqueeze(1)).view(-1, 1) </span><br><span class="line">                             for i, emb in enumerate(self.fm_1st_order_sparse_emb)]</span><br><span class="line">        fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim=1)  # [bs, cate_fea_size]</span><br><span class="line">        fm_1st_sparse_res = torch.sum(fm_1st_sparse_res, 1,  keepdim=True)  # [bs, 1]</span><br><span class="line">        </span><br><span class="line">        if X_dense is not None:</span><br><span class="line">            fm_1st_dense_res = self.fm_1st_order_dense(X_dense) </span><br><span class="line">            fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res</span><br><span class="line">        else:</span><br><span class="line">            fm_1st_part = fm_1st_sparse_res   # [bs, 1]</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;FM 二阶部分&quot;&quot;&quot;</span><br><span class="line">        fm_2nd_order_res = [emb(X_sparse[:, i].unsqueeze(1)) for i, emb in enumerate(self.fm_2nd_order_sparse_emb)]</span><br><span class="line">        fm_2nd_concat_1d = torch.cat(fm_2nd_order_res, dim=1)  # [bs, n, emb_size]  n为类别型特征个数(cate_fea_size)</span><br><span class="line">        </span><br><span class="line">        # 先求和再平方</span><br><span class="line">        sum_embed = torch.sum(fm_2nd_concat_1d, 1)  # [bs, emb_size]</span><br><span class="line">        square_sum_embed = sum_embed * sum_embed    # [bs, emb_size]</span><br><span class="line">        # 先平方再求和</span><br><span class="line">        square_embed = fm_2nd_concat_1d * fm_2nd_concat_1d  # [bs, n, emb_size]</span><br><span class="line">        sum_square_embed = torch.sum(square_embed, 1)  # [bs, emb_size]</span><br><span class="line">        # 相减除以2 </span><br><span class="line">        sub = square_sum_embed - sum_square_embed  </span><br><span class="line">        sub = sub * 0.5   # [bs, emb_size]</span><br><span class="line">        </span><br><span class="line">        fm_2nd_part = torch.sum(sub, 1, keepdim=True)   # [bs, 1]</span><br><span class="line">        </span><br><span class="line">        &quot;&quot;&quot;DNN部分&quot;&quot;&quot;</span><br><span class="line">        dnn_out = torch.flatten(fm_2nd_concat_1d, 1)   # [bs, n * emb_size]</span><br><span class="line">        </span><br><span class="line">        if X_dense is not None:</span><br><span class="line">            dense_out = self.relu(self.dense_linear(X_dense))   # [bs, n * emb_size]</span><br><span class="line">            dnn_out = dnn_out + dense_out   # [bs, n * emb_size]</span><br><span class="line">        </span><br><span class="line">        for i in range(1, len(self.all_dims)):</span><br><span class="line">            dnn_out = getattr(self, &apos;linear_&apos; + str(i))(dnn_out)</span><br><span class="line">            dnn_out = getattr(self, &apos;batchNorm_&apos; + str(i))(dnn_out)</span><br><span class="line">            dnn_out = getattr(self, &apos;activation_&apos; + str(i))(dnn_out)</span><br><span class="line">            dnn_out = getattr(self, &apos;dropout_&apos; + str(i))(dnn_out)</span><br><span class="line">        </span><br><span class="line">        dnn_out = self.dnn_linear(dnn_out)   # [bs, 1]</span><br><span class="line">        out = fm_1st_part + fm_2nd_part + dnn_out   # [bs, 1]</span><br><span class="line">        out = self.sigmoid(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>[1] <a href="https://blog.csdn.net/weixin_44556141/article/details/120790057" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44556141/article/details/120790057</a></p>
<p>[2] <a href="https://zhuanlan.zhihu.com/p/354994307" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/354994307</a></p>
<p>[3] <a href="https://blog.csdn.net/Jeremiah_/article/details/120740877" target="_blank" rel="noopener">https://blog.csdn.net/Jeremiah_/article/details/120740877</a></p>
<p>[4] <a href="https://zhuanlan.zhihu.com/p/361451464" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/361451464</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">CinKate</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://renxingkai.github.io/2022/07/14/deeprec/">http://renxingkai.github.io/2022/07/14/deeprec/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/排序-推荐系统/">排序 - 推荐系统</a></div><div class="social-share pull-right" data-disabled="google,facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="next-post pull-right"><a href="/2022/03/26/paper-lstm-ved/"><span>Learning Robust Models for e-Commerce Product Search阅读笔记</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By CinKate</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://renxingkai.github.io">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>