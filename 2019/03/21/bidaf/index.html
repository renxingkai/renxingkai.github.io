<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BIDAF代码阅读"><meta name="keywords" content="阅读理解,深度学习"><meta name="author" content="CinKate"><meta name="copyright" content="CinKate"><title>BIDAF代码阅读 | CinKate's Blogs</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b77222dd6b9929f160b8a04fc8705337";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '3.9.0'
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-assert的用法-：-主要用于检查条件，不符合就终止程序"><span class="toc-number">1.</span> <span class="toc-text">1.assert的用法 ： 主要用于检查条件，不符合就终止程序</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-打开文件codecs-open-解决不同文件的编码问题，会将文件内容转为unicode"><span class="toc-number">2.</span> <span class="toc-text">2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#以下是prepro-py文件的代码阅读与分析："><span class="toc-number">2.1.</span> <span class="toc-text">以下是prepro.py文件的代码阅读与分析：</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://imgsrc.baidu.com/forum/w%3D580/sign=f8de0e9b3e87e9504217f3642039531b/1c3bd133c895d143e395e57b77f082025baf0726.jpg"></div><div class="author-info__name text-center">CinKate</div><div class="author-info__description text-center">长笛一声人倚楼~</div><div class="follow-button"><a href="https://github.com/renxingkai">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">33</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">23</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">12</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CinKate's Blogs</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">BIDAF代码阅读</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-17</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/深度学习/">深度学习</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.8k</span><span class="post-meta__separator">|</span><span>Reading time: 14 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="1-assert的用法-：-主要用于检查条件，不符合就终止程序"><a href="#1-assert的用法-：-主要用于检查条件，不符合就终止程序" class="headerlink" title="1.assert的用法 ： 主要用于检查条件，不符合就终止程序"></a>1.assert的用法 ： 主要用于检查条件，不符合就终止程序</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=-1</span><br><span class="line">#报错</span><br><span class="line">assert a&gt;0,&quot;a超出范围&quot;</span><br><span class="line">#正常运行</span><br><span class="line">assert a&lt;0</span><br></pre></td></tr></table></figure>
<h2 id="2-打开文件codecs-open-解决不同文件的编码问题，会将文件内容转为unicode"><a href="#2-打开文件codecs-open-解决不同文件的编码问题，会将文件内容转为unicode" class="headerlink" title="2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicode"></a>2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicode</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import  codecs, sys</span><br><span class="line"></span><br><span class="line">#  用codecs提供的open方法来指定打开的文件的语言编码，它会在读 取的时候自动转换为内部unicode </span><br><span class="line">bfile  =  codecs.open( &quot; dddd.txt &quot; ,  &apos; r &apos; ,  &quot; big5 &quot; )</span><br><span class="line"># bfile = open(&quot;dddd.txt&quot;, &apos;r&apos;) </span><br><span class="line"> </span><br><span class="line">ss  =  bfile.read()</span><br><span class="line">bfile.close()</span><br><span class="line">#  输出，这个时候看到的就是转换后的结果。如果使用语言内建的open函数 来打开文件，这里看到的必定是乱码</span><br></pre></td></tr></table></figure>
<h3 id="以下是prepro-py文件的代码阅读与分析："><a href="#以下是prepro-py文件的代码阅读与分析：" class="headerlink" title="以下是prepro.py文件的代码阅读与分析："></a>以下是prepro.py文件的代码阅读与分析：</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import spacy</span><br><span class="line">import json</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">from collections import Counter</span><br><span class="line">import random</span><br><span class="line">import codecs</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import tensorflow  as tf</span><br><span class="line"></span><br><span class="line">#加载模型</span><br><span class="line">nlp=spacy.blank(&apos;en&apos;)</span><br><span class="line"></span><br><span class="line">#对句子进行分词</span><br><span class="line">def word_tokenize(sent):</span><br><span class="line">    doc=nlp(sent)</span><br><span class="line">    return [token.text for token in doc]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#常用的word2idx</span><br><span class="line">#此处输出spans形式：[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)]</span><br><span class="line">#意为取出该词当前所在的位置，并且结束长度+当前长度</span><br><span class="line">#两者之差即为该单词长度</span><br><span class="line">def convert_idx(text,tokens):</span><br><span class="line">    current=0</span><br><span class="line">    spans=[]</span><br><span class="line">    for token in tokens:</span><br><span class="line">        current=text.find(token,current)</span><br><span class="line">        if current&lt;0:</span><br><span class="line">            print(&apos;Token &#123;&#125; cannot be found!&apos;.format(token))</span><br><span class="line">            raise Exception()</span><br><span class="line">        #[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)]</span><br><span class="line">        #取出该词当前所在的位置，并且结束长度+当前长度</span><br><span class="line">        #两者之差即为该单词长度</span><br><span class="line">        spans.append((current,current+len(token)))</span><br><span class="line">        current+=len(token)</span><br><span class="line">    return spans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#预处理文件</span><br><span class="line">def process_file(filename,data_type=None,word_counter=None,char_counter=None):</span><br><span class="line">    print(&quot;Generating &#123;&#125; examples...&quot;.format(data_type))</span><br><span class="line">    examples = []</span><br><span class="line">    eval_examples = &#123;&#125;</span><br><span class="line">    total=0</span><br><span class="line">    with open(filename,&apos;r&apos;) as fh:</span><br><span class="line">        source=json.load(fh)</span><br><span class="line">        print(len(source[&apos;data&apos;]))</span><br><span class="line">        #遍历每篇文章dev有48篇文章</span><br><span class="line">        for article in tqdm(source[&quot;data&quot;]):</span><br><span class="line">            #遍历每篇文章的段落</span><br><span class="line">            for para in article[&apos;paragraphs&apos;]:</span><br><span class="line">                #替换段落中的&apos;&apos;和``</span><br><span class="line">                context = para[&quot;context&quot;].replace(&quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;)</span><br><span class="line">                #并对段落进行分词,分词中还是带了标点和特殊符号，需要后面进行处理</span><br><span class="line">                context_tokens=word_tokenize(context)</span><br><span class="line">                #[&apos;The&apos;, &apos;connection&apos;, &apos;between&apos;, &apos;macroscopic&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;and&apos;, &apos;microscopic&apos;, &apos;conservative&apos;, &apos;forces&apos;, &apos;is&apos;, &apos;described&apos;, &apos;by&apos;, &apos;detailed&apos;, &apos;treatment&apos;, &apos;with&apos;, &apos;statistical&apos;, &apos;mechanics&apos;, &apos;.&apos;, &apos;In&apos;, &apos;macroscopic&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;act&apos;, &apos;to&apos;, &apos;change&apos;, &apos;the&apos;, &apos;internal&apos;, &apos;energies&apos;, &apos;of&apos;, &apos;the&apos;, &apos;system&apos;, &apos;,&apos;, &apos;and&apos;, &apos;are&apos;, &apos;often&apos;, &apos;associated&apos;, &apos;with&apos;, &apos;the&apos;, &apos;transfer&apos;, &apos;of&apos;, &apos;heat&apos;, &apos;.&apos;, &apos;According&apos;, &apos;to&apos;, &apos;the&apos;, &apos;Second&apos;, &apos;law&apos;, &apos;of&apos;, &apos;thermodynamics&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;necessarily&apos;, &apos;result&apos;, &apos;in&apos;, &apos;energy&apos;, &apos;transformations&apos;, &apos;within&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;from&apos;, &apos;ordered&apos;, &apos;to&apos;, &apos;more&apos;, &apos;random&apos;, &apos;conditions&apos;, &apos;as&apos;, &apos;entropy&apos;, &apos;increases&apos;, &apos;.&apos;]</span><br><span class="line">                #获取每个单词的字符表示</span><br><span class="line">                context_chars = [list(token) for token in context_tokens]</span><br><span class="line">                #word2idx 每个词开始的位置和结束的位置</span><br><span class="line">                spans = convert_idx(context, context_tokens)</span><br><span class="line">                for token in context_tokens:</span><br><span class="line">                    #这儿加的是每个qas的长度？？</span><br><span class="line">                    word_counter[token] += len(para[&quot;qas&quot;])</span><br><span class="line">                    for char in token:</span><br><span class="line">                        #每个单词的字符这儿也加的是每个qas的长度</span><br><span class="line">                        #Counter(&#123;&apos;e&apos;: 28293, &apos;a&apos;: 19610, &apos;n&apos;: 17317, &apos;t&apos;: 17071, &apos;r&apos;: 15443, &apos;o&apos;: 15358, &apos;i&apos;:</span><br><span class="line">                        # 14669, &apos;s&apos;: 14081, &apos;h&apos;: 11839, &apos;l&apos;: 9031, &apos;d&apos;: 8982, &apos;c&apos;: 6540, &apos;u&apos;: 5885,</span><br><span class="line">                        # &apos;w&apos;: 5806, &apos;f&apos;: 4516, &apos;g&apos;: 4463, &apos;p&apos;: 4372, &apos;m&apos;: 4165, &apos;,&apos;: 3116, &apos;y&apos;: 2842, &apos;b&apos;: 2321, &apos;v&apos;: 2152,</span><br><span class="line">                        # &apos;.&apos;: 2057, &apos;B&apos;: 2052, &apos;S&apos;: 1832, &apos;1&apos;: 1776, &apos;k&apos;: 1553, &apos;0&apos;: 1168, &apos;C&apos;: 1107, &apos;F&apos;: 963, &apos;T&apos;: 876,</span><br><span class="line">                        # &apos;2&apos;: 856, &apos;P&apos;: 836, &apos;I&apos;: 819, &apos;5&apos;: 798, &apos;N&apos;: 766, &apos;L&apos;: 741, &apos;X&apos;: 714, &apos;M&apos;: 672, &apos;4&apos;: 662, &apos;3&apos;: 636,</span><br><span class="line">                        # &apos;A&apos;: 619, &apos;9&apos;: 584, &quot;&apos;&quot;: 552, &apos;-&apos;: 523, &apos;7&apos;: 488, &apos;D&apos;: 470, &apos;–&apos;: 415, &apos;(&apos;: 412, &apos;)&apos;: 412, &apos;8&apos;: 380,</span><br><span class="line">                        # &apos;6&apos;: 371, &apos;V&apos;: 352, &apos;O&apos;: 272, &apos;J&apos;: 268, &apos;j&apos;: 249, &apos;q&apos;: 235, &apos;&quot;&apos;: 222, &apos;G&apos;: 221, &apos;x&apos;: 220, &apos;E&apos;: 177,</span><br><span class="line">                        # &apos;R&apos;: 173, &apos;W&apos;: 168, &apos;K&apos;: 159, &apos;H&apos;: 117, &apos;U&apos;: 108, &apos;z&apos;: 107, &apos;½&apos;: 81, &apos;:&apos;: 81, &apos;;&apos;: 63, &apos;$&apos;: 49, &apos;#&apos;: 30,</span><br><span class="line">                        # &apos;é&apos;: 26, &apos;/&apos;: 21, &apos;Q&apos;: 15&#125;)</span><br><span class="line">                        char_counter[char] += len(para[&quot;qas&quot;])</span><br><span class="line">                #遍历qas</span><br><span class="line">                for qa in para[&quot;qas&quot;]:</span><br><span class="line">                    total += 1</span><br><span class="line">                    #替换问题&apos;&apos; ``</span><br><span class="line">                    ques = qa[&quot;question&quot;].replace(</span><br><span class="line">                        &quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;)</span><br><span class="line">                    #对问题进行分词</span><br><span class="line">                    ques_tokens = word_tokenize(ques)</span><br><span class="line">                    #取出问题中的字符</span><br><span class="line">                    ques_chars = [list(token) for token in ques_tokens]</span><br><span class="line">                    #遍历问题每个词</span><br><span class="line">                    for token in ques_tokens:</span><br><span class="line">                        #此处真的正确</span><br><span class="line">                        word_counter[token] += 1</span><br><span class="line">                        for char in token:</span><br><span class="line">                            char_counter[char] += 1</span><br><span class="line">                    y1s, y2s = [], []</span><br><span class="line">                    answer_texts = []</span><br><span class="line">                    #遍历答案文本</span><br><span class="line">                    for answer in qa[&quot;answers&quot;]:</span><br><span class="line">                        #答案文本</span><br><span class="line">                        answer_text = answer[&quot;text&quot;]</span><br><span class="line">                        #开始位置</span><br><span class="line">                        answer_start = answer[&apos;answer_start&apos;]</span><br><span class="line">                        answer_end = answer_start + len(answer_text)</span><br><span class="line">                        answer_texts.append(answer_text)</span><br><span class="line">                        answer_span = []</span><br><span class="line">                        #加入答案span answer_span</span><br><span class="line">                        for idx, span in enumerate(spans):</span><br><span class="line">                            if not (answer_end &lt;= span[0] or answer_start &gt;= span[1]):</span><br><span class="line">                                answer_span.append(idx)</span><br><span class="line">                        y1, y2 = answer_span[0], answer_span[-1]</span><br><span class="line">                        y1s.append(y1)</span><br><span class="line">                        y2s.append(y2)</span><br><span class="line">                    example = &#123;&quot;context_tokens&quot;: context_tokens, &quot;context_chars&quot;: context_chars,</span><br><span class="line">                               &quot;ques_tokens&quot;: ques_tokens,</span><br><span class="line">                               &quot;ques_chars&quot;: ques_chars, &quot;y1s&quot;: y1s, &quot;y2s&quot;: y2s, &quot;id&quot;: total&#125;</span><br><span class="line">                    examples.append(example)</span><br><span class="line">                    #未分词结果</span><br><span class="line">                    eval_examples[str(total)] = &#123;</span><br><span class="line">                        &quot;context&quot;: context, &quot;spans&quot;: spans, &quot;answers&quot;: answer_texts, &quot;uuid&quot;: qa[&quot;id&quot;]&#125;</span><br><span class="line">        random.shuffle(examples)</span><br><span class="line">        print(&quot;&#123;&#125; questions in total&quot;.format(len(examples)))</span><br><span class="line">    return examples, eval_examples</span><br><span class="line"></span><br><span class="line">#获取词向量</span><br><span class="line">def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None):</span><br><span class="line">    print(&quot;Generating &#123;&#125; embedding...&quot;.format(data_type))</span><br><span class="line">    embedding_dict=&#123;&#125;</span><br><span class="line">    #过滤掉低频词，仅取出频率较高的词</span><br><span class="line">    filtered_elements=[k for k,v in counter.items() if v&gt;limit]</span><br><span class="line">    #判断词向量文件是否为空</span><br><span class="line">    if emb_file is not None:</span><br><span class="line">        assert size is not None#如果size为空直接退出程序</span><br><span class="line">        assert vec_size is not None#如果vec_size为空直接退出程序</span><br><span class="line">        #读取词向量</span><br><span class="line">        with codecs.open(emb_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as fh:</span><br><span class="line">            # 依次遍历词向量每一行</span><br><span class="line">            for line in tqdm(fh, total=size):</span><br><span class="line">                #分开词和向量</span><br><span class="line">                array = line.split()</span><br><span class="line">                #取出开头的单词</span><br><span class="line">                word=&quot;&quot;.join(array[0:-vec_size])</span><br><span class="line">                #取出单词对应的词向量</span><br><span class="line">                vector=list(map(float,array[-vec_size:]))</span><br><span class="line">                #词向量的单词在counter单词中，并且 在文本中的单词数目&gt;limit</span><br><span class="line">                if word in counter and counter[word]&gt;limit:</span><br><span class="line">                    embedding_dict[word]=vector</span><br><span class="line">        print(&quot;&#123;&#125; / &#123;&#125; tokens have corresponding &#123;&#125; embedding vector&quot;.format(</span><br><span class="line">            len(embedding_dict), len(filtered_elements), data_type))</span><br><span class="line">    #如果词向量文件为空</span><br><span class="line">    else:</span><br><span class="line">        assert vec_size is not None</span><br><span class="line">        #遍历所有过滤的词</span><br><span class="line">        for token in filtered_elements:</span><br><span class="line">            #对每个单词进行随机初始化向量</span><br><span class="line">            embedding_dict[token]=[np.random.normal(scale=0.01) for _ in range(vec_size)]</span><br><span class="line">        print(&quot;&#123;&#125; tokens have corresponding embedding vector&quot;.format(</span><br><span class="line">            len(filtered_elements)))</span><br><span class="line">    #处理OOV词</span><br><span class="line">    NULL = &quot;--NULL--&quot;</span><br><span class="line">    OOV = &quot;--OOV--&quot;</span><br><span class="line">    #从下标2索引开始，过滤掉NULL和OOV  创建token2_idx_dict</span><br><span class="line">    token2idx_dict=&#123;token:idx for idx,token in enumerate(embedding_dict.keys(),2)&#125; if token2idx_dict is None else token2idx_dict</span><br><span class="line">    #NULL OOV 设置token2idx</span><br><span class="line">    token2idx_dict[NULL] = 0</span><br><span class="line">    token2idx_dict[OOV] = 1</span><br><span class="line">    #NULL OOV设置embedding_dict</span><br><span class="line">    embedding_dict[NULL] = [0. for _ in range(vec_size)]</span><br><span class="line">    embedding_dict[OOV] = [0. for _ in range(vec_size)]</span><br><span class="line">    #id2embedding 单词id对应的词向量</span><br><span class="line">    id2emb_dict=&#123;idx:embedding_dict[token] for token,idx in token2idx_dict.items() &#125;</span><br><span class="line">    #获取词向量矩阵</span><br><span class="line">    emb_mat=[id2emb_dict[idx] for idx in range(id2emb_dict)]</span><br><span class="line">    #仅返回 词向量矩阵，token2idx_dict</span><br><span class="line">    return emb_mat, token2idx_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#构建文本特征question paragraph answer and so on</span><br><span class="line">def build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False):</span><br><span class="line">    #文章长度</span><br><span class="line">    para_limit=config.test_para_limit if is_test else config.para_limit</span><br><span class="line">    #问题长度</span><br><span class="line">    ques_limit = config.test_ques_limit if is_test else config.ques_limit</span><br><span class="line">    #字符限制长度</span><br><span class="line">    char_limit = config.char_limit</span><br><span class="line">    #过滤文章和问题长度函数</span><br><span class="line">    def filter_func(example, is_test=False):</span><br><span class="line">        return len(example[&quot;context_tokens&quot;]) &gt; para_limit or len(example[&quot;ques_tokens&quot;]) &gt; ques_limit</span><br><span class="line"></span><br><span class="line">    print(&quot;Processing &#123;&#125; examples...&quot;.format(data_type))</span><br><span class="line">    writer = tf.python_io.TFRecordWriter(out_file)</span><br><span class="line">    total = 0</span><br><span class="line">    total_ = 0</span><br><span class="line">    meta = &#123;&#125;</span><br><span class="line">    #处理文章</span><br><span class="line">    for example in tqdm(examples):</span><br><span class="line">        total_+=1</span><br><span class="line">        #过滤长度大于限制值的文章</span><br><span class="line">        if filter_func(example, is_test):</span><br><span class="line">            continue</span><br><span class="line">        total += 1</span><br><span class="line">        #段落ids</span><br><span class="line">        context_idxs = np.zeros([para_limit], dtype=np.int32)</span><br><span class="line">        #段落id char对应的矩阵</span><br><span class="line">        context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)</span><br><span class="line">        ##问题ids</span><br><span class="line">        ques_idxs = np.zeros([ques_limit], dtype=np.int32)</span><br><span class="line">        ##问题id char对应的矩阵</span><br><span class="line">        ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)</span><br><span class="line">        #段落长度</span><br><span class="line">        y1 = np.zeros([para_limit], dtype=np.float32)</span><br><span class="line">        y2 = np.zeros([para_limit], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        #获取单词</span><br><span class="line">        def _get_word(word):</span><br><span class="line">            for each in (word, word.lower(), word.capitalize(), word.upper()):</span><br><span class="line">                if each in word2idx_dict:</span><br><span class="line">                    #返回每个单词对应的id</span><br><span class="line">                    return word2idx_dict[each]</span><br><span class="line">            return 1</span><br><span class="line"></span><br><span class="line">        #获取字符</span><br><span class="line">        def _get_char(char):</span><br><span class="line">            if char in char2idx_dict:</span><br><span class="line">                # 返回每个字符对应的id</span><br><span class="line">                return char2idx_dict[char]</span><br><span class="line">            return 1</span><br><span class="line">        #为每个文章内容获取对应的ids  context_tokens为已经分好词的文章</span><br><span class="line">        for i, token in enumerate(example[&quot;context_tokens&quot;]):</span><br><span class="line">            context_idxs[i] = _get_word(token)</span><br><span class="line">        # 为每个问题内容获取对应的ids   ques_tokens为已经分好词的问题</span><br><span class="line">        for i, token in enumerate(example[&quot;ques_tokens&quot;]):</span><br><span class="line">            ques_idxs[i] = _get_word(token)</span><br><span class="line">        # 为每个文章内容获取对应的chars</span><br><span class="line">        for i, token in enumerate(example[&quot;context_chars&quot;]):</span><br><span class="line">            for j, char in enumerate(token):</span><br><span class="line">                #不能超出char的限制</span><br><span class="line">                if j == char_limit:</span><br><span class="line">                    break</span><br><span class="line">                #赋值char 不够的用0填充</span><br><span class="line">                context_char_idxs[i, j] = _get_char(char)</span><br><span class="line">        # 为每个问题内容获取对应的chars</span><br><span class="line">        for i, token in enumerate(example[&quot;ques_chars&quot;]):</span><br><span class="line">            for j, char in enumerate(token):</span><br><span class="line">                #不能超出char的限制</span><br><span class="line">                if j == char_limit:</span><br><span class="line">                    break</span><br><span class="line">                # 赋值char 不够的用0填充</span><br><span class="line">                ques_char_idxs[i, j] = _get_char(char)</span><br><span class="line">        #开始，结束位置</span><br><span class="line">        start, end = example[&quot;y1s&quot;][-1], example[&quot;y2s&quot;][-1]</span><br><span class="line">        y1[start], y2[end] = 1.0, 1.0</span><br><span class="line">        #构建tensorflow 记录</span><br><span class="line">        record = tf.train.Example(features=tf.train.Features(feature=&#123;</span><br><span class="line">                                  &quot;context_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),</span><br><span class="line">                                  &quot;ques_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),</span><br><span class="line">                                  &quot;context_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),</span><br><span class="line">                                  &quot;ques_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),</span><br><span class="line">                                  &quot;y1&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),</span><br><span class="line">                                  &quot;y2&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),</span><br><span class="line">                                  &quot;id&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[example[&quot;id&quot;]]))</span><br><span class="line">                                  &#125;))</span><br><span class="line">        writer.write(record.SerializeToString())</span><br><span class="line">    print(&quot;Build &#123;&#125; / &#123;&#125; instances of features in total&quot;.format(total, total_))</span><br><span class="line">    meta[&quot;total&quot;] = total</span><br><span class="line">    writer.close()</span><br><span class="line">    return meta</span><br><span class="line"></span><br><span class="line">#保存文件</span><br><span class="line">def save(filename, obj, message=None):</span><br><span class="line">    if message is not None:</span><br><span class="line">        print(&quot;Saving &#123;&#125;...&quot;.format(message))</span><br><span class="line">        with open(filename, &quot;w&quot;) as fh:</span><br><span class="line">            json.dump(obj, fh)</span><br><span class="line"></span><br><span class="line"># 预处理文件</span><br><span class="line">def prepro(config):</span><br><span class="line">    #单词，字符计数器</span><br><span class="line">    word_counter, char_counter = Counter(), Counter()</span><br><span class="line">    #处理训练集</span><br><span class="line">    train_examples, train_eval = process_file(</span><br><span class="line">        config.train_file, &quot;train&quot;, word_counter, char_counter)</span><br><span class="line">    #处理验证集</span><br><span class="line">    dev_examples, dev_eval = process_file(</span><br><span class="line">        config.dev_file, &quot;dev&quot;, word_counter, char_counter)</span><br><span class="line">    #处理测试集</span><br><span class="line">    test_examples, test_eval = process_file(</span><br><span class="line">        config.test_file, &quot;test&quot;, word_counter, char_counter)</span><br><span class="line">    #词向量文件</span><br><span class="line">    word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file</span><br><span class="line">    #字符向量文件</span><br><span class="line">    char_emb_file = config.glove_char_file if config.pretrained_char else None</span><br><span class="line">    #字符向量大小</span><br><span class="line">    char_emb_size = config.glove_char_size if config.pretrained_char else None</span><br><span class="line">    #字符向量维度</span><br><span class="line">    char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim</span><br><span class="line">    #word2idx字典</span><br><span class="line">    word2idx_dict = None</span><br><span class="line">    #如果存在word2idx字典 则直接导入</span><br><span class="line">    if os.path.isfile(config.word2idx_file):</span><br><span class="line">        with open(config.word2idx_file, &quot;r&quot;) as fh:</span><br><span class="line">            word2idx_dict = json.load(fh)</span><br><span class="line">    #构建词向量矩阵</span><br><span class="line">    word_emb_mat, word2idx_dict = get_embedding(word_counter, &quot;word&quot;, emb_file=word_emb_file,</span><br><span class="line">                                                size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict)</span><br><span class="line"></span><br><span class="line">    #构建字符向量矩阵</span><br><span class="line">    char2idx_dict = None</span><br><span class="line">    # 如果存在char2idx字典 则直接导入</span><br><span class="line">    if os.path.isfile(config.char2idx_file):</span><br><span class="line">        with open(config.char2idx_file, &quot;r&quot;) as fh:</span><br><span class="line">            char2idx_dict = json.load(fh)</span><br><span class="line">    # 构建字符向量矩阵</span><br><span class="line">    char_emb_mat, char2idx_dict = get_embedding(</span><br><span class="line">        char_counter, &quot;char&quot;, emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim,</span><br><span class="line">        token2idx_dict=char2idx_dict)</span><br><span class="line"></span><br><span class="line">    #对训练集、验证集、测试集构建特征</span><br><span class="line">    build_features(config, train_examples, &quot;train&quot;,</span><br><span class="line">                   config.train_record_file, word2idx_dict, char2idx_dict)</span><br><span class="line">    dev_meta = build_features(config, dev_examples, &quot;dev&quot;,</span><br><span class="line">                              config.dev_record_file, word2idx_dict, char2idx_dict)</span><br><span class="line">    test_meta = build_features(config, test_examples, &quot;test&quot;,</span><br><span class="line">                               config.test_record_file, word2idx_dict, char2idx_dict, is_test=True)</span><br><span class="line"></span><br><span class="line">    #对预处理的文件进行保存</span><br><span class="line">    save(config.word_emb_file, word_emb_mat, message=&quot;word embedding&quot;)</span><br><span class="line">    save(config.char_emb_file, char_emb_mat, message=&quot;char embedding&quot;)</span><br><span class="line">    save(config.train_eval_file, train_eval, message=&quot;train eval&quot;)</span><br><span class="line">    save(config.dev_eval_file, dev_eval, message=&quot;dev eval&quot;)</span><br><span class="line">    save(config.test_eval_file, test_eval, message=&quot;test eval&quot;)</span><br><span class="line">    save(config.dev_meta, dev_meta, message=&quot;dev meta&quot;)</span><br><span class="line">    save(config.word2idx_file, word2idx_dict, message=&quot;word2idx&quot;)</span><br><span class="line">    save(config.char2idx_file, char2idx_dict, message=&quot;char2idx&quot;)</span><br><span class="line">    save(config.test_meta, test_meta, message=&quot;test meta&quot;)</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">CinKate</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://renxingkai.github.io/2019/03/21/bidaf/">http://renxingkai.github.io/2019/03/21/bidaf/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/阅读理解/">阅读理解</a><a class="post-meta__tags" href="/tags/深度学习/">深度学习</a></div><div class="social-share pull-right" data-disabled="google,facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/03/24/kerestricks/"><i class="fa fa-chevron-left">  </i><span>Keras踩坑总结</span></a></div><div class="next-post pull-right"><a href="/2019/03/20/nntuningparameter/"><span>神经网络调参的一些tips</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2021 By CinKate</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://renxingkai.github.io">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>