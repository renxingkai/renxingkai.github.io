<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="FastQA学习"><meta name="keywords" content="阅读理解"><meta name="author" content="CinKate"><meta name="copyright" content="CinKate"><title>FastQA学习 | CinKate's Blogs</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b77222dd6b9929f160b8a04fc8705337";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '3.9.0'
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Highway-Network的使用"><span class="toc-number">1.</span> <span class="toc-text">1.Highway Network的使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-tf-sequence-mask的学习"><span class="toc-number">2.</span> <span class="toc-text">2.tf.sequence_mask的学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-tf-expand-dims-学习"><span class="toc-number">3.</span> <span class="toc-text">3.tf.expand_dims()学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-tf-tile-学习"><span class="toc-number">4.</span> <span class="toc-text">4.tf.tile()学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-tf-equal-学习"><span class="toc-number">5.</span> <span class="toc-text">5.tf.equal()学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-tf-reduce-any-学习"><span class="toc-number">6.</span> <span class="toc-text">6.tf.reduce_any()学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-tf-squeeze-学习"><span class="toc-number">7.</span> <span class="toc-text">7.tf.squeeze()学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-RepeatVector层"><span class="toc-number">8.</span> <span class="toc-text">8.RepeatVector层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-tf-gather-学习"><span class="toc-number">9.</span> <span class="toc-text">9.tf.gather()学习</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://imgsrc.baidu.com/forum/w%3D580/sign=f8de0e9b3e87e9504217f3642039531b/1c3bd133c895d143e395e57b77f082025baf0726.jpg"></div><div class="author-info__name text-center">CinKate</div><div class="author-info__description text-center">长笛一声人倚楼~</div><div class="follow-button"><a href="https://github.com/renxingkai">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">41</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">14</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CinKate's Blogs</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">FastQA学习</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-17</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/自然语言处理/">自然语言处理</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 <strong>emedding-, encoding-, interaction-, answer-layer</strong> ），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层，比如BIDAF、R-Net等等，大量的工作都在问题和文章的交互query-aware表示上创新，类似人类做阅读理解问题的思路“重复多读文章”，“带着问题读文章”等等，普通的“阅读理解思路”也都被实现了，这篇论文作者发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans:</p>
<ol>
<li>与 question 对应的 answer type 匹配，比如说问 when 就回答 time；</li>
<li>与重要的 question words 位置上临近；</li>
<li>添加问题单词是否出现在文章中这一“重要”特征；<br>并没有使用复杂的question与context的交互，就取得了在SQuAD榜上与SOTA接近的结果，这篇论文之后，后来的研究者们在做MRC时也会将基础特征加入到embedding中进行共同训练，<a href="https://github.com/BrambleXu/keras_fastqa" target="_blank" rel="noopener">开源链接</a>。</li>
</ol>
<p>以下是阅读源码的一些总结：</p>
<h2 id="1-Highway-Network的使用"><a href="#1-Highway-Network的使用" class="headerlink" title="1.Highway Network的使用"></a>1.Highway Network的使用</h2><p>Highway Network主要解决的问题是，网络深度加深，梯度信息回流受阻造成网络训练困难的问题。</p>
<p>当网络加深，训练的误差反而上升了，而加入了Highway Network之后，这个问题得到了缓解。一般来说，深度网络训练困难是由于梯度回流受阻的问题，可能浅层网络没有办法得到调整。Highway Network 受LSTM启发，增加了一个门函数，让网络的输出由两部分组成，分别是网络的直接输入以及输入变形后的部分。</p>
<p><strong>网络中把此层放在embedding层后面</strong></p>
<p><img src="/highwaydes.jpg" alt="图片来源 https://www.cnblogs.com/jie-dcai/p/5803220.html"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from keras import backend as K</span><br><span class="line">from keras.engine.topology import Layer</span><br><span class="line">from keras.layers import Lambda, Wrapper</span><br><span class="line"></span><br><span class="line">class Highway(Layer):</span><br><span class="line">    def __init__(self, hidden_size, **kwargs):</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.projection = self.add_weight(name=&apos;projection&apos;,</span><br><span class="line">                                          shape=(1, input_shape[-1], self.hidden_size),</span><br><span class="line">                                          initializer=&apos;glorot_uniform&apos;)</span><br><span class="line">        self.W_h = self.add_weight(name=&apos;W_h&apos;,</span><br><span class="line">                                   shape=(1, self.hidden_size, self.hidden_size),</span><br><span class="line">                                   initializer=&apos;glorot_uniform&apos;)</span><br><span class="line">        self.b_h = self.add_weight(name=&apos;b_h&apos;,</span><br><span class="line">                                   shape=(self.hidden_size,),</span><br><span class="line">                                   initializer=&apos;zeros&apos;)</span><br><span class="line">        self.W_t = self.add_weight(name=&apos;W_t&apos;,</span><br><span class="line">                                   shape=(1, self.hidden_size, self.hidden_size),</span><br><span class="line">                                   initializer=&apos;glorot_uniform&apos;)</span><br><span class="line">        self.b_t = self.add_weight(name=&apos;b_t&apos;,</span><br><span class="line">                                   shape=(self.hidden_size,),</span><br><span class="line">                                   initializer=&apos;zeros&apos;)</span><br><span class="line"></span><br><span class="line">    def call(self, x):</span><br><span class="line">        x = K.conv1d(x, self.projection)</span><br><span class="line">        H = tf.nn.tanh(K.bias_add(K.conv1d(x, self.W_h), self.b_h))</span><br><span class="line">        T = tf.nn.sigmoid(K.bias_add(K.conv1d(x, self.W_t), self.b_t))</span><br><span class="line">        return T * x + (1 - T) * H</span><br><span class="line"></span><br><span class="line">    def compute_output_shape(self, input_shape):</span><br><span class="line">        batch, seq_len, d = input_shape</span><br><span class="line">        return (batch, seq_len, self.hidden_size)</span><br></pre></td></tr></table></figure>
<h2 id="2-tf-sequence-mask的学习"><a href="#2-tf-sequence-mask的学习" class="headerlink" title="2.tf.sequence_mask的学习"></a>2.tf.sequence_mask的学习</h2><p>这个操作和one hot也很像，但是指定的不是index而是从前到后有多少个True，返回的是True和False。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sq_mask = tf.sequence_mask([1, 3, 2], 5)</span><br><span class="line">print(sess.run(sq_mask))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[True, False, False, False, False],</span><br><span class="line">[True, True, True, False, False],</span><br><span class="line">[True, True, False, False, False]]</span><br></pre></td></tr></table></figure>
<h2 id="3-tf-expand-dims-学习"><a href="#3-tf-expand-dims-学习" class="headerlink" title="3.tf.expand_dims()学习"></a>3.tf.expand_dims()学习</h2><p>TensorFlow中，想要维度增加一维，可以使用 <strong>tf.expand_dims(input, dim, name=None)</strong> 函数。当然，我们常用tf.reshape(input,shape=[])也可以达到相同效果，但是有些时候在构建图的过程中，placeholder没有被feed具体的值，这时就会包下面的错误：TypeError: Expected binary or unicode string, got 1 </p>
<p>在这种情况下，我们就可以考虑使用expand_dims来将维度加1。比如我自己代码中遇到的情况，在对图像维度降到二维做特定操作后，要还原成四维[batch, height, width, channels]，前后各增加一维。如果用reshape，则因为上述原因报错</p>
<p>给出官方的例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># &apos;t&apos; is a tensor of shape [2]</span><br><span class="line">shape(expand_dims(t, 0)) ==&gt; [1, 2]</span><br><span class="line">shape(expand_dims(t, 1)) ==&gt; [2, 1]</span><br><span class="line">shape(expand_dims(t, -1)) ==&gt; [2, 1]</span><br><span class="line"></span><br><span class="line"># &apos;t2&apos; is a tensor of shape [2, 3, 5]</span><br><span class="line">shape(expand_dims(t2, 0)) ==&gt; [1, 2, 3, 5]</span><br><span class="line">shape(expand_dims(t2, 2)) ==&gt; [2, 3, 1, 5]</span><br><span class="line">shape(expand_dims(t2, 3)) ==&gt; [2, 3, 5, 1]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Args: </span><br><span class="line">input: A Tensor. </span><br><span class="line">dim: A Tensor. Must be one of the following types: int32, int64. 0-D (scalar). Specifies the dimension index at which to expand the shape of input. </span><br><span class="line">name: A name for the operation (optional).</span><br><span class="line"></span><br><span class="line">Returns: </span><br><span class="line">A Tensor. Has the same type as input. Contains the same data as input, but its shape has an additional dimension of size 1 added.</span><br></pre></td></tr></table></figure>
<h2 id="4-tf-tile-学习"><a href="#4-tf-tile-学习" class="headerlink" title="4.tf.tile()学习"></a>4.tf.tile()学习</h2><p><a href="https://blog.csdn.net/tsyccnh/article/details/82459859" target="_blank" rel="noopener">推荐博客</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.tile(  </span><br><span class="line">    input,     #输入  </span><br><span class="line">    multiples,  #某一维度上复制的次数  </span><br><span class="line">    name=None  </span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)</span><br><span class="line">a1 = tf.tile(a, [2, 3])</span><br><span class="line">a2 = tf.tile(a, [1, 2])</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(a))</span><br><span class="line">    print(sess.run(a1))</span><br><span class="line">    print(sess.run(a2))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1. 2.]</span><br><span class="line"> [3. 4.]</span><br><span class="line"> [5. 6.]]</span><br><span class="line"> </span><br><span class="line">[[1. 2. 1. 2. 1. 2.]</span><br><span class="line"> [3. 4. 3. 4. 3. 4.]</span><br><span class="line"> [5. 6. 5. 6. 5. 6.]</span><br><span class="line"> [1. 2. 1. 2. 1. 2.]</span><br><span class="line"> [3. 4. 3. 4. 3. 4.]</span><br><span class="line"> [5. 6. 5. 6. 5. 6.]]</span><br><span class="line"></span><br><span class="line">[[1. 2. 1. 2.]</span><br><span class="line"> [3. 4. 3. 4.]</span><br><span class="line"> [5. 6. 5. 6.]]</span><br></pre></td></tr></table></figure>
<h2 id="5-tf-equal-学习"><a href="#5-tf-equal-学习" class="headerlink" title="5.tf.equal()学习"></a>5.tf.equal()学习</h2><p> equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，而是逐个元素进行判断，如果相等就是 True，不相等，就是 False。</p>
<p>由于是逐个元素判断，所以 x，y 的维度要一致。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = [[1,2,3],[4,5,6]]</span><br><span class="line">b = [[1,0,3],[1,5,1]]</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(tf.equal(a,b)))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[ True False  True]</span><br><span class="line"> [False  True False]]</span><br></pre></td></tr></table></figure>
<h2 id="6-tf-reduce-any-学习"><a href="#6-tf-reduce-any-学习" class="headerlink" title="6.tf.reduce_any()学习"></a>6.tf.reduce_any()学习</h2><p>在boolean张量的维度上计算元素的 “逻辑或”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x = tf.constant([[True,  True], [False, False]])</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(tf.reduce_any(x))  # True</span><br><span class="line">    print(tf.reduce_any(x, 0))  # [True, True]</span><br><span class="line">    print(tf.reduce_any(x, 1))  # [True, False]</span><br></pre></td></tr></table></figure>
<h2 id="7-tf-squeeze-学习"><a href="#7-tf-squeeze-学习" class="headerlink" title="7.tf.squeeze()学习"></a>7.tf.squeeze()学习</h2><p>该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果<br>axis可以用来指定要删掉的为1的维度，此处要注意指定的维度必须确保其是1，否则会报错<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">squeeze(</span><br><span class="line">    input,</span><br><span class="line">    axis=None,</span><br><span class="line">    name=None,</span><br><span class="line">    squeeze_dims=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>例子：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#  &apos;t&apos; 是一个维度是[1, 2, 1, 3, 1, 1]的张量</span><br><span class="line">tf.shape(tf.squeeze(t))   # [2, 3]， 默认删除所有为1的维度</span><br><span class="line"></span><br><span class="line"># &apos;t&apos; 是一个维度[1, 2, 1, 3, 1, 1]的张量</span><br><span class="line">tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1</span><br></pre></td></tr></table></figure></p>
<h2 id="8-RepeatVector层"><a href="#8-RepeatVector层" class="headerlink" title="8.RepeatVector层"></a>8.RepeatVector层</h2><p>RepeatVector层将输入重复n次<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">keras.layers.core.RepeatVector(n)</span><br></pre></td></tr></table></figure></p>
<p><strong>参数</strong></p>
<p>n：整数，重复的次数</p>
<ul>
<li><p>输入shape<br>形如（nb_samples, features）的2D张量</p>
</li>
<li><p>输出shape<br>形如（nb_samples, n, features）的3D张量</p>
</li>
<li><p>例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(32, input_dim=32))</span><br><span class="line"># now: model.output_shape == (None, 32)</span><br><span class="line"># note: `None` is the batch dimension</span><br><span class="line"></span><br><span class="line">model.add(RepeatVector(3))</span><br><span class="line"># now: model.output_shape == (None, 3, 32)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="9-tf-gather-学习"><a href="#9-tf-gather-学习" class="headerlink" title="9.tf.gather()学习"></a>9.tf.gather()学习</h2><p>类似于数组的索引，可以把向量中某些索引值提取出来，得到新的向量，适用于要提取的索引为不连续的情况。这个函数似乎只适合在一维的情况下使用。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf </span><br><span class="line"> </span><br><span class="line">a = tf.Variable([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])</span><br><span class="line">index_a = tf.Variable([0,2])</span><br><span class="line"> </span><br><span class="line">b = tf.Variable([1,2,3,4,5,6,7,8,9,10])</span><br><span class="line">index_b = tf.Variable([2,4,6,8])</span><br><span class="line"> </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(tf.gather(a, index_a)))</span><br><span class="line">    print(sess.run(tf.gather(b, index_b)))</span><br><span class="line"> </span><br><span class="line">#  [[ 1  2  3  4  5]</span><br><span class="line">#   [11 12 13 14 15]]</span><br><span class="line"> </span><br><span class="line">#  [3 5 7 9]</span><br></pre></td></tr></table></figure>
<p><strong>tf.gather_nd</strong><br>同上，但允许在多维上进行索引，例子只展示了一种很简单的用法，</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">CinKate</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://renxingkai.github.io/2019/04/21/fastqa/">http://renxingkai.github.io/2019/04/21/fastqa/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/阅读理解/">阅读理解</a></div><div class="social-share pull-right" data-disabled="google,facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/05/12/WordEmbeddingsforReadingComprehension/"><i class="fa fa-chevron-left">  </i><span>《A Comparative Study of Word Embeddings for Reading Comprehension》论文阅读</span></a></div><div class="next-post pull-right"><a href="/2019/04/10/tfidfkeyextraction/"><span>基于NLTK的TF-IDF关键词抽取</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2021 By CinKate</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://renxingkai.github.io">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>