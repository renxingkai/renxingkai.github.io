<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="基于NLTK的TF-IDF关键词抽取"><meta name="keywords" content="关键词抽取"><meta name="author" content="CinKate"><meta name="copyright" content="CinKate"><title>基于NLTK的TF-IDF关键词抽取 | CinKate's Blogs</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?b77222dd6b9929f160b8a04fc8705337";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '3.9.0'
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SVD奇异值分解"><span class="toc-number">1.</span> <span class="toc-text">SVD奇异值分解</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#删除换行-进行分句"><span class="toc-number">1.0.0.1.</span> <span class="toc-text">删除换行,进行分句</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#转移HTML标签"><span class="toc-number">1.0.0.2.</span> <span class="toc-text">转移HTML标签</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#缩写词表"><span class="toc-number">1.0.0.3.</span> <span class="toc-text">缩写词表</span></a></li></ol></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#文本标准化"><span class="toc-number"></span> <span class="toc-text">文本标准化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#文本分词"><span class="toc-number">0.0.0.1.</span> <span class="toc-text">文本分词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#扩展缩写"><span class="toc-number">0.0.0.2.</span> <span class="toc-text">扩展缩写</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#标记文本词性"><span class="toc-number">0.0.0.3.</span> <span class="toc-text">标记文本词性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#基于词性标签提取主干词"><span class="toc-number">0.0.0.4.</span> <span class="toc-text">基于词性标签提取主干词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#删除特殊字符"><span class="toc-number">0.0.0.5.</span> <span class="toc-text">删除特殊字符</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#删除停用词"><span class="toc-number">0.0.0.6.</span> <span class="toc-text">删除停用词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#转移HTML标签-1"><span class="toc-number">0.0.0.7.</span> <span class="toc-text">转移HTML标签</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#标准化文本-合并执行上面流程"><span class="toc-number">0.0.0.8.</span> <span class="toc-text">标准化文本(合并执行上面流程)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#文本特征提取"><span class="toc-number"></span> <span class="toc-text">文本特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#构建特征矩阵binary、frequency、tfidf"><span class="toc-number">0.0.0.1.</span> <span class="toc-text">构建特征矩阵binary、frequency、tfidf</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#关键短语提取"><span class="toc-number"></span> <span class="toc-text">关键短语提取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#词项搭配"><span class="toc-number">0.1.</span> <span class="toc-text">词项搭配</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#将语料压缩成1个大的文本串"><span class="toc-number">0.1.0.1.</span> <span class="toc-text">将语料压缩成1个大的文本串</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#计算n元分词（比较巧妙）"><span class="toc-number">0.1.0.2.</span> <span class="toc-text">计算n元分词（比较巧妙）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#获取n元分词"><span class="toc-number">0.1.0.3.</span> <span class="toc-text">获取n元分词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#输出频率前10的二元分词"><span class="toc-number">0.1.0.4.</span> <span class="toc-text">输出频率前10的二元分词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#频率前10的三元分词"><span class="toc-number">0.1.0.5.</span> <span class="toc-text">频率前10的三元分词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#频率前10的一元分词"><span class="toc-number">0.1.0.6.</span> <span class="toc-text">频率前10的一元分词</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用nltk的搭配查找器"><span class="toc-number">0.2.</span> <span class="toc-text">使用nltk的搭配查找器</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#二元词项"><span class="toc-number">0.2.0.1.</span> <span class="toc-text">二元词项</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#二元使用点互信息PMI进行查找搭配"><span class="toc-number">0.2.0.2.</span> <span class="toc-text">二元使用点互信息PMI进行查找搭配</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#三元词组"><span class="toc-number">0.2.0.3.</span> <span class="toc-text">三元词组</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#三元使用点互信息PMI进行查找搭配"><span class="toc-number">0.2.0.4.</span> <span class="toc-text">三元使用点互信息PMI进行查找搭配</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#基于权重标签的短语提取"><span class="toc-number"></span> <span class="toc-text">基于权重标签的短语提取</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#提取文档中的名词短语-v-adj-adv"><span class="toc-number">0.0.0.1.</span> <span class="toc-text">提取文档中的名词短语 v adj adv</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#获取TF-IDF关键短语权重"><span class="toc-number">0.0.0.2.</span> <span class="toc-text">获取TF-IDF关键短语权重</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#前两个关键短语"><span class="toc-number">0.0.0.3.</span> <span class="toc-text">前两个关键短语</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#其他语料实验"><span class="toc-number">0.0.0.4.</span> <span class="toc-text">其他语料实验</span></a></li></ol></li></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://imgsrc.baidu.com/forum/w%3D580/sign=f8de0e9b3e87e9504217f3642039531b/1c3bd133c895d143e395e57b77f082025baf0726.jpg"></div><div class="author-info__name text-center">CinKate</div><div class="author-info__description text-center">长笛一声人倚楼~</div><div class="follow-button"><a href="https://github.com/renxingkai">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">43</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">27</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">15</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CinKate's Blogs</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">基于NLTK的TF-IDF关键词抽取</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-17</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/自然语言处理/">自然语言处理</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.5k</span><span class="post-meta__separator">|</span><span>Reading time: 19 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>基于nltk总结了用TF-IDF提取关键词的方法，同时总结了文本标准化（预处理），SVD分解、基于TF-IDF、词频等的关键词抽取</p>
<h2 id="SVD奇异值分解"><a href="#SVD奇异值分解" class="headerlink" title="SVD奇异值分解"></a>SVD奇异值分解</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from scipy.sparse.linalg import svds</span><br><span class="line">import re</span><br><span class="line">import nltk</span><br><span class="line">import unicodedata</span><br><span class="line"></span><br><span class="line">def low_rank_svd(matrix,singular_count=2):</span><br><span class="line">    u,s,vt=svds(matrix,k=singular_count)</span><br><span class="line">    return u,s,vt</span><br></pre></td></tr></table></figure>
<h5 id="删除换行-进行分句"><a href="#删除换行-进行分句" class="headerlink" title="删除换行,进行分句"></a>删除换行,进行分句</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def parse_document(document):</span><br><span class="line">    document=re.sub(&apos;\n&apos;,&apos; &apos;,document)</span><br><span class="line">    if isinstance(document,str):</span><br><span class="line">        document=document</span><br><span class="line">    elif isinstance(document,unicode):</span><br><span class="line">        return unicodedata.normalize(&apos;NFKD&apos;,document).encode(&apos;ascii&apos;,&apos;ignore&apos;)</span><br><span class="line">    else:</span><br><span class="line">        raise ValueError(&apos;Document is not string or unicode!&apos;)</span><br><span class="line">    document=document.strip()</span><br><span class="line">    sentences=nltk.sent_tokenize(document)</span><br><span class="line">    sentences=[sentence.strip() for sentence in sentences]</span><br><span class="line">    return sentences</span><br></pre></td></tr></table></figure>
<h5 id="转移HTML标签"><a href="#转移HTML标签" class="headerlink" title="转移HTML标签"></a>转移HTML标签</h5><p>from html.parser import HTMLParser</p>
<p>html_parser=HTMLParser()<br>def unescape_html(parser,text):<br>    return parser.unescape_html(text)</p>
<h5 id="缩写词表"><a href="#缩写词表" class="headerlink" title="缩写词表"></a>缩写词表</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CONTRACTION_MAP = &#123;</span><br><span class="line">&quot;ain&apos;t&quot;: &quot;is not&quot;,</span><br><span class="line">&quot;aren&apos;t&quot;: &quot;are not&quot;,</span><br><span class="line">&quot;can&apos;t&quot;: &quot;cannot&quot;,</span><br><span class="line">&quot;can&apos;t&apos;ve&quot;: &quot;cannot have&quot;,</span><br><span class="line">&quot;&apos;cause&quot;: &quot;because&quot;,</span><br><span class="line">&quot;could&apos;ve&quot;: &quot;could have&quot;,</span><br><span class="line">&quot;couldn&apos;t&quot;: &quot;could not&quot;,</span><br><span class="line">&quot;couldn&apos;t&apos;ve&quot;: &quot;could not have&quot;,</span><br><span class="line">&quot;didn&apos;t&quot;: &quot;did not&quot;,</span><br><span class="line">&quot;doesn&apos;t&quot;: &quot;does not&quot;,</span><br><span class="line">&quot;don&apos;t&quot;: &quot;do not&quot;,</span><br><span class="line">&quot;hadn&apos;t&quot;: &quot;had not&quot;,</span><br><span class="line">&quot;hadn&apos;t&apos;ve&quot;: &quot;had not have&quot;,</span><br><span class="line">&quot;hasn&apos;t&quot;: &quot;has not&quot;,</span><br><span class="line">&quot;haven&apos;t&quot;: &quot;have not&quot;,</span><br><span class="line">&quot;he&apos;d&quot;: &quot;he would&quot;,</span><br><span class="line">&quot;he&apos;d&apos;ve&quot;: &quot;he would have&quot;,</span><br><span class="line">&quot;he&apos;ll&quot;: &quot;he will&quot;,</span><br><span class="line">&quot;he&apos;ll&apos;ve&quot;: &quot;he he will have&quot;,</span><br><span class="line">&quot;he&apos;s&quot;: &quot;he is&quot;,</span><br><span class="line">&quot;how&apos;d&quot;: &quot;how did&quot;,</span><br><span class="line">&quot;how&apos;d&apos;y&quot;: &quot;how do you&quot;,</span><br><span class="line">&quot;how&apos;ll&quot;: &quot;how will&quot;,</span><br><span class="line">&quot;how&apos;s&quot;: &quot;how is&quot;,</span><br><span class="line">&quot;I&apos;d&quot;: &quot;I would&quot;,</span><br><span class="line">&quot;I&apos;d&apos;ve&quot;: &quot;I would have&quot;,</span><br><span class="line">&quot;I&apos;ll&quot;: &quot;I will&quot;,</span><br><span class="line">&quot;I&apos;ll&apos;ve&quot;: &quot;I will have&quot;,</span><br><span class="line">&quot;I&apos;m&quot;: &quot;I am&quot;,</span><br><span class="line">&quot;I&apos;ve&quot;: &quot;I have&quot;,</span><br><span class="line">&quot;i&apos;d&quot;: &quot;i would&quot;,</span><br><span class="line">&quot;i&apos;d&apos;ve&quot;: &quot;i would have&quot;,</span><br><span class="line">&quot;i&apos;ll&quot;: &quot;i will&quot;,</span><br><span class="line">&quot;i&apos;ll&apos;ve&quot;: &quot;i will have&quot;,</span><br><span class="line">&quot;i&apos;m&quot;: &quot;i am&quot;,</span><br><span class="line">&quot;i&apos;ve&quot;: &quot;i have&quot;,</span><br><span class="line">&quot;isn&apos;t&quot;: &quot;is not&quot;,</span><br><span class="line">&quot;it&apos;d&quot;: &quot;it would&quot;,</span><br><span class="line">&quot;it&apos;d&apos;ve&quot;: &quot;it would have&quot;,</span><br><span class="line">&quot;it&apos;ll&quot;: &quot;it will&quot;,</span><br><span class="line">&quot;it&apos;ll&apos;ve&quot;: &quot;it will have&quot;,</span><br><span class="line">&quot;it&apos;s&quot;: &quot;it is&quot;,</span><br><span class="line">&quot;let&apos;s&quot;: &quot;let us&quot;,</span><br><span class="line">&quot;ma&apos;am&quot;: &quot;madam&quot;,</span><br><span class="line">&quot;mayn&apos;t&quot;: &quot;may not&quot;,</span><br><span class="line">&quot;might&apos;ve&quot;: &quot;might have&quot;,</span><br><span class="line">&quot;mightn&apos;t&quot;: &quot;might not&quot;,</span><br><span class="line">&quot;mightn&apos;t&apos;ve&quot;: &quot;might not have&quot;,</span><br><span class="line">&quot;must&apos;ve&quot;: &quot;must have&quot;,</span><br><span class="line">&quot;mustn&apos;t&quot;: &quot;must not&quot;,</span><br><span class="line">&quot;mustn&apos;t&apos;ve&quot;: &quot;must not have&quot;,</span><br><span class="line">&quot;needn&apos;t&quot;: &quot;need not&quot;,</span><br><span class="line">&quot;needn&apos;t&apos;ve&quot;: &quot;need not have&quot;,</span><br><span class="line">&quot;o&apos;clock&quot;: &quot;of the clock&quot;,</span><br><span class="line">&quot;oughtn&apos;t&quot;: &quot;ought not&quot;,</span><br><span class="line">&quot;oughtn&apos;t&apos;ve&quot;: &quot;ought not have&quot;,</span><br><span class="line">&quot;shan&apos;t&quot;: &quot;shall not&quot;,</span><br><span class="line">&quot;sha&apos;n&apos;t&quot;: &quot;shall not&quot;,</span><br><span class="line">&quot;shan&apos;t&apos;ve&quot;: &quot;shall not have&quot;,</span><br><span class="line">&quot;she&apos;d&quot;: &quot;she would&quot;,</span><br><span class="line">&quot;she&apos;d&apos;ve&quot;: &quot;she would have&quot;,</span><br><span class="line">&quot;she&apos;ll&quot;: &quot;she will&quot;,</span><br><span class="line">&quot;she&apos;ll&apos;ve&quot;: &quot;she will have&quot;,</span><br><span class="line">&quot;she&apos;s&quot;: &quot;she is&quot;,</span><br><span class="line">&quot;should&apos;ve&quot;: &quot;should have&quot;,</span><br><span class="line">&quot;shouldn&apos;t&quot;: &quot;should not&quot;,</span><br><span class="line">&quot;shouldn&apos;t&apos;ve&quot;: &quot;should not have&quot;,</span><br><span class="line">&quot;so&apos;ve&quot;: &quot;so have&quot;,</span><br><span class="line">&quot;so&apos;s&quot;: &quot;so as&quot;,</span><br><span class="line">&quot;that&apos;d&quot;: &quot;that would&quot;,</span><br><span class="line">&quot;that&apos;d&apos;ve&quot;: &quot;that would have&quot;,</span><br><span class="line">&quot;that&apos;s&quot;: &quot;that is&quot;,</span><br><span class="line">&quot;there&apos;d&quot;: &quot;there would&quot;,</span><br><span class="line">&quot;there&apos;d&apos;ve&quot;: &quot;there would have&quot;,</span><br><span class="line">&quot;there&apos;s&quot;: &quot;there is&quot;,</span><br><span class="line">&quot;they&apos;d&quot;: &quot;they would&quot;,</span><br><span class="line">&quot;they&apos;d&apos;ve&quot;: &quot;they would have&quot;,</span><br><span class="line">&quot;they&apos;ll&quot;: &quot;they will&quot;,</span><br><span class="line">&quot;they&apos;ll&apos;ve&quot;: &quot;they will have&quot;,</span><br><span class="line">&quot;they&apos;re&quot;: &quot;they are&quot;,</span><br><span class="line">&quot;they&apos;ve&quot;: &quot;they have&quot;,</span><br><span class="line">&quot;to&apos;ve&quot;: &quot;to have&quot;,</span><br><span class="line">&quot;wasn&apos;t&quot;: &quot;was not&quot;,</span><br><span class="line">&quot;we&apos;d&quot;: &quot;we would&quot;,</span><br><span class="line">&quot;we&apos;d&apos;ve&quot;: &quot;we would have&quot;,</span><br><span class="line">&quot;we&apos;ll&quot;: &quot;we will&quot;,</span><br><span class="line">&quot;we&apos;ll&apos;ve&quot;: &quot;we will have&quot;,</span><br><span class="line">&quot;we&apos;re&quot;: &quot;we are&quot;,</span><br><span class="line">&quot;we&apos;ve&quot;: &quot;we have&quot;,</span><br><span class="line">&quot;weren&apos;t&quot;: &quot;were not&quot;,</span><br><span class="line">&quot;what&apos;ll&quot;: &quot;what will&quot;,</span><br><span class="line">&quot;what&apos;ll&apos;ve&quot;: &quot;what will have&quot;,</span><br><span class="line">&quot;what&apos;re&quot;: &quot;what are&quot;,</span><br><span class="line">&quot;what&apos;s&quot;: &quot;what is&quot;,</span><br><span class="line">&quot;what&apos;ve&quot;: &quot;what have&quot;,</span><br><span class="line">&quot;when&apos;s&quot;: &quot;when is&quot;,</span><br><span class="line">&quot;when&apos;ve&quot;: &quot;when have&quot;,</span><br><span class="line">&quot;where&apos;d&quot;: &quot;where did&quot;,</span><br><span class="line">&quot;where&apos;s&quot;: &quot;where is&quot;,</span><br><span class="line">&quot;where&apos;ve&quot;: &quot;where have&quot;,</span><br><span class="line">&quot;who&apos;ll&quot;: &quot;who will&quot;,</span><br><span class="line">&quot;who&apos;ll&apos;ve&quot;: &quot;who will have&quot;,</span><br><span class="line">&quot;who&apos;s&quot;: &quot;who is&quot;,</span><br><span class="line">&quot;who&apos;ve&quot;: &quot;who have&quot;,</span><br><span class="line">&quot;why&apos;s&quot;: &quot;why is&quot;,</span><br><span class="line">&quot;why&apos;ve&quot;: &quot;why have&quot;,</span><br><span class="line">&quot;will&apos;ve&quot;: &quot;will have&quot;,</span><br><span class="line">&quot;won&apos;t&quot;: &quot;will not&quot;,</span><br><span class="line">&quot;won&apos;t&apos;ve&quot;: &quot;will not have&quot;,</span><br><span class="line">&quot;would&apos;ve&quot;: &quot;would have&quot;,</span><br><span class="line">&quot;wouldn&apos;t&quot;: &quot;would not&quot;,</span><br><span class="line">&quot;wouldn&apos;t&apos;ve&quot;: &quot;would not have&quot;,</span><br><span class="line">&quot;y&apos;all&quot;: &quot;you all&quot;,</span><br><span class="line">&quot;y&apos;all&apos;d&quot;: &quot;you all would&quot;,</span><br><span class="line">&quot;y&apos;all&apos;d&apos;ve&quot;: &quot;you all would have&quot;,</span><br><span class="line">&quot;y&apos;all&apos;re&quot;: &quot;you all are&quot;,</span><br><span class="line">&quot;y&apos;all&apos;ve&quot;: &quot;you all have&quot;,</span><br><span class="line">&quot;you&apos;d&quot;: &quot;you would&quot;,</span><br><span class="line">&quot;you&apos;d&apos;ve&quot;: &quot;you would have&quot;,</span><br><span class="line">&quot;you&apos;ll&quot;: &quot;you will&quot;,</span><br><span class="line">&quot;you&apos;ll&apos;ve&quot;: &quot;you will have&quot;,</span><br><span class="line">&quot;you&apos;re&quot;: &quot;you are&quot;,</span><br><span class="line">&quot;you&apos;ve&quot;: &quot;you have&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="文本标准化"><a href="#文本标准化" class="headerlink" title="文本标准化"></a>文本标准化</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import string</span><br><span class="line">from nltk.stem import WordNetLemmatizer</span><br><span class="line"></span><br><span class="line">stopword_list = nltk.corpus.stopwords.words(&apos;english&apos;)</span><br><span class="line">wnl = WordNetLemmatizer()</span><br><span class="line">html_parser = HTMLParser()</span><br></pre></td></tr></table></figure>
<h5 id="文本分词"><a href="#文本分词" class="headerlink" title="文本分词"></a>文本分词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def tokenize_text(text):</span><br><span class="line">    tokens = nltk.word_tokenize(text) </span><br><span class="line">    tokens = [token.strip() for token in tokens]</span><br><span class="line">    return tokens</span><br></pre></td></tr></table></figure>
<h5 id="扩展缩写"><a href="#扩展缩写" class="headerlink" title="扩展缩写"></a>扩展缩写</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def expand_contractions(text, contraction_mapping):</span><br><span class="line">    </span><br><span class="line">    contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), </span><br><span class="line">                                      flags=re.IGNORECASE|re.DOTALL)</span><br><span class="line">    def expand_match(contraction):</span><br><span class="line">        match = contraction.group(0)</span><br><span class="line">        first_char = match[0]</span><br><span class="line">        expanded_contraction = contraction_mapping.get(match if contraction_mapping.get(match) else contraction_mapping.get(match.lower()))                    </span><br><span class="line">        expanded_contraction = first_char+expanded_contraction[1:]</span><br><span class="line">        return expanded_contraction</span><br><span class="line">        </span><br><span class="line">    expanded_text = contractions_pattern.sub(expand_match, text)</span><br><span class="line">    expanded_text = re.sub(&quot;&apos;&quot;, &quot;&quot;, expanded_text)</span><br><span class="line">    return expanded_text</span><br></pre></td></tr></table></figure>
<h5 id="标记文本词性"><a href="#标记文本词性" class="headerlink" title="标记文本词性"></a>标记文本词性</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from pattern.en import tag</span><br><span class="line">from nltk.corpus import wordnet as wn</span><br><span class="line"></span><br><span class="line"># 标记文本词性</span><br><span class="line">def pos_tag_text(text):</span><br><span class="line">    </span><br><span class="line">    def penn_to_wn_tags(pos_tag):</span><br><span class="line">        if pos_tag.startswith(&apos;J&apos;):</span><br><span class="line">            return wn.ADJ</span><br><span class="line">        elif pos_tag.startswith(&apos;V&apos;):</span><br><span class="line">            return wn.VERB</span><br><span class="line">        elif pos_tag.startswith(&apos;N&apos;):</span><br><span class="line">            return wn.NOUN</span><br><span class="line">        elif pos_tag.startswith(&apos;R&apos;):</span><br><span class="line">            return wn.ADV</span><br><span class="line">        else:</span><br><span class="line">            return None</span><br><span class="line">    </span><br><span class="line">    tagged_text = tag(text)</span><br><span class="line">    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text]</span><br><span class="line">    return tagged_lower_text</span><br></pre></td></tr></table></figure>
<h5 id="基于词性标签提取主干词"><a href="#基于词性标签提取主干词" class="headerlink" title="基于词性标签提取主干词"></a>基于词性标签提取主干词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def lemmatize_text(text):</span><br><span class="line">    </span><br><span class="line">    pos_tagged_text = pos_tag_text(text)</span><br><span class="line">    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag</span><br><span class="line">                         else word                     </span><br><span class="line">                         for word, pos_tag in pos_tagged_text]</span><br><span class="line">    lemmatized_text = &apos; &apos;.join(lemmatized_tokens)</span><br><span class="line">    return lemmatized_text</span><br></pre></td></tr></table></figure>
<h5 id="删除特殊字符"><a href="#删除特殊字符" class="headerlink" title="删除特殊字符"></a>删除特殊字符</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def remove_special_characters(text):</span><br><span class="line">    tokens = tokenize_text(text)</span><br><span class="line">    pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation)))</span><br><span class="line">    filtered_tokens = filter(None, [pattern.sub(&apos; &apos;, token) for token in tokens])</span><br><span class="line">    filtered_text = &apos; &apos;.join(filtered_tokens)</span><br><span class="line">    return filtered_text</span><br></pre></td></tr></table></figure>
<h5 id="删除停用词"><a href="#删除停用词" class="headerlink" title="删除停用词"></a>删除停用词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def remove_stopwords(text):</span><br><span class="line">    tokens = tokenize_text(text)</span><br><span class="line">    filtered_tokens = [token for token in tokens if token not in stopword_list]</span><br><span class="line">    filtered_text = &apos; &apos;.join(filtered_tokens)    </span><br><span class="line">    return filtered_text</span><br></pre></td></tr></table></figure>
<h5 id="转移HTML标签-1"><a href="#转移HTML标签-1" class="headerlink" title="转移HTML标签"></a>转移HTML标签</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def unescape_html(parser, text):</span><br><span class="line">    return parser.unescape(text)</span><br></pre></td></tr></table></figure>
<h5 id="标准化文本-合并执行上面流程"><a href="#标准化文本-合并执行上面流程" class="headerlink" title="标准化文本(合并执行上面流程)"></a>标准化文本(合并执行上面流程)</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def normalize_corpus(corpus, lemmatize=True, tokenize=False):</span><br><span class="line">    </span><br><span class="line">    normalized_corpus = []  </span><br><span class="line">    for text in corpus:</span><br><span class="line">        text = html_parser.unescape(text)</span><br><span class="line">        text = expand_contractions(text, CONTRACTION_MAP)</span><br><span class="line">        if lemmatize:</span><br><span class="line">            text = lemmatize_text(text)</span><br><span class="line">        else:</span><br><span class="line">            text = text.lower()</span><br><span class="line">        text = remove_special_characters(text)</span><br><span class="line">        text = remove_stopwords(text)</span><br><span class="line">        if tokenize:</span><br><span class="line">            text = tokenize_text(text)</span><br><span class="line">            normalized_corpus.append(text)</span><br><span class="line">        else:</span><br><span class="line">            normalized_corpus.append(text)</span><br><span class="line">            </span><br><span class="line">    return normalized_corpus</span><br></pre></td></tr></table></figure>
<h1 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h1><ul>
<li>基于词项次数的二值特征</li>
<li>基于词袋模型的频率特征</li>
<li>TF-IDF权重特征</li>
</ul>
<h5 id="构建特征矩阵binary、frequency、tfidf"><a href="#构建特征矩阵binary、frequency、tfidf" class="headerlink" title="构建特征矩阵binary、frequency、tfidf"></a>构建特征矩阵binary、frequency、tfidf</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer</span><br><span class="line">def build_feature_matrix(documents,feature_type=&apos;frequency&apos;):</span><br><span class="line">    feature_type=feature_type.lower().strip()</span><br><span class="line">    if feature_type==&apos;binary&apos;:</span><br><span class="line">        vectorizer=CountVectorizer(binary=True,min_df=1,ngram_range=(1,1))</span><br><span class="line">    elif feature_type==&apos;frequency&apos;:</span><br><span class="line">        vectorizer=CountVectorizer(binary=False,min_df=1,ngram_range=(1,1))</span><br><span class="line">    elif feature_type==&apos;tfidf&apos;:</span><br><span class="line">        vectorizer=TfidfVectorizer(min_df=1,ngram_range=(1,1))</span><br><span class="line">    else:</span><br><span class="line">        raise Exception(&quot;Wrong feature type entered. Possible values: &apos;binary&apos;, &apos;frequency&apos;, &apos;tfidf&apos;&quot;)</span><br><span class="line">    feature_matrix=vectorizer.fit_transform(documents).astype(float)</span><br><span class="line">    return vectorizer,feature_matrix</span><br></pre></td></tr></table></figure>
<h1 id="关键短语提取"><a href="#关键短语提取" class="headerlink" title="关键短语提取"></a>关键短语提取</h1><h3 id="词项搭配"><a href="#词项搭配" class="headerlink" title="词项搭配"></a>词项搭配</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.corpus import gutenberg</span><br><span class="line">import nltk</span><br><span class="line">from operator import itemgetter</span><br><span class="line"></span><br><span class="line">alice = gutenberg.sents(fileids=&apos;carroll-alice.txt&apos;)</span><br><span class="line">alice = [&apos; &apos;.join(ts) for ts in alice]</span><br><span class="line">norm_alice = normalize_corpus(alice, lemmatize=False)</span><br></pre></td></tr></table></figure>
<h5 id="将语料压缩成1个大的文本串"><a href="#将语料压缩成1个大的文本串" class="headerlink" title="将语料压缩成1个大的文本串"></a>将语料压缩成1个大的文本串</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def flatten_corpus(corpus):</span><br><span class="line">    return &apos; &apos;.join([document.strip() </span><br><span class="line">                     for document in corpus])</span><br></pre></td></tr></table></figure>
<h5 id="计算n元分词（比较巧妙）"><a href="#计算n元分词（比较巧妙）" class="headerlink" title="计算n元分词（比较巧妙）"></a>计算n元分词（比较巧妙）</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def compute_ngrams(sequence,n):</span><br><span class="line">#     print([sequence[index:] for index in range(n)])</span><br><span class="line">#     print(list(zip(*[sequence[index:] for index in range(n)])))</span><br><span class="line">    #解压时仅按最小元素数组数量进行解压</span><br><span class="line">    return zip(*[sequence[index:] for index in range(n)])</span><br></pre></td></tr></table></figure>
<h5 id="获取n元分词"><a href="#获取n元分词" class="headerlink" title="获取n元分词"></a>获取n元分词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_top_ngram(corpus,ngram_val=1,limit=5):</span><br><span class="line">    corpus=flatten_corpus(corpus)</span><br><span class="line">    tokens=nltk.word_tokenize(corpus)</span><br><span class="line">    </span><br><span class="line">    ngrams=compute_ngrams(tokens,ngram_val)</span><br><span class="line">    #获取单词频率</span><br><span class="line">    ngrams_freq_dist=nltk.FreqDist(ngrams)</span><br><span class="line">    #排序频率</span><br><span class="line">    sorted_ngrams_fd=sorted(ngrams_freq_dist.items(),key=itemgetter(1),reverse=True)</span><br><span class="line">    sorted_ngrams=sorted_ngrams_fd[0:limit]</span><br><span class="line">    sorted_ngrams=[(&apos; &apos;.join(text),freq) for text,freq in sorted_ngrams]</span><br><span class="line">    return sorted_ngrams</span><br></pre></td></tr></table></figure>
<h5 id="输出频率前10的二元分词"><a href="#输出频率前10的二元分词" class="headerlink" title="输出频率前10的二元分词"></a>输出频率前10的二元分词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get_top_ngram(corpus=norm_alice,ngram_val=2,limit=10)</span><br></pre></td></tr></table></figure>
<p>输出结果<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;said alice&apos;, 123),</span><br><span class="line"> (&apos;mock turtle&apos;, 56),</span><br><span class="line"> (&apos;march hare&apos;, 31),</span><br><span class="line"> (&apos;said king&apos;, 29),</span><br><span class="line"> (&apos;thought alice&apos;, 26),</span><br><span class="line"> (&apos;white rabbit&apos;, 22),</span><br><span class="line"> (&apos;said hatter&apos;, 22),</span><br><span class="line"> (&apos;said mock&apos;, 20),</span><br><span class="line"> (&apos;said caterpillar&apos;, 18),</span><br><span class="line"> (&apos;said gryphon&apos;, 18)]</span><br></pre></td></tr></table></figure></p>
<h5 id="频率前10的三元分词"><a href="#频率前10的三元分词" class="headerlink" title="频率前10的三元分词"></a>频率前10的三元分词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get_top_ngram(corpus=norm_alice,ngram_val=3,limit=10)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;said mock turtle&apos;, 20),</span><br><span class="line"> (&apos;said march hare&apos;, 9),</span><br><span class="line"> (&apos;poor little thing&apos;, 6),</span><br><span class="line"> (&apos;little golden key&apos;, 5),</span><br><span class="line"> (&apos;certainly said alice&apos;, 5),</span><br><span class="line"> (&apos;white kid gloves&apos;, 5),</span><br><span class="line"> (&apos;march hare said&apos;, 5),</span><br><span class="line"> (&apos;mock turtle said&apos;, 5),</span><br><span class="line"> (&apos;know said alice&apos;, 4),</span><br><span class="line"> (&apos;might well say&apos;, 4)]</span><br></pre></td></tr></table></figure>
<h5 id="频率前10的一元分词"><a href="#频率前10的一元分词" class="headerlink" title="频率前10的一元分词"></a>频率前10的一元分词</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get_top_ngram(corpus=norm_alice,ngram_val=1,limit=10)</span><br></pre></td></tr></table></figure>
<p>输出结果<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;said&apos;, 462),</span><br><span class="line"> (&apos;alice&apos;, 398),</span><br><span class="line"> (&apos;little&apos;, 128),</span><br><span class="line"> (&apos;one&apos;, 104),</span><br><span class="line"> (&apos;know&apos;, 88),</span><br><span class="line"> (&apos;like&apos;, 85),</span><br><span class="line"> (&apos;would&apos;, 83),</span><br><span class="line"> (&apos;went&apos;, 83),</span><br><span class="line"> (&apos;could&apos;, 77),</span><br><span class="line"> (&apos;queen&apos;, 75)]</span><br></pre></td></tr></table></figure></p>
<h3 id="使用nltk的搭配查找器"><a href="#使用nltk的搭配查找器" class="headerlink" title="使用nltk的搭配查找器"></a>使用nltk的搭配查找器</h3><h5 id="二元词项"><a href="#二元词项" class="headerlink" title="二元词项"></a>二元词项</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.collocations import BigramCollocationFinder</span><br><span class="line">from nltk.collocations import BigramAssocMeasures</span><br><span class="line"></span><br><span class="line">finder=BigramCollocationFinder.from_documents([item.split() for item in norm_alice])</span><br><span class="line"></span><br><span class="line">bigram_measures=BigramAssocMeasures()</span><br><span class="line"></span><br><span class="line">#使用原始频率进行查找</span><br><span class="line">finder.nbest(bigram_measures.raw_freq,10)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;said&apos;, &apos;alice&apos;),</span><br><span class="line"> (&apos;mock&apos;, &apos;turtle&apos;),</span><br><span class="line"> (&apos;march&apos;, &apos;hare&apos;),</span><br><span class="line"> (&apos;said&apos;, &apos;king&apos;),</span><br><span class="line"> (&apos;thought&apos;, &apos;alice&apos;),</span><br><span class="line"> (&apos;said&apos;, &apos;hatter&apos;),</span><br><span class="line"> (&apos;white&apos;, &apos;rabbit&apos;),</span><br><span class="line"> (&apos;said&apos;, &apos;mock&apos;),</span><br><span class="line"> (&apos;said&apos;, &apos;caterpillar&apos;),</span><br><span class="line"> (&apos;said&apos;, &apos;gryphon&apos;)]</span><br></pre></td></tr></table></figure>
<h5 id="二元使用点互信息PMI进行查找搭配"><a href="#二元使用点互信息PMI进行查找搭配" class="headerlink" title="二元使用点互信息PMI进行查找搭配"></a>二元使用点互信息PMI进行查找搭配</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">finder.nbest(bigram_measures.pmi,10)</span><br></pre></td></tr></table></figure>
<h5 id="三元词组"><a href="#三元词组" class="headerlink" title="三元词组"></a>三元词组</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from nltk.collocations import TrigramAssocMeasures</span><br><span class="line">from nltk.collocations import TrigramCollocationFinder</span><br><span class="line"></span><br><span class="line">finder=TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])</span><br><span class="line"></span><br><span class="line">trigram_measures=TrigramAssocMeasures()</span><br><span class="line"></span><br><span class="line">#三元组频率</span><br><span class="line">finder.nbest(trigram_measures.raw_freq,10)</span><br></pre></td></tr></table></figure>
<p>输出结果<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;said&apos;, &apos;mock&apos;, &apos;turtle&apos;),</span><br><span class="line"> (&apos;said&apos;, &apos;march&apos;, &apos;hare&apos;),</span><br><span class="line"> (&apos;poor&apos;, &apos;little&apos;, &apos;thing&apos;),</span><br><span class="line"> (&apos;little&apos;, &apos;golden&apos;, &apos;key&apos;),</span><br><span class="line"> (&apos;march&apos;, &apos;hare&apos;, &apos;said&apos;),</span><br><span class="line"> (&apos;mock&apos;, &apos;turtle&apos;, &apos;said&apos;),</span><br><span class="line"> (&apos;white&apos;, &apos;kid&apos;, &apos;gloves&apos;),</span><br><span class="line"> (&apos;beau&apos;, &apos;ootiful&apos;, &apos;soo&apos;),</span><br><span class="line"> (&apos;certainly&apos;, &apos;said&apos;, &apos;alice&apos;),</span><br><span class="line"> (&apos;might&apos;, &apos;well&apos;, &apos;say&apos;)]</span><br></pre></td></tr></table></figure></p>
<h5 id="三元使用点互信息PMI进行查找搭配"><a href="#三元使用点互信息PMI进行查找搭配" class="headerlink" title="三元使用点互信息PMI进行查找搭配"></a>三元使用点互信息PMI进行查找搭配</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">finder.nbest(trigram_measures.pmi,10)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;accustomed&apos;, &apos;usurpation&apos;, &apos;conquest&apos;),</span><br><span class="line"> (&apos;adjourn&apos;, &apos;immediate&apos;, &apos;adoption&apos;),</span><br><span class="line"> (&apos;adoption&apos;, &apos;energetic&apos;, &apos;remedies&apos;),</span><br><span class="line"> (&apos;ancient&apos;, &apos;modern&apos;, &apos;seaography&apos;),</span><br><span class="line"> (&apos;apple&apos;, &apos;roast&apos;, &apos;turkey&apos;),</span><br><span class="line"> (&apos;arithmetic&apos;, &apos;ambition&apos;, &apos;distraction&apos;),</span><br><span class="line"> (&apos;brother&apos;, &apos;latin&apos;, &apos;grammar&apos;),</span><br><span class="line"> (&apos;canvas&apos;, &apos;bag&apos;, &apos;tied&apos;),</span><br><span class="line"> (&apos;cherry&apos;, &apos;tart&apos;, &apos;custard&apos;),</span><br><span class="line"> (&apos;circle&apos;, &apos;exact&apos;, &apos;shape&apos;)]</span><br></pre></td></tr></table></figure>
<h1 id="基于权重标签的短语提取"><a href="#基于权重标签的短语提取" class="headerlink" title="基于权重标签的短语提取"></a>基于权重标签的短语提取</h1><ul>
<li>使用浅层分析提取所有名词短语词块</li>
<li>计算每个词块的TF-IDF权重并返回最大加权短语</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toy_text = &quot;&quot;&quot;</span><br><span class="line">Elephants are large mammals of the family Elephantidae </span><br><span class="line">and the order Proboscidea. Two species are traditionally recognised, </span><br><span class="line">the African elephant and the Asian elephant. Elephants are scattered </span><br><span class="line">throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male </span><br><span class="line">African elephants are the largest extant terrestrial animals. All </span><br><span class="line">elephants have a long trunk used for many purposes, </span><br><span class="line">particularly breathing, lifting water and grasping objects. Their </span><br><span class="line">incisors grow into tusks, which can serve as weapons and as tools </span><br><span class="line">for moving objects and digging. Elephants&apos; large ear flaps help </span><br><span class="line">to control their body temperature. Their pillar-like legs can </span><br><span class="line">carry their great weight. African elephants have larger ears </span><br><span class="line">and concave backs while Asian elephants have smaller ears </span><br><span class="line">and convex or level backs.  </span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import itertools</span><br><span class="line">from gensim import corpora, models</span><br></pre></td></tr></table></figure>
<p>基本上，我们有一个已定义的语法模式来分块或提取名词短语。我们在同一模式中定义一个分块器，对于文档中的每个句子，首先用它的POS标签来标注它(因此，不应该对文本进行规范化)，然后构建一个具有名词短语的浅层分析树作为词块和其他全部基于POS标签的单词作为缝隙，缝隙是不属于任何词块的部分。完成此操作后，我们使用tree2conl1tags函数来生成(w, t，c)三元组，它们是的单词、POS标签和IOB格式的词块标签。删除所有词块带有’O ‘标签的标签，因为它们基本上是不属于任何词块的单词或词项。最后，从这些有效的词块中，组合分块的词项，并从每个词块分组中生成短语。</p>
<h5 id="提取文档中的名词短语-v-adj-adv"><a href="#提取文档中的名词短语-v-adj-adv" class="headerlink" title="提取文档中的名词短语 v adj adv"></a>提取文档中的名词短语 v adj adv</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#提取文档中的名词短语 v adj adv</span><br><span class="line">def get_chunks(sentences, grammar = r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;):</span><br><span class="line">    </span><br><span class="line">    all_chunks = []</span><br><span class="line">    chunker = nltk.chunk.regexp.RegexpParser(grammar)</span><br><span class="line">    </span><br><span class="line">    for sentence in sentences:</span><br><span class="line">        </span><br><span class="line">        tagged_sents = nltk.pos_tag_sents(</span><br><span class="line">                            [nltk.word_tokenize(sentence)])</span><br><span class="line">        </span><br><span class="line">        chunks = [chunker.parse(tagged_sent) </span><br><span class="line">                  for tagged_sent in tagged_sents]</span><br><span class="line">        </span><br><span class="line">        wtc_sents = [nltk.chunk.tree2conlltags(chunk)</span><br><span class="line">                     for chunk in chunks]    </span><br><span class="line">         </span><br><span class="line">        flattened_chunks = list(</span><br><span class="line">                            itertools.chain.from_iterable(</span><br><span class="line">                                wtc_sent for wtc_sent in wtc_sents)</span><br><span class="line">                           )</span><br><span class="line">#         print(flattened_chunks)</span><br><span class="line">#         print(flattened_chunks)</span><br><span class="line">        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) </span><br><span class="line">                        for status, chunk in itertools.groupby(flattened_chunks,lambda chunk: chunk != &apos;O&apos;)]</span><br><span class="line">#         print(&apos;---&apos;*20)</span><br><span class="line">#         print(valid_chunks_tagged)</span><br><span class="line">        valid_chunks = [&apos; &apos;.join(word.lower() </span><br><span class="line">                                for word, tag, chunk </span><br><span class="line">                                in wtc_group </span><br><span class="line">                                    if word.lower() </span><br><span class="line">                                        not in stopword_list) </span><br><span class="line">                                    for status, wtc_group </span><br><span class="line">                                    in valid_chunks_tagged</span><br><span class="line">                                        if status]</span><br><span class="line">        all_chunks.append(valid_chunks)</span><br><span class="line">    </span><br><span class="line">    return all_chunks</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sentences = parse_document(toy_text)          </span><br><span class="line">valid_chunks = get_chunks(sentences)</span><br></pre></td></tr></table></figure>
<h5 id="获取TF-IDF关键短语权重"><a href="#获取TF-IDF关键短语权重" class="headerlink" title="获取TF-IDF关键短语权重"></a>获取TF-IDF关键短语权重</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_tfidf_weighted_keyphrases(sentences, </span><br><span class="line">                                  grammar=r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;,</span><br><span class="line">                                  top_n=10):</span><br><span class="line">    </span><br><span class="line">    valid_chunks = get_chunks(sentences, grammar=grammar)</span><br><span class="line">                                     </span><br><span class="line">    dictionary = corpora.Dictionary(valid_chunks)</span><br><span class="line">    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]</span><br><span class="line">    </span><br><span class="line">    tfidf = models.TfidfModel(corpus)</span><br><span class="line">    corpus_tfidf = tfidf[corpus]</span><br><span class="line">    </span><br><span class="line">    weighted_phrases = &#123;dictionary.get(id): round(value,3) </span><br><span class="line">                        for doc in corpus_tfidf </span><br><span class="line">                        for id, value in doc&#125;</span><br><span class="line">                            </span><br><span class="line">    weighted_phrases = sorted(weighted_phrases.items(), </span><br><span class="line">                              key=itemgetter(1), reverse=True)</span><br><span class="line">    </span><br><span class="line">    return weighted_phrases[:top_n]</span><br></pre></td></tr></table></figure>
<h5 id="前两个关键短语"><a href="#前两个关键短语" class="headerlink" title="前两个关键短语"></a>前两个关键短语</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get_tfidf_weighted_keyphrases(sentences, top_n=2)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&apos;elephants large mammals family elephantidae order proboscidea .&apos;, 1.0),</span><br><span class="line"> (&apos;two species traditionally recognised , african elephant asian elephant .&apos;,</span><br><span class="line">  1.0)]</span><br></pre></td></tr></table></figure>
<h5 id="其他语料实验"><a href="#其他语料实验" class="headerlink" title="其他语料实验"></a>其他语料实验</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get_tfidf_weighted_keyphrases(alice, top_n=10)</span><br></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(&quot;[ alice &apos; adventures wonderland lewis carroll 1865 ]&quot;, 1.0),</span><br><span class="line"> (&apos;chapter .&apos;, 1.0),</span><br><span class="line"> (&apos;rabbit - hole&apos;, 1.0),</span><br><span class="line"> (&quot;alice beginning get tired sitting sister bank , nothing : twice peeped book sister reading , pictures conversations , &apos; use book , &apos; thought alice &apos; without pictures conversation ? &apos;&quot;,</span><br><span class="line">  1.0),</span><br><span class="line"> (&apos;considering mind ( well could , hot day made feel sleepy stupid ) , whether pleasure making daisy - chain would worth trouble getting picking daisies , suddenly white rabbit pink eyes ran close .&apos;,</span><br><span class="line">  1.0),</span><br><span class="line"> (&quot;nothing remarkable ; alice think much way hear rabbit say , &apos; oh dear !&quot;,</span><br><span class="line">  1.0),</span><br><span class="line"> (&apos;oh dear !&apos;, 1.0),</span><br><span class="line"> (&quot;shall late ! &apos;&quot;, 1.0),</span><br><span class="line"> (&apos;( thought afterwards , occurred ought wondered , time seemed quite natural ) ; rabbit actually took watch waistcoat - pocket , looked , hurried , alice started feet , flashed across mind never seen rabbit either waistcoat - pocket , watch take , burning curiosity , ran across field , fortunately time see pop large rabbit - hole hedge .&apos;,</span><br><span class="line">  1.0),</span><br><span class="line"> (&apos;another moment went alice , never considering world get .&apos;, 1.0)]</span><br></pre></td></tr></table></figure></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">CinKate</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://renxingkai.github.io/2019/04/10/tfidfkeyextraction/">http://renxingkai.github.io/2019/04/10/tfidfkeyextraction/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/关键词抽取/">关键词抽取</a></div><div class="social-share pull-right" data-disabled="google,facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/04/21/fastqa/"><i class="fa fa-chevron-left">  </i><span>FastQA学习</span></a></div><div class="next-post pull-right"><a href="/2019/04/05/word-tfidf/"><span>Word2Vec相关(用TFIDF加权词向量)</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://p17.qhimg.com/d/_open360/fengjing0403/21.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By CinKate</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://renxingkai.github.io">blog</a>!</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>